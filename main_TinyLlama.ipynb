{"cells":[{"cell_type":"markdown","source":["### Instalaciones necesarias"],"metadata":{"id":"ZAL_o9_wG1e5"},"id":"ZAL_o9_wG1e5"},{"cell_type":"code","source":["%pip install unsloth[colab-new] xformers trl peft accelerate bitsandbytes rapidfuzz\n","%reset -f"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TcYxpu00GjeX","executionInfo":{"status":"ok","timestamp":1751081236437,"user_tz":240,"elapsed":5013,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"f5241866-a85c-4ad8-f27d-8b3498a36902"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.30)\n","Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.19.0)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (3.13.0)\n","Requirement already satisfied: unsloth[colab-new] in /usr/local/lib/python3.11/dist-packages (2025.6.8)\n","Requirement already satisfied: unsloth_zoo>=2025.6.4 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2025.6.6)\n","Requirement already satisfied: torch<=2.7.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.7.0)\n","Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (24.2)\n","Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.9.24)\n","Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (4.52.4)\n","Requirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.6.0)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.0.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.20.3)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.33.0)\n","Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.1.9)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.34.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.22.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (4.14.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.11.1.6)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth[colab-new]) (75.2.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.70.15)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth[colab-new]) (1.1.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth[colab-new]) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth[colab-new]) (0.21.2)\n","Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.4->unsloth[colab-new]) (25.1.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.4->unsloth[colab-new]) (11.2.1)\n","Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.4->unsloth[colab-new]) (0.19.0)\n","Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth[colab-new]) (8.7.0)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (13.9.4)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (1.7.2)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (4.4.4)\n","\u001b[33mWARNING: unsloth 2025.6.8 does not provide the extra 'triton'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (3.11.15)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (2025.6.15)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (2.19.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.3.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata->diffusers->unsloth[colab-new]) (3.23.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.20.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth[colab-new]) (1.17.0)\n"]}],"id":"TcYxpu00GjeX"},{"cell_type":"markdown","id":"205c6de8","metadata":{"id":"205c6de8"},"source":["### G-Drive Setup"]},{"cell_type":"markdown","id":"G91y3WGR8JVS","metadata":{"id":"G91y3WGR8JVS"},"source":["Este notebook asume que se está ejecutando en Google Colab, y que el dataset `LLM-Redial` disponible en https://drive.google.com/drive/folders/1TIP4PFm9z0C4R4--KnHoWuiB1uK-dv5m se encuentra descargado en el drive del usuario."]},{"cell_type":"code","execution_count":2,"id":"Zpy6FRKqFfjT","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24119,"status":"ok","timestamp":1751079767838,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"Zpy6FRKqFfjT","outputId":"4f70d474-8763-4074-a917-7c577f30b9f9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"id":"uop1TotjGGPP","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1750961168926,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"uop1TotjGGPP","outputId":"c3662727-0d03-466a-9bd4-c726525aa617"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Tools.py'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import shutil\n","import os\n","\n","source_path = '/content/gdrive/MyDrive/Proyecto LLMonkeys/Tools.py'\n","destination_path = '/content/Tools.py'\n","\n","shutil.copy(source_path, destination_path)"]},{"cell_type":"code","execution_count":null,"id":"Xs5gDWtwGbge","metadata":{"id":"Xs5gDWtwGbge"},"outputs":[],"source":["import zipfile\n","\n","zip_path = '/content/gdrive/MyDrive/Proyecto LLMonkeys/LLM_Redial.zip'\n","extract_path = '/content/LLM_Redial'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)"]},{"cell_type":"markdown","source":["### Seleccionamos el modelo"],"metadata":{"id":"Ldpe3ss8gOeE"},"id":"Ldpe3ss8gOeE"},{"cell_type":"code","execution_count":null,"id":"495d7471","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4940,"status":"ok","timestamp":1750961174755,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"495d7471","outputId":"6eb7f96c-5227-410c-fb05-63a6cf61aca0"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["from transformers import AutoTokenizer\n","\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","source":["### Cargamos los datos del dataset y funciones útiles"],"metadata":{"id":"lRWQWyt3gooq"},"id":"lRWQWyt3gooq"},{"cell_type":"code","execution_count":null,"id":"c9f178ea","metadata":{"id":"c9f178ea"},"outputs":[],"source":["import Tools as t\n","\n","path = \"./LLM_Redial/Movie\"\n","final_data_path = '{}/final_data.jsonl'.format(path)\n","Conversation_path = '{}/Conversation.txt'.format(path)\n","user_map_path = '{}/user_ids.json'.format(path)\n","item_map_path = '{}/item_map.json'.format(path)\n","\n","final_data = t.read_jsonl(final_data_path)\n","user_map = t.read_json(user_map_path)\n","item_map = t.read_json(item_map_path)\n","Conversation = t.read_dialogue(Conversation_path)"]},{"cell_type":"markdown","id":"tbkknIrVmsd_","metadata":{"id":"tbkknIrVmsd_"},"source":["### Para limpiar el entorno y guardar las variables relevantes"]},{"cell_type":"code","execution_count":null,"id":"76eC882Smq-8","metadata":{"id":"76eC882Smq-8"},"outputs":[],"source":["import gc\n","import torch\n","import dill\n","\n","def cleanup_for_dill_serialization():\n","    \"\"\"\n","    Limpia todas las variables pesadas que pueden causar problemas con dill\n","    Mantiene solo los outputs y variables esenciales\n","    \"\"\"\n","    # Variables que debes eliminar ANTES de dill.dump_session()\n","    variables_to_delete = [\n","        'model', 'base_model', 'config', 'pipe',\n","\n","        # Tensores y objetos de PyTorch\n","        'inputs', 'output_ids', 'outputs',\n","\n","        # Variables temporales del loop\n","        'decoded', 'prompt', 'messages', 'ctx', 'msg',\n","\n","        # Indices y variables de control\n","        'i', 'n', 'k',\n","    ]\n","\n","    # Lista de variables que SÍ quieres mantener\n","    variables_to_keep = [\n","        'item_map',\n","        'Conversation',\n","        'model_name',\n","        'output_r'\n","        'outputs_mp'\n","        'outputs_z_s',\n","        'outputs_f_s',\n","        'outputs_ft_s',\n","        'outputs_z_n',\n","        'outputs_f_n',\n","        'outputs_ft_n',\n","        'rand_conversations',\n","        'num_test_items',\n","        'train_conv',\n","        'test_conv',\n","        'few_shot_users',\n","        'num_test_items',\n","        'few_shot_data'\n","    ]\n","\n","    # Obtener todas las variables globales\n","    global_vars = list(globals().keys())\n","\n","    # Eliminar variables específicamente problemáticas\n","    for var_name in variables_to_delete:\n","        if var_name in globals():\n","            print(f\"   ├── Eliminando: {var_name}\")\n","            try:\n","                del globals()[var_name]\n","            except:\n","                print(f\"No se pudo eliminar {var_name}\")\n","\n","    vars_to_remove = []\n","    for var_name in global_vars:\n","        if var_name.startswith('_'):  # Variables privadas\n","            continue\n","\n","        if var_name in variables_to_keep:  # No eliminar variables importantes\n","            continue\n","\n","        try:\n","            var_obj = globals().get(var_name)\n","            var_type = str(type(var_obj))\n","\n","            # Detectar objetos problemáticos\n","            problematic_types = [\n","                'transformers',\n","                'peft',\n","                'torch.nn',\n","                'pipeline',\n","                'PreTrainedModel',\n","                'PreTrainedTokenizer',\n","                'PeftModel',\n","                'Tensor'\n","            ]\n","\n","            if any(prob_type in var_type for prob_type in problematic_types):\n","                vars_to_remove.append(var_name)\n","\n","        except Exception as e:\n","            vars_to_remove.append(var_name)\n","\n","    # Eliminar variables problemáticas detectadas\n","    for var_name in vars_to_remove:\n","        print(f\"   ├── Eliminando: {var_name}\")\n","        try:\n","            del globals()[var_name]\n","        except:\n","            print(f\"No se pudo eliminar: {var_name}\")\n","\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","    gc.collect()\n","\n","    # Verificar tamaño de variables mantenidas\n","    print(\"\\n📊 Variables mantenidas:\")\n","    for var_name in variables_to_keep:\n","        if var_name in globals():\n","            var_obj = globals()[var_name]\n","            try:\n","                if hasattr(var_obj, '__len__'):\n","                    print(f\"   ├── {var_name}: {len(var_obj)} elementos\")\n","                else:\n","                    print(f\"   ├── {var_name}: {type(var_obj)}\")\n","            except:\n","                print(f\"   ├── {var_name}: (no se puede medir)\")\n","\n","    return True\n","\n","# FUNCIÓN PRINCIPAL PARA TU CASO\n","def cleanup_after_lora_generation():\n","    \"\"\"\n","    Limpieza específica después de generar con modelo LoRA\n","    \"\"\"\n","    print(\"🎯 Limpieza específica para modelo LoRA...\")\n","\n","    # Variables específicas de tu código LoRA\n","    lora_specific_vars = [\n","        'peft_model_path',\n","        'config',           # PeftConfig\n","        'base_model',       # Modelo base\n","        'model',           # PeftModel final\n","        'tokenizer',       # Tokenizer\n","        'pipe',           # Pipeline\n","        'inputs',          # Tensors de input\n","        'output_ids',      # Tensors de output IDs\n","        'prompt',          # Prompt generado\n","        'messages',        # Mensajes del chat template\n","        'ctx',            # Context string\n","        'msg',            # Message string\n","        'decoded',        # String decodificado\n","        'outputs',        # Lista temporal (no outputs_ft_s)\n","    ]\n","\n","    for var_name in lora_specific_vars:\n","        if var_name in globals():\n","            print(f\"   ├── Eliminando: {var_name}\")\n","            try:\n","                del globals()[var_name]\n","            except Exception as e:\n","                print(f\"   │   └── Error: {e}\")\n","\n","    # Limpieza general\n","    cleanup_for_dill_serialization()\n","\n","def limpiar_y_guardar():\n","  cleanup_after_lora_generation()\n","  path = '/content/gdrive/MyDrive/Proyecto LLMonkeys/sessions/TinyLlama/TinyLlama_notebook_env.db'\n","  with open(path, 'wb') as f:\n","      dill.dump_session(f)"]},{"cell_type":"markdown","id":"SZkhMuOgm5eB","metadata":{"id":"SZkhMuOgm5eB"},"source":["### Para retornar la respuesta del modelo como una lista de nombres"]},{"cell_type":"code","execution_count":null,"id":"Juqn42WWm4pO","metadata":{"id":"Juqn42WWm4pO"},"outputs":[],"source":["import re\n","def format_ans(ans, n):\n","    answer = \"None\"\n","    try:\n","        answer_list = []\n","        current_pos = 0\n","\n","        for j in range(1, n + 1):\n","            # Busca el patrón del número y el inicio del texto\n","            patterns = [\n","                f\"{j}. \",\n","                f\"Movie {j}: \",\n","                f\"Movie name {j}: \",\n","                f\"Movie Name {j}: \",\n","                f\"[{j}] \",\n","            ]\n","            for pattern in patterns:\n","                pattern_start = pattern\n","                start_index = ans.find(pattern_start, current_pos)\n","                if start_index != -1:\n","                    break\n","\n","            if start_index == -1:\n","                break  # Si no se encuentra el número, salimos del bucle\n","\n","            start_text = start_index + len(pattern_start)\n","\n","            # Busca el final del texto: el inicio del siguiente número O un \" - \"\n","            next_number_start = ans.find(f\"{j + 1}. \", start_text)\n","            dash_start = ans.find(\" - \", start_text)\n","\n","            end_text = -1\n","\n","            # Determina el final del texto basado en la primera ocurrencia\n","            if next_number_start != -1 and dash_start != -1:\n","                end_text = min(next_number_start, dash_start)\n","            elif next_number_start != -1:\n","                end_text = next_number_start\n","            elif dash_start != -1:\n","                end_text = dash_start\n","            else:\n","                # Si no hay siguiente número ni \" - \", toma hasta el final de la línea o la cadena\n","                end_line = ans.find(\"\\n\", start_text)\n","                if end_line != -1:\n","                    end_text = end_line\n","                else:\n","                    end_text = len(ans)\n","\n","            if end_text == -1: # Si no se encontró un final válido, toma hasta el final de la cadena\n","                end_text = len(ans)\n","\n","            movie_name = ans[start_text:end_text].strip()\n","            answer_list.append(movie_name)\n","            current_pos = end_text # Actualiza la posición para la próxima búsqueda\n","\n","        if not answer_list: # Si no se encontraron películas\n","             return \"Formato de respuesta incorrecto\", i\n","\n","    except Exception as e:\n","        answer = f\"Error: {e}\"\n","\n","    finally:\n","        pattern = r\"\\(\\d{4}\\)\"\n","        answer_list = [m.replace('\\\"', '') for m in answer_list]\n","        answer_list = [re.sub(pattern, '', m).strip() for m in answer_list]\n","        return answer_list"]},{"cell_type":"markdown","id":"7Dw3xNSv2Im7","metadata":{"id":"7Dw3xNSv2Im7"},"source":["### Para guardar y cargar las respuestas del modelo"]},{"cell_type":"code","execution_count":null,"id":"4KkUn2zl2NGh","metadata":{"id":"4KkUn2zl2NGh"},"outputs":[],"source":["import json\n","\n","def guardar_datos_json(lista_de_listas, nombre_archivo):\n","  \"\"\"\n","  Guarda una lista de listas en un archivo de texto en formato JSON.\n","\n","  Args:\n","    lista_de_listas: La lista de listas a guardar.\n","    nombre_archivo: El nombre del archivo donde se guardará.\n","  \"\"\"\n","  try:\n","    os.makedirs(os.path.dirname(nombre_archivo), exist_ok=True)\n","\n","    with open(nombre_archivo, 'w', encoding='utf-8') as f:\n","      json.dump(lista_de_listas, f, ensure_ascii=False, indent=4)\n","    print(f\"Outputs guardados exitosamente en '{nombre_archivo}'\")\n","  except Exception as e:\n","    print(f\"Error al guardar outputs: {e}\")\n","\n","def cargar_datos_json(nombre_archivo):\n","  \"\"\"\n","  Carga una lista de listas desde un archivo de texto en formato JSON.\n","\n","  Args:\n","    nombre_archivo: El nombre del archivo desde donde se cargará.\n","\n","  Returns:\n","    La lista de listas cargada, o None si hay un error.\n","  \"\"\"\n","  try:\n","    with open(nombre_archivo, 'r', encoding='utf-8') as f:\n","      lista_de_listas = json.load(f)\n","    print(f\"Outputs cargados exitosamente desde '{nombre_archivo}'\")\n","    return lista_de_listas\n","  except FileNotFoundError:\n","    print(f\"Error: El archivo '{nombre_archivo}' no fue encontrado.\")\n","    return None\n","  except json.JSONDecodeError:\n","    print(f\"Error: El archivo '{nombre_archivo}' no es un JSON válido.\")\n","    return None\n","  except Exception as e:\n","    print(f\"Error al cargar outputs: {e}\")\n","    return None"]},{"cell_type":"markdown","source":["### Para obtener las _n_ películas más populares"],"metadata":{"id":"OcjD46UQetUi"},"id":"OcjD46UQetUi"},{"cell_type":"code","source":["def get_n_most_popular(conversations, n):\n","  movie_likes = {}\n","  for u in conversations:\n","    user = u[0]\n","    user_data = user_data_map[user]\n","    convs = user_data.get(\"Conversation\", [])\n","    for c in convs:\n","      user_likes = list(c.values())[0][\"user_likes\"]\n","      for m in user_likes:\n","        try:\n","          movie_likes[m] += 1\n","        except:\n","          movie_likes[m] = 1\n","\n","  sorted_movie_likes = dict(sorted(movie_likes.items(), key=lambda item: item[1], reverse=True))\n","  top_n_movies = dict(list(sorted_movie_likes.items())[:n])\n","  top_n_movie_names = [item_map[m] for m in list(top_n_movies.keys())]\n","\n","  return top_n_movie_names"],"metadata":{"id":"9IQv1Fwxdbew"},"execution_count":null,"outputs":[],"id":"9IQv1Fwxdbew"},{"cell_type":"markdown","source":["### Para preparar los datos para el entrenamiento, ya sea few-shot o fine-tuning"],"metadata":{"id":"yCLSJ3xk6j4V"},"id":"yCLSJ3xk6j4V"},{"cell_type":"code","source":["import random\n","\n","def prepare_training_data(rand_conversations):\n","\n","    training_data = []\n","\n","    base_ctx = (\n","        \"You are a Movie Recommendation System.\"\n","        \"Generate a numbered list of 10 Movies. \\n\"\n","        \"RULES: \\n\"\n","        \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","        \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","        \"c) You MUST recommend 10 movies, nothing more, nothing less. \\n\"\n","        \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","        \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","    )\n","\n","    for conversation_data in rand_conversations:\n","      msg = (\n","          \"\\nBased on the following conversation: \\n\"\n","          f\"{conversation_data[1]} \\n\\n\"\n","          \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","      )\n","\n","      input_text = base_ctx + msg\n","\n","      top_10_recommendations = []\n","\n","      top_10_recommendations.append(item_map[conversation_data[4][0]]) # ground truth numero 1\n","\n","      user_convs = list(conversation_data[2].get(\"Conversation\", []))\n","      user_conv_id = conversation_data[0]\n","      user_likes = list(user_convs[user_conv_id].values())[0][\"user_likes\"]\n","      user_likes = [item_map[m] for m in user_likes]\n","\n","      # Agregamos 3 películas que le gustan al usuario que no se mencionan en el extracto\n","      for m in user_likes:\n","        if m not in conversation_data[1] and len(top_10_recommendations) < 4:\n","          top_10_recommendations.append(m)\n","\n","      # Agregamos 4 películas de sus interacciones\n","      interactions = random.sample(conversation_data[5], min(10, len(conversation_data[5])))\n","      top_10_recommendations.extend(interactions[:4])\n","\n","      # Rellenamos con películas al azar de las 20 más populares\n","      while len(top_10_recommendations) < 10:\n","        random_movie = random.choice(top_20_movie_names)\n","        if random_movie not in top_10_recommendations:\n","          top_10_recommendations.append(random_movie)\n","\n","      output_text = \"\\n\".join([f\"{i+1}. {movie}\" for i, movie in enumerate(top_10_recommendations)])\n","      training_data.append({\"text\": f\"\"\"{input_text}{output_text}<|endoftext|>\"\"\"})\n","\n","    return training_data\n"],"metadata":{"id":"jcid-U6n6i4K"},"id":"jcid-U6n6i4K","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Instanciar todas las conversaciones en una lista"],"metadata":{"id":"f1neSitTkVCp"},"id":"f1neSitTkVCp"},{"cell_type":"code","source":["def load_conversations(conv_path):\n","\n","    all_conversations = {}\n","    current_conv_id = None\n","    current_conv_lines = []\n","\n","    with open(conv_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            line_stripped = line.strip()\n","\n","            # Si es un número (ID de conversación)\n","            if line_stripped.isdigit():\n","                # Guardar conversación anterior si existe\n","                if current_conv_id is not None:\n","                    all_conversations[current_conv_id] = \"\".join(current_conv_lines[2:])\n","\n","                # Iniciar nueva conversación\n","                current_conv_id = int(line_stripped)\n","                current_conv_lines = [line]\n","            else:\n","                # Agregar línea a la conversación actual\n","                if current_conv_id is not None:\n","                    current_conv_lines.append(line)\n","\n","    if current_conv_id is not None:\n","        all_conversations[current_conv_id] = current_conv_lines\n","\n","\n","    all_conversations[len(all_conversations)-1] = \"\".join(all_conversations[len(all_conversations)-1][2:])\n","\n","    return all_conversations\n","\n","all_conversations = load_conversations(Conversation_path)\n","n_conversations = len(all_conversations)\n","print(n_conversations)"],"metadata":{"id":"REHcyqT0kdKa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750963149523,"user_tz":240,"elapsed":74,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"c83e0faf-973a-412d-b027-34fc38a6f89c"},"id":"REHcyqT0kdKa","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10089\n"]}]},{"cell_type":"markdown","id":"14e7c87a","metadata":{"id":"14e7c87a"},"source":["### Separación de diálogos en train, test y val, cuidando que todas las conversaciones de un usuario en particular se encuentren en sólo train, sólo en test o sólo en val\n"]},{"cell_type":"code","execution_count":null,"id":"93ada744","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":349,"status":"ok","timestamp":1750963438877,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"93ada744","outputId":"6d7dfb46-7a8f-460d-ecf9-e08d5bb49e56"},"outputs":[{"output_type":"stream","name":"stdout","text":["Numero total de conversaciones procesadas: 10089\n","Usuarios en conjunto TEST: 297\n","Usuarios en conjunto VALIDACIÓN: 325\n","Usuarios en conjunto ENTRENAMIENTO: 2509\n","Total usuarios en conjuntos generados: 3131\n"]}],"source":["import random\n","import json\n","\n","path = './LLM_Redial/Movie/final_data.jsonl'\n","\n","# Cada entrada de final_data.jsonl se ve así\n","#{\n","#  \"A30Q8X8B1S3GGT\": {\n","#    \"history_interaction\": [...],\n","#    \"user_might_like\": [...],\n","#    \"Conversation\": [...]\n","#  }\n","#}\n","\n","with open(path, 'r', encoding='utf-8') as file:\n","    data = [json.loads(line) for line in file]\n","\n","# Construimos el mapa de usuario\n","user_data_map = {user_id: user_info for item in data for user_id, user_info in item.items()}\n","user_ids = list(user_data_map.keys())\n","n_conversations = sum(len(info[\"Conversation\"]) for info in user_data_map.values())\n","\n","train_len = n_conversations * 0.8\n","test_val_len = n_conversations * 0.1\n","\n","train_conv = []\n","test_conv = []\n","val_conv = []\n","used_users = []\n","\n","aux = []\n","\n","# Seleccionamos las conversaciones para el Testeo\n","while len(aux) < test_val_len:\n","    try:\n","        convs = []\n","        random.seed(42)\n","        available_users = list(set(user_ids) - set(used_users))\n","        user_id = random.choice(available_users)\n","\n","        used_users.append(user_id)\n","        user_data = user_data_map.get(user_id)\n","        user_conversations = user_data.get(\"Conversation\", [])\n","\n","        for conv in user_conversations:\n","            conversation_details = list(conv.values())[0]\n","            conversation_id = conversation_details[\"conversation_id\"]\n","            conversation = all_conversations[conversation_id]\n","            convs.append(conversation)\n","            aux.append(conversation)\n","\n","        test_conv.append([user_id, convs])\n","\n","    except ValueError:\n","        print(f\"[ERROR - TEST] ValueError al seleccionar usuario: {user_id}, conversación: {conversation_id}\")\n","\n","aux = []\n","\n","# Seleccionamos las conversaciones para la Validación\n","while len(aux) < test_val_len:\n","    try:\n","        convs = []\n","        random.seed(42)\n","        available_users = list(set(user_ids) - set(used_users))\n","        user_id = random.choice(available_users)\n","\n","        used_users.append(user_id)\n","        user_data = user_data_map.get(user_id)\n","        user_conversations = user_data.get(\"Conversation\", [])\n","\n","        for conv in user_conversations:\n","            conversation_details = list(conv.values())[0]\n","            conversation_id = conversation_details[\"conversation_id\"]\n","            conversation = all_conversations[conversation_id]\n","            convs.append(conversation)\n","            aux.append(conversation)\n","\n","        val_conv.append([user_id, convs])\n","\n","    except ValueError:\n","        print(f\"[ERROR - VALIDACIÓN] ValueError al seleccionar usuario: {user_id}, conversación: {conversation_id}\")\n","\n","# Asignamos el resto de usuarios a Training\n","for user_id in list(set(user_ids) - set(used_users)):\n","    try:\n","        convs = []\n","        user_data = user_data_map.get(user_id)\n","        user_conversations = user_data.get(\"Conversation\", [])\n","\n","        for conv in user_conversations:\n","            conversation_details = list(conv.values())[0]\n","            conversation_id = conversation_details[\"conversation_id\"]\n","            conversation = all_conversations[conversation_id]\n","            convs.append(conversation)\n","\n","        train_conv.append([user_id, convs])\n","\n","    except ValueError:\n","        print(f\"[ERROR - ENTRENAMIENTO] ValueError al procesar usuario: {user_id}, conversación: {conversation_id}\")\n","\n","total = len(test_conv) + len(val_conv) + len(train_conv)\n","\n","print(f\"Numero total de conversaciones procesadas: {n_conversations}\")\n","print(f\"Usuarios en conjunto TEST: {len(test_conv)}\")\n","print(f\"Usuarios en conjunto VALIDACIÓN: {len(val_conv)}\")\n","print(f\"Usuarios en conjunto ENTRENAMIENTO: {len(train_conv)}\")\n","print(f\"Total usuarios en conjuntos generados: {total}\")"]},{"cell_type":"markdown","source":["### Instanciar las 50 películas más populares"],"metadata":{"id":"nZj7FvNk7Z5g"},"id":"nZj7FvNk7Z5g"},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpJiVHkQnWf8"},"outputs":[],"source":["n = 50\n","top_50_movie_names = get_n_most_popular(train_conv, n)"],"id":"dpJiVHkQnWf8"},{"cell_type":"markdown","id":"0426f46b","metadata":{"id":"0426f46b"},"source":["### Selección aleatoria de diálogos para testear"]},{"cell_type":"code","execution_count":null,"id":"aFWZECPmDrYK","metadata":{"id":"aFWZECPmDrYK"},"outputs":[],"source":["import random\n","\n","num_test_items = 100\n","\n","def extraer_dialogos(conversations, num_conversations):\n","  rand_conversations = []\n","  all = []\n","  used_convs = []\n","\n","  for conv in conversations:\n","    all.extend([f\"{conv[0]}:::{c}\" for c in conv[1]])\n","\n","  while len(rand_conversations) < num_conversations:\n","    rand_user = random.choice(list(set(all)^set(used_convs)))\n","    used_convs.append(rand_user)\n","    rand_user = rand_user.split(\":::\")\n","    user_id = rand_user[0]\n","    user_conversation  = rand_user[1]\n","    user_data = user_data_map[user_id]\n","    convs = user_data.get(\"Conversation\", [])\n","\n","    for i in range(len(convs)):\n","      conv_details = list(convs[i].values())[0]\n","      conversation = all_conversations[conv_details[\"conversation_id\"]]\n","\n","      if user_conversation == conversation:\n","        rand_user_conv_id = i\n","\n","    dialog = \"\\n\\n\".join(user_conversation.split(\"\\n\\n\")[:3])\n","    dialog_id = list(convs[rand_user_conv_id].values())[0][\"conversation_id\"]\n","    dialog_ground_truth = list(convs[rand_user_conv_id].values())[0][\"rec_item\"]\n","\n","    rand_user_interactions = user_data.get(\"history_interaction\", [])\n","    rand_user_interactions = [item_map[m] for m in rand_user_interactions]\n","    rand_conversations.append([\n","        rand_user_conv_id, # 0: Índice de conversación\n","        dialog, # 1: Texto del diálogo parcial\n","        user_data, # 2: Info estructurada del usuario\n","        dialog_id, # 3: ID de conversación\n","        dialog_ground_truth, # 4: Item recomendado como verdad\n","        rand_user_interactions  # 5: Interaccion aleatoria de usuario\n","    ])\n","\n","  return rand_conversations"]},{"cell_type":"code","execution_count":null,"id":"29c1fa03","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29c1fa03","executionInfo":{"status":"ok","timestamp":1750963588963,"user_tz":240,"elapsed":11217,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"132ad41c-3005-4d53-f6ce-3d17278ae264"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generar respuestas nuevas (1) o cargar respuestas anteriores (2)? 2\n","Cargando datos anteriores...\n","Ingresa el número de seed a cargar / guardar / sobreescribir: 1\n","Outputs cargados exitosamente desde '/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed1/rand_conversations.json'\n","Datos cargados desde: /content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed1/rand_conversations.json\n"]}],"source":["import random\n","import os\n","\n","# Elegimos si generamos respuestas nuevas o si cargamos datos ya generados\n","\n","eleccion = \"\"\n","\n","while eleccion not in [\"1\", \"2\"]:\n","  eleccion = input(\"Generar respuestas nuevas (1) o cargar respuestas anteriores (2)? \")\n","  if eleccion not in [\"1\", \"2\"]:\n","    print(\"Opción inválida. Por favor, elige 1 o 2.\\n\")\n","\n","print(\"Generando respuestas nuevas...\" if eleccion == \"1\" else \"Cargando datos anteriores...\")\n","\n","# Con seed nos referimos al archivo en el drive que buscamos crear para guardar los datos\n","# O el que ya existe para cargarlos (CUIDADO CON SOBREESCRIBIR SEEDS, REVISAR DE ANTEMANO)\n","\n","seed_num = input(\"Ingresa el número de seed a cargar / guardar / sobreescribir: \")\n","\n","output_dir = f\"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed{seed_num}\"\n","os.makedirs(output_dir, exist_ok=True)\n","output_path = f\"{output_dir}/rand_conversations.json\"\n","\n","if eleccion == \"1\":\n","    rand_conversations = extraer_dialogos(test_conv, num_test_items)\n","    guardar_datos_json(rand_conversations, output_path)\n","    print(f\"Datos guardados en: {output_path}\")\n","else:\n","    rand_conversations = cargar_datos_json(output_path)\n","    print(f\"Datos cargados desde: {output_path}\")"]},{"cell_type":"markdown","id":"guy_SY1wevkC","metadata":{"id":"guy_SY1wevkC"},"source":["# Generación de 20 listas de recomendación de 10 películas"]},{"cell_type":"markdown","id":"24214eec","metadata":{"id":"24214eec"},"source":["### Zero-Shot con interacciones históricas:"]},{"cell_type":"code","execution_count":null,"id":"kG3zS4s2Ci5n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9430,"status":"ok","timestamp":1750963634573,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"kG3zS4s2Ci5n","outputId":"a55a80cb-a33c-4b06-f16e-0f34a69c9103"},"outputs":[{"output_type":"stream","name":"stdout","text":["Outputs cargados exitosamente desde '/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed1/TinyLlama/zero_shot.json'\n"]}],"source":["import torch\n","from transformers import pipeline\n","import random\n","import time\n","\n","path = f\"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed{seed_num}/TinyLlama/zero_shot.json\"\n","\n","if eleccion == \"1\":\n","\n","  k = 20\n","  outputs_z_s = []\n","\n","  pipe = pipeline(\n","      \"text-generation\",\n","      model=model_name,\n","      device_map=\"auto\",\n","      torch_dtype=torch.float16,\n","  )\n","\n","  base_ctx = (\n","      \"You are a Movie Recommendation System.\"\n","      \"Generate a numbered list of 10 Movies. \\n\"\n","      \"RULES: \\n\"\n","      \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","      \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","      \"c) You MUST recommend 10 movies, nothing more, nothing less. \\n\"\n","      \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","      \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","  )\n","\n","  gen_kwargs = {\n","      \"max_new_tokens\": 200,\n","      \"do_sample\": True,\n","      \"temperature\": 0.7,\n","      \"top_k\": 50,\n","      \"top_p\": 0.95,\n","      \"pad_token_id\": pipe.tokenizer.eos_token_id  # Prevents warnings\n","  }\n","\n","  total_start_time = time.time()\n","\n","  for n in range(num_test_items):\n","      if torch.cuda.is_available():\n","          torch.cuda.empty_cache()\n","\n","      if n%5 == 0:\n","        start_time = time.time()\n","\n","      print(f\"Generating for test item {n+1}...\")\n","      outputs = []\n","\n","      # Build message once per test item\n","      msg = (\n","          \"\\nBased on the following conversation: \\n\"\n","          f\"{rand_conversations[n][1]} \\n\\n\"\n","          \"And the movies the user has previously interacted with: \\n\"\n","          f\"{random.sample(rand_conversations[n][5], min(10, len(rand_conversations[n][5])))}\\n\\n\"\n","          \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","      )\n","\n","      messages = [{\"role\": \"user\", \"content\": base_ctx + msg}]\n","      prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","      # Tokenize once per test item, not per generation\n","      inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","      # Batch generation option (much faster if memory allows)\n","      with torch.no_grad():\n","          iter_start_time = time.time()\n","          output_ids = pipe.model.generate(\n","              **inputs,\n","              num_return_sequences=k,\n","              **gen_kwargs\n","          )\n","          for output_id in output_ids:\n","              decoded = pipe.tokenizer.decode(output_id, skip_special_tokens=True)\n","              outputs.append(decoded)\n","          # print(f\"Iteration {i} of test item {n+1} generated in {time.time() - iter_start_time} seconds.\")\n","\n","      outputs_z_s.append(outputs)\n","\n","      if (n+1)%5 == 0:\n","        print(f\"Test items {n-3}-{n+1} generated in {(time.time() - start_time):.3f} seconds.\\n\")\n","\n","  total = time.time() - total_start_time\n","  print(f\"Total generation time: {int(total//60)} minutes and {int(total - total//60*60)} seconds.\\n\")\n","\n","  guardar_datos_json(outputs_z_s, path)\n","\n","elif eleccion == \"2\":\n","  outputs_z_s = cargar_datos_json(path)"]},{"cell_type":"markdown","id":"bpYn7Iyp3vJ4","metadata":{"id":"bpYn7Iyp3vJ4"},"source":["### Few-Shot con interacción histórica"]},{"cell_type":"code","source":["# ESTO QUEDA POR ARREGLAR, PASAR DATOS FEW-SHOT CON EL MISMO FORMATO DEL INPUT-RESPUESTA\n","\n","# Seleccionamos aleatoriamente los datos de Few-Shot desde Training\n","\n","few_shot_data = []\n","random.seed(42)\n","\n","sorted_train_conv = []\n","for user, texts in train_conv:\n","  ordered_text = sorted(texts, key=len)\n","  sorted_train_conv.append([user, ordered_text])\n","\n","sorted_train_conv.sort(key=lambda x: len(x[1][0]))\n","\n","few_shot_users = random.sample(sorted_train_conv[:500], 5)\n","\n","for u in few_shot_users:\n","  user_data = next((item[u[0]] for item in data if u[0] in item), None)\n","  user_interactions = user_data.get(\"history_interaction\", [])\n","  user_interactions = [item_map[m] for m in user_interactions]\n","\n","  conversation = min(u[1], key=len)\n","  conversation[conversation.index(\"User:\"):-2]\n","  convs = user_data.get(\"Conversation\", [])\n","  for c in convs:\n","    user_likes = list(c.values())[0][\"user_likes\"]\n","    user_dislikes = list(c.values())[0][\"user_dislikes\"]\n","    recs= list(c.values())[0][\"rec_item\"]\n","\n","    user_likes = [item_map[m] for m in user_likes]\n","    user_dislikes = [item_map[m] for m in user_dislikes]\n","    recs = [item_map[m] for m in recs]\n","  few_shot_data.append({\n","      # \"user_interactions\": user_interactions,\n","      \"conversation\": conversation,\n","      \"user_likes\": user_likes,\n","      \"user_dislikes\": user_dislikes,\n","      \"recs\": recs\n","  })\n","\n","  def contar_tokens(texto):\n","    print(len(tokenizer.encode(texto)))"],"metadata":{"id":"YzC6S3Hrt2dD"},"id":"YzC6S3Hrt2dD","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"woC8p7lVrgZW","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1979,"status":"ok","timestamp":1750964454067,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"woC8p7lVrgZW","outputId":"fc07c16a-c64f-4786-ceac-a245c96b28f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Outputs cargados exitosamente desde '/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed1/TinyLlama/few_shot.json'\n"]}],"source":["import torch\n","from transformers import pipeline\n","import random\n","import time\n","\n","path = f\"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed{seed_num}/TinyLlama/few_shot.json\"\n","\n","if eleccion == \"1\":\n","\n","  k = 20\n","  outputs_f_s = []\n","\n","  pipe = pipeline(\n","      \"text-generation\",\n","      model=model_name,\n","      device_map=\"auto\",\n","      torch_dtype=torch.float16,\n","  )\n","\n","  base_ctx = (\n","      \"You are a Movie Recommendation System.\"\n","      \"Generate a numbered list of 10 Movies. \\n\"\n","      \"RULES: \\n\"\n","      \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","      \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","      \"c) You MUST recommend 10 movies. \\n\"\n","      \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","      \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","  )\n","\n","  gen_kwargs = {\n","      \"max_new_tokens\": 180,\n","      \"do_sample\": True,\n","      \"temperature\": 0.7,\n","      \"top_k\": 50,\n","      \"top_p\": 0.95,\n","      \"pad_token_id\": pipe.tokenizer.eos_token_id  # Prevents warnings\n","  }\n","\n","  total_start_time = time.time()\n","\n","  for n in range(num_test_items):\n","      if torch.cuda.is_available():\n","          torch.cuda.empty_cache()\n","\n","      if n%5 == 0:\n","        start_time = time.time()\n","\n","      print(f\"Generating for test item {n+1}...\")\n","      outputs = []\n","\n","      # Build message once per test item\n","      msg = (\n","        \"\\nBased on these 4 examples: \\n\"\n","        f\"{few_shot_data[0]}\\n\"\n","        f\"{few_shot_data[1]}\\n\"\n","        f\"{few_shot_data[2]}\\n\"\n","        f\"{few_shot_data[3]}\\n\\n\"\n","        \"And based on the following conversation: \\n\"\n","        f\"{rand_conversations[n][1]} \\n\\n\"\n","        \"And the movies the user has previously interacted with: \\n\"\n","        f\"{random.sample(rand_conversations[n][5], min(10, len(rand_conversations[n][5])))}\\n\\n\"\n","        \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","    )\n","\n","      messages = [{\"role\": \"user\", \"content\": base_ctx + msg}]\n","      prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","      # Tokenize once per test item, not per generation\n","      inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","      # Batch generation option (much faster if memory allows)\n","      with torch.no_grad():\n","          iter_start_time = time.time()\n","          output_ids = pipe.model.generate(\n","              **inputs,\n","              num_return_sequences=k,\n","              **gen_kwargs\n","          )\n","          for output_id in output_ids:\n","              decoded = pipe.tokenizer.decode(output_id, skip_special_tokens=True)\n","              outputs.append(decoded)\n","          # print(f\"Iteration {i} of test item {n+1} generated in {time.time() - iter_start_time} seconds.\")\n","\n","      outputs_f_s.append(outputs)\n","\n","      if (n+1)%5 == 0:\n","        print(f\"Test items {n-3}-{n+1} generated in {(time.time() - start_time):.3f} seconds.\\n\")\n","\n","  total = time.time() - total_start_time\n","  print(f\"Total generation time: {int(total//60)} minutes and {int(total - total//60*60)} seconds.\\n\")\n","\n","  guardar_datos_json(outputs_f_s, path)\n","\n","elif eleccion == \"2\":\n","  outputs_f_s = cargar_datos_json(path)"]},{"cell_type":"markdown","id":"Mr5beh1_2nRo","metadata":{"id":"Mr5beh1_2nRo"},"source":["### Fine-Tuning"]},{"cell_type":"code","execution_count":null,"id":"wPVY25F87MvP","metadata":{"id":"wPVY25F87MvP"},"outputs":[],"source":["from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","source":["entrenar = \"\"\n","\n","while entrenar not in [\"1\", \"2\"]:\n","  entrenar = input(\"Entrenar desde cero (1) o cargar modelo pre-entrenado (2)? \")\n","\n","  if entrenar not in [\"1\", \"2\"]:\n","    print(\"Opción inválida. Por favor, elige 1 o 2.\\n\")"],"metadata":{"id":"zkygHv4gGA_n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750964090391,"user_tz":240,"elapsed":3878,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"f88d32d6-d318-457b-940a-2840e6d3d1d2"},"id":"zkygHv4gGA_n","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Entrenar desde cero (1) o cargar modelo pre-entrenado (2)? 2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrvlZg_5DUo8","colab":{"base_uri":"https://localhost:8080/","height":355,"referenced_widgets":["f1a33ae15e854819af3d02649e8bcd4c","7e7b81f625774fc79631208bb620d3e9","30421a275ffb40199bd111644209d924","d67d264b961c446aa4b353bf64e4b35a","fe528a78955441fdb9952545509529bb","9263cefe3c174f07b439ab1ca9647b71","5fe3f633822649e39ac333cbb42b2097","b565442c09fb4d20b8cd1de3bfed54c7","08b5ebfd72914efe8a7ff0a4d4c5eceb","6dba2c2766d94d36b05b663ab81e1a19","d451deead68347a8a96be7c6262a3d08","b00c7a1645284b34ac320cbff216c079","6f080b480acb46aba9f8ada2f23bb328","ff3b5895aea246deb539ca28af753ce9","457eb056dae8402eb1cd3f9e2a43982b","b744d449380e41bfb1b365bb6d9594ec","862252f78d9143f0877a0e734cf96655","c2455ee7bf5b41ae8059eece6a3cc0a7","92fe12b97d704678970fdb94cd1258fa","bac87d1dc496429e9003c4f2043d1987","cc3b320b8e5a4a89bbeeb7a23d2bd97e","5d573944883f443e935d2c2e1a1ad9a7"]},"executionInfo":{"status":"ok","timestamp":1750964161292,"user_tz":240,"elapsed":35944,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"c3f85cb6-90f2-4c55-c268-064abd5aefb9"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-24-2051322881.py:3: UserWarning: WARNING: Unsloth should be imported before transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth import FastLanguageModel\n"]},{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n","==((====))==  Unsloth 2025.6.7: Fast Llama patching. Transformers: 4.52.4.\n","   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/762M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1a33ae15e854819af3d02649e8bcd4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b00c7a1645284b34ac320cbff216c079"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["unsloth/tinyllama-chat-bnb-4bit does not have a padding token! Will use pad_token = <unk>.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.6.7 patched 22 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"]}],"source":["from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel, PeftConfig\n","from unsloth import FastLanguageModel\n","from trl import SFTTrainer\n","from datasets import Dataset\n","import torch\n","\n","fine_tune_path = f\"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed{seed_num}/TinyLlama/fine-tuning/R3\"\n","\n","if entrenar == \"1\":\n","\n","  fine_tune_convs = extraer_dialogos(train_conv, 4000)\n","  fine_tune_convs_val = extraer_dialogos(val_conv, min(1000, len(val_conv)))\n","  fine_tune_data = preparar_datos_fine_tuning(fine_tune_convs)\n","  fine_tune_data_val = preparar_datos_fine_tuning(fine_tune_convs_val)\n","\n","  model, tokenizer = FastLanguageModel.from_pretrained(\n","      model_name = model_name,\n","      max_seq_length = 2048,\n","      dtype = None,\n","      load_in_4bit = True,\n","      trust_remote_code = True,\n","  )\n","\n","  model = FastLanguageModel.get_peft_model(\n","      model,\n","      r = 16,\n","      target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n","      lora_alpha = 16,\n","      lora_dropout = 0.1,\n","      bias = \"none\",\n","      use_gradient_checkpointing = True,\n","      random_state = 1234,\n","  )\n","\n","  train_dataset = Dataset.from_list(fine_tune_data)\n","  val_dataset = Dataset.from_list(fine_tune_data_val)\n","\n","  data_collator = DataCollatorForLanguageModeling(\n","      tokenizer=tokenizer,\n","      mlm=False,  # False para TinyLlama/modelos como (causal LM)\n","      pad_to_multiple_of=8,  # Optimización para tensor cores\n","      return_tensors=\"pt\"\n","  )\n","\n","  training_args = TrainingArguments(\n","      output_dir=fine_tune_path,\n","      per_device_train_batch_size=4,        # batch por GPU\n","      gradient_accumulation_steps=8,        # acumular gradientes para simular un batch mayor\n","      warmup_steps = 50,\n","      max_steps = 500,                    # ~2-3 epochs para 4000 ejemplos\n","      learning_rate = 2e-4,               # Relativamente alto para LoRA\n","      bf16 = True,                        # Crucial para eficiencia\n","      logging_steps = 25,\n","      optim = \"adamw_8bit\",               # Optimizador eficiente\n","      weight_decay = 0.01,\n","      lr_scheduler_type = \"cosine\",\n","      seed = 1234,\n","      save_steps = 100,\n","      eval_steps = 100,\n","      eval_strategy = \"steps\",\n","      load_best_model_at_end = True,\n","      metric_for_best_model = \"eval_loss\",\n","      greater_is_better = False,\n","  )\n","\n","  trainer = SFTTrainer(\n","      model = model,\n","      tokenizer = tokenizer,\n","      train_dataset = train_dataset,\n","      eval_dataset = val_dataset,\n","      data_collator=data_collator,\n","      dataset_text_field = \"text\",\n","      max_seq_length = 2048,\n","      dataset_num_proc = 2,\n","      args = training_args,\n","  )\n","\n","  trainer.train()\n","\n","  model.save_pretrained(fine_tune_path)\n","  tokenizer.save_pretrained(fine_tune_path)\n","\n","elif entrenar == \"2\":\n","  ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n","      model_name = fine_tune_path,\n","      max_seq_length = 2048,\n","      dtype = None,\n","      load_in_4bit = True,\n","  )\n","\n","  FastLanguageModel.for_inference(ft_model)"],"id":"LrvlZg_5DUo8"},{"cell_type":"code","execution_count":null,"id":"xiCufUvsQ5R-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":834,"status":"ok","timestamp":1750964219894,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"xiCufUvsQ5R-","outputId":"1a1bb121-eb8e-41ac-d0ec-9a133e806f39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Outputs cargados exitosamente desde '/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed1/TinyLlama/fine_tuned.json'\n"]}],"source":["from transformers import pipeline\n","\n","path = f\"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/seed{seed_num}/TinyLlama/fine_tuned.json\"\n","\n","if eleccion == \"1\":\n","\n","  k = 20\n","  outputs_ft_s = []\n","\n","  base_ctx = (\n","      \"You are a Movie Recommendation System.\"\n","      \"Generate a numbered list of 10 Movies. \\n\"\n","      \"RULES: \\n\"\n","      \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","      \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","      \"c) You MUST recommend 10 movies, nothing more, nothing less. \\n\"\n","      \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","      \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","  )\n","\n","  gen_kwargs = {\n","      \"max_new_tokens\": 200,\n","      \"do_sample\": True,\n","      \"temperature\": 0.7,\n","      \"top_k\": 50,\n","      \"top_p\": 0.95,\n","      \"pad_token_id\": ft_tokenizer.eos_token_id,\n","      \"eos_token_id\": ft_tokenizer.eos_token_id\n","    }\n","\n","  total_start_time = time.time()\n","\n","  for n in range(num_test_items):\n","      if torch.cuda.is_available():\n","          torch.cuda.empty_cache()\n","\n","      if n%5 == 0:\n","        start_time = time.time()\n","\n","      print(f\"Generating for test item {n+1}...\")\n","      outputs = []\n","\n","      # Build message once per test item\n","      msg = (\n","          \"\\nBased on the following conversation: \\n\"\n","          f\"{rand_conversations[n][1]} \\n\\n\"\n","          # \"And the movies the user has previously interacted with: \\n\"\n","          # f\"{random.sample(rand_conversations[n][5], min(10, len(rand_conversations[n][5])))}\\n\\n\"\n","          \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","      )\n","\n","      messages = [{\"role\": \"user\", \"content\": base_ctx + msg}]\n","      prompt = ft_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","      # Tokenize once per test item, not per generation\n","      inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","      # Batch generation option (much faster if memory allows)\n","      with torch.no_grad():\n","          iter_start_time = time.time()\n","          output_ids = ft_model.generate(\n","              **inputs,\n","              num_return_sequences=k,\n","              **gen_kwargs\n","          )\n","          for output_id in output_ids:\n","              decoded = ft_tokenizer.decode(output_id, skip_special_tokens=True)\n","              outputs.append(decoded)\n","          # print(f\"Iteration {i} of test item {n+1} generated in {time.time() - iter_start_time} seconds.\")\n","\n","      outputs_ft_s.append(outputs)\n","\n","      if (n+1)%5 == 0:\n","        print(f\"Test items {n-3}-{n+1} generated in {(time.time() - start_time):.3f} seconds.\\n\")\n","\n","  total = time.time() - total_start_time\n","  print(f\"Total generation time: {int(total//60)} minutes and {int(total - total//60*60)} seconds.\\n\")\n","\n","  guardar_datos_json(outputs_ft_s, path)\n","\n","elif eleccion == \"2\":\n","  outputs_ft_s = cargar_datos_json(path)"]},{"cell_type":"markdown","id":"kEEEo2aUjYsJ","metadata":{"id":"kEEEo2aUjYsJ"},"source":["## Evaluación"]},{"cell_type":"code","execution_count":null,"id":"5ckBE_kOquuF","metadata":{"id":"5ckBE_kOquuF"},"outputs":[],"source":["import numpy as np\n","import re\n","from sklearn.metrics import ndcg_score\n","import re\n","import html\n","from rapidfuzz import fuzz\n","\n","def normalizar_titulo(titulo):\n","    # Decode entidades HTML como &amp;\n","    titulo = html.unescape(titulo)\n","    # Minúsculas\n","    titulo = titulo.lower()\n","    # Eliminar puntuación excepto letras, números y &\n","    titulo = re.sub(r\"[^a-z0-9& ]+\", \"\", titulo)\n","    # Eliminar múltiples espacios\n","    titulo = re.sub(r\"\\s+\", \" \", titulo).strip()\n","    return titulo\n","\n","def comparar_titulos(t1, t2):\n","    t1 = normalizar_titulo(t1)\n","    t2 = normalizar_titulo(t2)\n","    return fuzz.token_set_ratio(t1, t2)\n","\n","# Funciones generadas por DeepSeek\n","def recall_at_k(generated_recommendations, ground_truth, k=10):\n","    hits = 0\n","    # Tomar las primeras K recomendaciones generadas\n","    top_k = generated_recommendations[:k]\n","    for e in top_k:\n","      if comparar_titulos(e, ground_truth[0]) > 80:\n","        hits = 1\n","\n","    # Evitar división por cero\n","    return hits\n","\n","def ndcg_at_k(generated_recommendations, ground_truth, k=10):\n","    # Crear una lista binaria de relevancia (1 si está en ground truth, 0 si no)\n","    relevance = [1 if item in ground_truth else 0 for item in generated_recommendations[:k]]\n","\n","    # Crear el \"ideal ranking\" (todas las relevantes primero)\n","    ideal_relevance = sorted(relevance, reverse=True)\n","\n","    # Calcular NDCG\n","    return ndcg_score([relevance], [ideal_relevance])\n"]},{"cell_type":"markdown","id":"W9dFcT15rGqg","metadata":{"id":"W9dFcT15rGqg"},"source":["### Recall@5 y NDCG@5:"]},{"cell_type":"code","execution_count":null,"id":"z8V6EPI4nvqU","metadata":{"id":"z8V6EPI4nvqU"},"outputs":[],"source":["z_s_rec_lists = []\n","i=1\n","for outputs in outputs_z_s:\n","  rec_lists = []\n","  for out in outputs:\n","      ans = format_ans(out[out.index(\"<|assistant|>\"):],10)\n","      rec_lists.append(format_ans(out[out.index(\"<|assistant|>\"):],10))\n","\n","  # print(rec_lists)\n","  z_s_rec_lists.append(rec_lists)"]},{"cell_type":"code","execution_count":null,"id":"KFtGD754n0ao","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1135,"status":"ok","timestamp":1750964377604,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"KFtGD754n0ao","outputId":"f9530469-d161-44c6-c97e-88a94763c0d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.12 0.03079388872450849\n","0.17 0.06764274720732359\n"]}],"source":["recall_zs_1_5 = 0\n","ndcg_zs_1_5 = 0\n","best_recall_zs_5 = 0.0\n","best_ndcg_zs_5 = 0.0\n","\n","for i in range(num_test_items):\n","  # Zero-shot sin sampling\n","  # print(i)\n","  if len(z_s_rec_lists[i][0]) > 1:\n","    recall_zs_1_5 += recall_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    ndcg_zs_1_5 += ndcg_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","\n","  # Zero-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","\n","  for l in z_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_zs_5 += best_r\n","  best_ndcg_zs_5 += best_n\n","print(recall_zs_1_5/num_test_items, ndcg_zs_1_5/num_test_items)\n","print(best_recall_zs_5/num_test_items, best_ndcg_zs_5/num_test_items)"]},{"cell_type":"code","execution_count":null,"id":"2AGf4D8nxxsF","metadata":{"id":"2AGf4D8nxxsF"},"outputs":[],"source":["f_s_rec_lists = []\n","i=1\n","for outputs in outputs_f_s:\n","  rec_lists = []\n","  for out in outputs:\n","      ans = format_ans(out[out.index(\"<|assistant|>\"):],10)\n","      rec_lists.append(format_ans(out[out.index(\"<|assistant|>\"):],10))\n","\n","  # print(rec_lists)\n","  f_s_rec_lists.append(rec_lists)"]},{"cell_type":"code","source":["print(len(f_s_rec_lists))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0xoQqv4PnyFL","executionInfo":{"status":"ok","timestamp":1750964622060,"user_tz":240,"elapsed":30,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"82749cb9-a28d-4c1a-ed42-60e690f0d3a9"},"id":"0xoQqv4PnyFL","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["50\n"]}]},{"cell_type":"code","execution_count":null,"id":"hzbGGXLXxbXH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":560,"status":"ok","timestamp":1750964641118,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"hzbGGXLXxbXH","outputId":"8f849714-2472-4ae8-9e2a-43921d9cc873"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0 0.0\n","0.01 0.0\n"]}],"source":["recall_fs_1_5 = 0\n","ndcg_fs_1_5 = 0\n","best_recall_fs_5 = 0.0\n","best_ndcg_fs_5 = 0.0\n","\n","for i in range(num_test_items//2):\n","  # Few-shot sin sampling\n","  # print(f\"\\n{f_s_rec_lists[i][0]}\")\n","  # print([item_map[m] for m in rand_conversations[i][4]])\n","  if len(f_s_rec_lists[i][0]) > 1:\n","    recall_fs_1_5 += recall_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    ndcg_fs_1_5 += ndcg_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","\n","  # Few-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in f_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fs_5 += best_r\n","  best_ndcg_fs_5 += best_n\n","print(recall_fs_1_5/num_test_items, ndcg_fs_1_5/num_test_items)\n","print(best_recall_fs_5/num_test_items, best_ndcg_fs_5/num_test_items)"]},{"cell_type":"code","source":["ft_s_rec_lists = []\n","i=1\n","for outputs in outputs_ft_s:\n","  rec_lists = []\n","  for out in outputs:\n","      ans = format_ans(out[out.index(\"<|assistant|>\"):],10)\n","      rec_lists.append(format_ans(out[out.index(\"<|assistant|>\"):],10))\n","\n","  # print(rec_lists)\n","  ft_s_rec_lists.append(rec_lists)"],"metadata":{"id":"fcbfXEatWRNQ"},"id":"fcbfXEatWRNQ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"zQ1TgaF3RSd4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1135,"status":"ok","timestamp":1750964390230,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"zQ1TgaF3RSd4","outputId":"d5b37ce6-cf00-497f-c687-2c35a8655aa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.11 0.08743509370014568\n","0.2 0.15230624149734415\n"]}],"source":["recall_fts_1_5 = 0\n","ndcg_fts_1_5 = 0\n","best_recall_fts_5 = 0.0\n","best_ndcg_fts_5 = 0.0\n","\n","for i in range(num_test_items):\n","  # Few-shot sin sampling\n","  # print(f\"\\n{ft_s_rec_lists[i][0]}\")\n","  # print([item_map[m] for m in rand_conversations[i][4]])\n","  if len(ft_s_rec_lists[i][0]) > 1:\n","    recall_fts_1_5 += recall_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    ndcg_fts_1_5 += ndcg_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","\n","  # Few-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in ft_s_rec_lists[i]:\n","    # print(l)\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fts_5 += best_r\n","  best_ndcg_fts_5 += best_n\n","print(recall_fts_1_5/num_test_items, ndcg_fts_1_5/num_test_items)\n","print(best_recall_fts_5/num_test_items, best_ndcg_fts_5/num_test_items)"]},{"cell_type":"markdown","id":"qmki7Uk51XRq","metadata":{"id":"qmki7Uk51XRq"},"source":["### Recall@10 y NDCG@10:"]},{"cell_type":"markdown","id":"_vUG0JzS27dP","metadata":{"id":"_vUG0JzS27dP"},"source":["Zero-Shot:"]},{"cell_type":"code","execution_count":null,"id":"V2-5q7OS2658","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1192,"status":"ok","timestamp":1750964398224,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"V2-5q7OS2658","outputId":"74a4a07d-0eb8-4083-82c3-fc26ffcafc3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.14 0.031455116214975126\n","0.22 0.07299769145246339\n"]}],"source":["recall_zs_1_10 = 0\n","ndcg_zs_1_10 = 0\n","best_recall_zs_10 = 0.0\n","best_ndcg_zs_10 = 0.0\n","\n","for i in range(num_test_items):\n","  # Zero-shot sin sampling\n","  if len(z_s_rec_lists[i][0]) > 1:\n","    recall_zs_1_10 += recall_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    ndcg_zs_1_10 += ndcg_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","\n","  # Zero-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in z_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_zs_10 += best_r\n","  best_ndcg_zs_10 += best_n\n","print(recall_zs_1_10/num_test_items, ndcg_zs_1_10/num_test_items)\n","print(best_recall_zs_10/num_test_items, best_ndcg_zs_10/num_test_items)"]},{"cell_type":"markdown","id":"TfxN0Sbb3MQU","metadata":{"id":"TfxN0Sbb3MQU"},"source":["Few-Shot"]},{"cell_type":"code","execution_count":null,"id":"eTt6k8Rm3NAE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":529,"status":"ok","timestamp":1750189901001,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"eTt6k8Rm3NAE","outputId":"424c38ba-76bf-4e2a-faf8-332ca69166b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.06 0.015749152613725975\n","0.24 0.07094383904259266\n"]}],"source":["recall_fs_1_10 = 0\n","ndcg_fs_1_10 = 0\n","best_recall_fs_10 = 0.0\n","best_ndcg_fs_10 = 0.0\n","\n","for i in range(num_test_items):\n","  # Few-shot sin sampling\n","  if len(f_s_rec_lists[i][0]) > 1:\n","    recall_fs_1_10 += recall_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    ndcg_fs_1_10 += ndcg_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","\n","  # Few-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in f_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fs_10 += best_r\n","  best_ndcg_fs_10 += best_n\n","print(recall_fs_1_10/num_test_items, ndcg_fs_1_10/num_test_items)\n","print(best_recall_fs_10/num_test_items, best_ndcg_fs_10/num_test_items)"]},{"cell_type":"code","execution_count":null,"id":"8bb9hE3VYSz6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1291,"status":"ok","timestamp":1750964403483,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"8bb9hE3VYSz6","outputId":"01d19c08-1d53-496e-b05b-df34689a818d"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.12 0.08444275460530175\n","0.24 0.15678200754544513\n"]}],"source":["recall_fts_1_10 = 0\n","ndcg_fts_1_10 = 0\n","best_recall_fts_10 = 0.0\n","best_ndcg_fts_10 = 0.0\n","\n","for i in range(num_test_items):\n","  # Fine-tuned sin sampling\n","  if len(ft_s_rec_lists[i][0]) > 1:\n","    recall_fts_1_10 += recall_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    ndcg_fts_1_10 += ndcg_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","\n","  # Fine-tuned con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in ft_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fts_10 += best_r\n","  best_ndcg_fts_10 += best_n\n","print(recall_fts_1_10/num_test_items, ndcg_fts_1_10/num_test_items)\n","print(best_recall_fts_10/num_test_items, best_ndcg_fts_10/num_test_items)"]},{"cell_type":"code","execution_count":null,"id":"UzWXLtfi3XxD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1750194669474,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"UzWXLtfi3XxD","outputId":"eaa9ded4-f8e8-42dd-a918-86bb1e2b124e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Zero-Shot con interacciones históricas:\n","Sin sampling: Recall@5: 0.120, NDCG@5: 0.031, Recall@10: 0.140, NDCG@10: 0.031\n","Con sampling: Recall@5: 0.170, NDCG@5: 0.068, Recall@10: 0.220, NDCG@10: 0.073\n","\n","Fine-tuned sin interacciones históricas:\n","Sin sampling: Recall@5: 0.110, NDCG@5: 0.087, Recall@10: 0.120, NDCG@10: 0.084\n","Con sampling: Recall@5: 0.200, NDCG@5: 0.152, Recall@10: 0.240, NDCG@10: 0.157\n"]}],"source":["print(\"\\nZero-Shot con interacciones históricas:\")\n","print(f\"Sin sampling: Recall@5: {recall_zs_1_5/num_test_items:.3f}, NDCG@5: {ndcg_zs_1_5/num_test_items:.3f}, Recall@10: {recall_zs_1_10/num_test_items:.3f}, NDCG@10: {ndcg_zs_1_10/num_test_items:.3f}\")\n","print(f\"Con sampling: Recall@5: {best_recall_zs_5/num_test_items:.3f}, NDCG@5: {best_ndcg_zs_5/num_test_items:.3f}, Recall@10: {best_recall_zs_10/num_test_items:.3f}, NDCG@10: {best_ndcg_zs_10/num_test_items:.3f}\")\n","\n","# print(\"\\nFew-Shot con interacciones históricas:\")\n","# print(f\"Sin sampling: Recall@5: {recall_fs_1_5/num_test_items:.3f}, NDCG@5: {ndcg_fs_1_5/num_test_items:.3f}, Recall@10: {recall_fs_1_10/num_test_items:.3f}, NDCG@10: {ndcg_fs_1_10/num_test_items:.3f}\")\n","# print(f\"Con sampling: Recall@5: {best_recall_fs_5/num_test_items:.3f}, NDCG@5: {best_ndcg_fs_5/num_test_items:.3f}, Recall@10: {best_recall_fs_10/num_test_items:.3f}, NDCG@10: {best_ndcg_fs_10/num_test_items:.3f}\")\n","\n","print(\"\\nFine-tuned sin interacciones históricas:\")\n","print(f\"Sin sampling: Recall@5: {recall_fts_1_5/num_test_items:.3f}, NDCG@5: {ndcg_fts_1_5/num_test_items:.3f}, Recall@10: {recall_fts_1_10/num_test_items:.3f}, NDCG@10: {ndcg_fts_1_10/num_test_items:.3f}\")\n","print(f\"Con sampling: Recall@5: {best_recall_fts_5/num_test_items:.3f}, NDCG@5: {best_ndcg_fts_5/num_test_items:.3f}, Recall@10: {best_recall_fts_10/num_test_items:.3f}, NDCG@10: {best_ndcg_fts_10/num_test_items:.3f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"JWy3oCvI4PsI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2104,"status":"ok","timestamp":1749224091155,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"JWy3oCvI4PsI","outputId":"b07de7da-b4f9-4e37-f646-db8b74cb7e4b"},"outputs":[{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import torch.nn.functional as F\n","import random\n","import numpy as np\n","\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Ahora con AutoModelForCausal a diferencia de la primera instancia\n","model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"id":"sIndZYYaOO60","metadata":{"id":"sIndZYYaOO60"},"outputs":[],"source":["import torch\n","import numpy as np\n","from collections import Counter\n","import re\n","import gc\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from scipy.stats import entropy\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer\n","\n","def extract_answer(text, question_text):\n","    \"\"\"Extrae la respuesta del texto generado\"\"\"\n","    # Buscar después de \"Answer:\" o similar\n","    patterns = [r\"Answer:\\s*(.+?)(?:\\n|$)\", r\"answer:\\s*(.+?)(?:\\n|$)\", r\"Answer is\\s*(.+?)(?:\\n|$)\"]\n","\n","    for pattern in patterns:\n","        match = re.search(pattern, text, re.IGNORECASE)\n","        if match:\n","            return match.group(1).strip()\n","\n","    # Si no encuentra patrón, tomar lo que viene después del prompt\n","    try:\n","        # Dividir por el texto de la pregunta y tomar la parte después\n","        parts = text.split(\"Answer:\")\n","        if len(parts) > 1:\n","            return parts[-1].strip().split('\\n')[0].strip()\n","    except:\n","        pass\n","\n","    return text.strip()\n","\n","def calculate_uncertainty_metrics_lightweight(outputs_per_paraphrase, paraphrases):\n","    \"\"\"\n","    Calcula métricas de incertidumbre usando Input Clarification Ensembling\n","\n","    Args:\n","        outputs_per_paraphrase: Lista de listas, cada sublista contiene outputs para una paráfrasis\n","        paraphrases: Lista de paráfrasis usadas\n","\n","    Returns:\n","        dict con métricas de incertidumbre\n","    \"\"\"\n","\n","    # 1. Extraer respuestas limpias\n","    all_answers = []\n","    answers_by_paraphrase = []\n","\n","    for i, outputs in enumerate(outputs_per_paraphrase):\n","        paraphrase_answers = []\n","        for output in outputs:\n","            answer = extract_answer(output, paraphrases[i])\n","            paraphrase_answers.append(answer)\n","            all_answers.append(answer)\n","        answers_by_paraphrase.append(paraphrase_answers)\n","\n","    # 2. Calcular frecuencias de respuestas\n","    answer_counts = Counter(all_answers)\n","    total_responses = len(all_answers)\n","\n","    # 3. INCERTIDUMBRE TOTAL (Shannon Entropy de todas las respuestas)\n","    probs = np.array(list(answer_counts.values())) / total_responses\n","    total_uncertainty = entropy(probs, base=2)  # bits\n","\n","    # 4. INCERTIDUMBRE ALEATORIA (promedio de entropías por paráfrasis)\n","    aleatoric_uncertainties = []\n","    for answers in answers_by_paraphrase:\n","        local_counts = Counter(answers)\n","        local_probs = np.array(list(local_counts.values())) / len(answers)\n","        if len(local_probs) > 1:\n","            aleatoric_uncertainties.append(entropy(local_probs, base=2))\n","        else:\n","            aleatoric_uncertainties.append(0.0)\n","\n","    aleatoric_uncertainty = np.mean(aleatoric_uncertainties)\n","\n","    # 5. INCERTIDUMBRE EPISTÉMICA (diferencia)\n","    epistemic_uncertainty = total_uncertainty - aleatoric_uncertainty\n","\n","    # 6. Métricas adicionales\n","    unique_answers = len(set(all_answers))\n","    most_common_answer, most_common_count = answer_counts.most_common(1)[0]\n","    confidence = most_common_count / total_responses\n","\n","    # 7. Consistencia entre paráfrasis\n","    consistency_scores = []\n","    for i in range(len(paraphrases)):\n","        for j in range(i+1, len(paraphrases)):\n","            # Comparar respuestas más frecuentes de cada paráfrasis\n","            answers_i = Counter(answers_by_paraphrase[i])\n","            answers_j = Counter(answers_by_paraphrase[j])\n","\n","            most_common_i = answers_i.most_common(1)[0][0] if answers_i else \"\"\n","            most_common_j = answers_j.most_common(1)[0][0] if answers_j else \"\"\n","\n","            # Similaridad simple (exacta o parcial)\n","            if most_common_i.lower().strip() == most_common_j.lower().strip():\n","                consistency_scores.append(1.0)\n","            else:\n","                # Similaridad parcial usando tokens comunes\n","                tokens_i = set(most_common_i.lower().split())\n","                tokens_j = set(most_common_j.lower().split())\n","                if tokens_i and tokens_j:\n","                    jaccard = len(tokens_i.intersection(tokens_j)) / len(tokens_i.union(tokens_j))\n","                    consistency_scores.append(jaccard)\n","                else:\n","                    consistency_scores.append(0.0)\n","\n","    consistency = np.mean(consistency_scores) if consistency_scores else 0.0\n","\n","    return {\n","        'total_uncertainty': float(total_uncertainty),\n","        'aleatoric_uncertainty': float(aleatoric_uncertainty),\n","        'epistemic_uncertainty': float(epistemic_uncertainty),\n","        'confidence': float(confidence),\n","        'unique_answers': int(unique_answers),\n","        'most_common_answer': str(most_common_answer),\n","        'consistency_across_paraphrases': float(consistency),\n","        'answer_distribution': {str(k): int(v) for k, v in answer_counts.items()},\n","        'num_paraphrases': len(paraphrases),\n","        'samples_per_paraphrase': len(outputs_per_paraphrase[0]) if outputs_per_paraphrase else 0\n","    }\n","\n","def calculate_logit_uncertainty_lightweight(model, tokenizer, paraphrases, device=\"cuda\", cleanup=True):\n","    \"\"\"\n","    Versión optimizada que limpia memoria agresivamente\n","    \"\"\"\n","    print(\"Calculando incertidumbre por logits (versión ligera)...\")\n","\n","    uncertainties_per_paraphrase = []\n","    logits_stats = []\n","\n","    for i, paraphrase in enumerate(paraphrases):\n","        print(f\"Procesando logits para paráfrasis {i+1}/{len(paraphrases)}\")\n","\n","        ctx = \"You are an oracle who only responds with short and concise answers.\"\n","        msg = f\"Answer the following question: {paraphrase}\\nAnswer:\"\n","        messages = [{\"role\": \"user\", \"content\": ctx + msg}]\n","\n","        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            next_token_logits = outputs.logits[0, -1, :].cpu()  # Mover a CPU inmediatamente\n","\n","            # Calcular probabilidades y entropía\n","            probs = torch.softmax(next_token_logits, dim=-1)\n","            uncertainty = entropy(probs.numpy(), base=2)\n","            uncertainties_per_paraphrase.append(float(uncertainty))\n","\n","            # Guardar solo estadísticas básicas, no los logits completos\n","            logits_stats.append({\n","                'mean': float(next_token_logits.mean()),\n","                'std': float(next_token_logits.std()),\n","                'max': float(next_token_logits.max()),\n","                'min': float(next_token_logits.min())\n","            })\n","\n","            # Limpiar memoria inmediatamente\n","            del outputs, next_token_logits, probs, inputs\n","            if cleanup:\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","    # Calcular métricas finales sin guardar arrays grandes\n","    mean_uncertainty = np.mean(uncertainties_per_paraphrase)\n","    std_uncertainty = np.std(uncertainties_per_paraphrase)\n","\n","    return {\n","        'logit_uncertainties_per_paraphrase': uncertainties_per_paraphrase,\n","        'mean_logit_uncertainty': float(mean_uncertainty),\n","        'std_logit_uncertainty': float(std_uncertainty),\n","        'logits_stats_summary': {\n","            'mean_of_means': float(np.mean([s['mean'] for s in logits_stats])),\n","            'mean_of_stds': float(np.mean([s['std'] for s in logits_stats])),\n","        }\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"vXg4zGJy4D6e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45589,"status":"ok","timestamp":1749232577003,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"vXg4zGJy4D6e","outputId":"3268e98d-e674-4820-8f71-d54775586785"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generando respuestas...\n","Procesando paráfrasis 1/10: What year did the Berlin Wall fall?\n","Procesando paráfrasis 2/10: When did the Berlin Wall come down?\n","Procesando paráfrasis 3/10: In which year was the Berlin Wall demolished?\n","Procesando paráfrasis 4/10: What year did the fall of the Berlin Wall occur?\n","Procesando paráfrasis 5/10: When was the Berlin Wall brought down?\n","Procesando paráfrasis 6/10: In what year did the Berlin Wall collapse?\n","Procesando paráfrasis 7/10: What year did they tear down the Berlin Wall?\n","Procesando paráfrasis 8/10: When did the destruction of the Berlin Wall happen?\n","Procesando paráfrasis 9/10: In which year did the Berlin Wall get demolished?\n","Procesando paráfrasis 10/10: What year marked the fall of the Berlin Wall?\n"]}],"source":["\n","# Configuración\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # o el modelo que uses\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Cargar modelo y tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Paráfrasis de la pregunta\n","paraphrases = [\n","    \"What year did the Berlin Wall fall?\",\n","    \"When did the Berlin Wall come down?\",\n","    \"In which year was the Berlin Wall demolished?\",\n","    \"What year did the fall of the Berlin Wall occur?\",\n","    \"When was the Berlin Wall brought down?\",\n","    \"In what year did the Berlin Wall collapse?\",\n","    \"What year did they tear down the Berlin Wall?\",\n","    \"When did the destruction of the Berlin Wall happen?\",\n","    \"In which year did the Berlin Wall get demolished?\",\n","    \"What year marked the fall of the Berlin Wall?\"\n","]\n","\n","# Generar respuestas con sampling\n","outputs_per_paraphrase = []\n","k = 5  # Número de samples por paráfrasis\n","\n","print(\"Generando respuestas...\")\n","for i, paraphrase in enumerate(paraphrases):\n","    print(f\"Procesando paráfrasis {i+1}/{len(paraphrases)}: {paraphrase}\")\n","\n","    outputs = []\n","    ctx = \"You are an oracle who only responds with short and concise answers.\"\n","    msg = f\"Answer the following question: {paraphrase}\\nAnswer:\"\n","    messages = [{\"role\": \"user\", \"content\": ctx + msg}]\n","\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","    for j in range(k):\n","        with torch.no_grad():\n","            output_ids = model.generate(\n","                **inputs,\n","                max_new_tokens=50,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_k=50,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id\n","            )\n","\n","            # Decodificar solo la parte nueva (sin el prompt)\n","            new_tokens = output_ids[0][inputs['input_ids'].shape[1]:]\n","            decoded = tokenizer.decode(new_tokens, skip_special_tokens=True)\n","            outputs.append(decoded)\n","\n","    outputs_per_paraphrase.append(outputs)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"WuCMGEixYJQF","metadata":{"id":"WuCMGEixYJQF"},"outputs":[],"source":["include_logits = True\n","cleanup_model = True"]},{"cell_type":"code","execution_count":null,"id":"J7IbxEarPAV0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4683,"status":"ok","timestamp":1749232589598,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"J7IbxEarPAV0","outputId":"28aff982-0159-42e2-ce68-a225bc5bfd33"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================================\n","CALCULANDO INCERTIDUMBRE (VERSIÓN OPTIMIZADA)...\n","==================================================\n","Respuestas extraídas:\n","Paráfrasis 1: [\"The Berlin Wall fell in 1989, on November 9, 1989, when East Germany's government announced the end of the wall separating East and West Berlin.\", 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the division of East and West Germany.', 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.', 'The Berlin Wall fell on November 9, 1989, which was 24 years ago.', 'The Berlin Wall fell on November 9, 1989, at 03:15 a.m. CET (Central European Time) in the early morning hours of November 9, 1989.']\n","Paráfrasis 2: ['The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.', \"The Berlin Wall came down on November 9, 1989, on the evening of that day. The Soviet Union's fall from power and the subsequent peaceful reunification of Germany marked a historic turning point in Europe's history\", 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union. The wall was constructed by the East German government to keep East Germans from escaping to', 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Eastern Bloc.', 'The Berlin Wall came down on November 9, 1989, after the Soviet Union had officially withdrawn its troops from the eastern part of Germany. This event marked the end of the Cold War and the collapse of the Eastern B']\n","Paráfrasis 3: ['The Berlin Wall was demolished on November 9, 1989.', 'In 1989, the Berlin Wall was demolished.', 'The question states that you are an oracle who only responds with short and concise answers. The answer to the question is in 1989, when the Berlin Wall was demolished.', 'In 1989, the Berlin Wall was demolished, which marked the end of the Cold War and the division of Europe.', 'The Berlin Wall was demolished in 1989, which was the year the wall was officially dismantled.']\n","Paráfrasis 4: ['The fall of the Berlin Wall occurred in 1989, not 1984 as stated in the given text.', 'The fall of the Berlin Wall occurred in 1989, which was 31 years ago.', 'The fall of the Berlin Wall occurred in 1989, which was also the year that the Soviet Union officially dismantled its military presence in the country.', 'The fall of the Berlin Wall occurred in 1989.', 'The fall of the Berlin Wall occurred in 1989, not in 1987.']\n","Paráfrasis 5: ['The Berlin Wall was brought down on November 9, 1989, by East German soldiers with a barricade made of concrete and barbed wire.', 'The Berlin Wall was brought down on November 9, 1989, after the Soviet Union allowed German citizens to travel freely for the first time in decades.', 'The Berlin Wall was brought down on November 9, 1989, when East German border guards opened fire on a group of demonstrators attempting to cross the border from West Berlin into East Germany. The event led to the collapse of', 'The Berlin Wall was brought down on November 9, 1989, by the East German government, following a series of protests and demonstrations that began on November 9, 1989, calling for the release of', 'The Berlin Wall was brought down on October 3, 1989, by a group of East German soldiers and civilians, led by East German General Walter Ulbricht, who had been sent to negotiate with the West German']\n","Paráfrasis 6: ['The Berlin Wall collapsed in 1989, at the beginning of the fall of the Berlin Wall.', 'The Berlin Wall collapsed in 1989, not in 1981 as stated in the given material.', 'The Berlin Wall collapsed in 1989, the year 1989.', 'The Berlin Wall collapsed on November 9, 1989, in the early hours of the morning.', 'The Berlin Wall collapsed on November 9, 1989, marking the end of the Cold War and the fall of the Soviet Union.']\n","Paráfrasis 7: ['The year when the Berlin Wall was torn down is not specified in the given text.', 'The question is not directly related to the given material.', 'that it was on November 9, 1989, when East Germany officially announced its independence from the Soviet Union and the wall was removed.', '1989.', 'The question does not specify a year for which they tear down the Berlin Wall.']\n","Paráfrasis 8: ['The destruction of the Berlin Wall happened on November 9, 1989, when Soviet forces removed the border barrier that had separated East and West Berlin for over 28 years.', 'The destruction of the Berlin Wall happened on November 9, 1989, when East German troops and police forcibly removed barbed wire and blockades from the eastern side of the city, allowing West Berliners to cross over', 'The destruction of the Berlin Wall happened on November 9, 1989, when East German officials unveiled a new wall that sealed off East Berlin from the rest of West Berlin. The wall had been built as a symbol of', 'The destruction of the Berlin Wall happened on November 9, 1989, when East Germany officially abandoned its border with the West and began to open its borders to West Germans. The Wall had been erected in 196', 'The destruction of the Berlin Wall happened on October 3, 1989, after the fall of the Soviet Union.']\n","Paráfrasis 9: ['The Berlin Wall got demolished in 1989, during the Cold War era.', 'The Berlin Wall was demolished on November 9, 1989, in the early hours of the morning after the German Democratic Republic (GDR) government announced its intention to dismantle it.', 'The Berlin Wall got demolished on August 15, 1961, in the early hours of the morning.', 'The Berlin Wall got demolished in 1989.', 'The Berlin Wall was demolished on November 9, 1989.']\n","Paráfrasis 10: [\"The fall of the Berlin Wall occurred in 1989, marking the end of the Cold War and the end of the Soviet Union's control over East Germany.\", 'The fall of the Berlin Wall occurred in 1989, not 1989 as stated in the given text.', 'The fall of the Berlin Wall occurred in 1989, which was a year before the question was asked.', 'The fall of the Berlin Wall occurred in 1989, not in 1984.', 'The fall of the Berlin Wall, which occurred in 1989, marked the end of the Cold War and the collapse of the Soviet Union. The fall of the wall was a turning point in history, signaling the end of a long']\n","Calculando incertidumbre por logits (versión ligera)...\n","Procesando logits para paráfrasis 1/10\n","Procesando logits para paráfrasis 2/10\n","Procesando logits para paráfrasis 3/10\n","Procesando logits para paráfrasis 4/10\n","Procesando logits para paráfrasis 5/10\n","Procesando logits para paráfrasis 6/10\n","Procesando logits para paráfrasis 7/10\n","Procesando logits para paráfrasis 8/10\n","Procesando logits para paráfrasis 9/10\n","Procesando logits para paráfrasis 10/10\n","Limpiando modelo de memoria...\n","✓ Modelo removido de memoria\n","\n","📊 MÉTRICAS DE INCERTIDUMBRE:\n","├── Incertidumbre Total: 5.604 bits\n","├── Incertidumbre Aleatoria: 2.322 bits\n","├── Incertidumbre Epistémica: 3.282 bits\n","├── Confianza: 0.040\n","├── Respuestas únicas: 49\n","├── Consistencia entre paráfrasis: 0.233\n","└── Respuesta más común: 'The Berlin Wall was demolished on November 9, 1989.'\n","\n","🔢 MÉTRICAS DE LOGITS:\n","├── Incertidumbre promedio: 0.926 bits\n","├── Desviación estándar: 0.528 bits\n","└── Media de logits: -1.714\n","\n","🧠 INTERPRETACIÓN:\n","├── El modelo tiene más incertidumbre sobre QUÉ responder\n","└── → Sugiere falta de conocimiento específico\n","├── Baja consistencia - posible confusión del modelo\n","{'uncertainty_metrics': {'total_uncertainty': 5.603856189774723, 'aleatoric_uncertainty': 2.3219280948873626, 'epistemic_uncertainty': 3.281928094887361, 'confidence': 0.04, 'unique_answers': 49, 'most_common_answer': 'The Berlin Wall was demolished on November 9, 1989.', 'consistency_across_paraphrases': 0.2328805773295286, 'answer_distribution': {\"The Berlin Wall fell in 1989, on November 9, 1989, when East Germany's government announced the end of the wall separating East and West Berlin.\": 1, 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the division of East and West Germany.': 1, 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.': 1, 'The Berlin Wall fell on November 9, 1989, which was 24 years ago.': 1, 'The Berlin Wall fell on November 9, 1989, at 03:15 a.m. CET (Central European Time) in the early morning hours of November 9, 1989.': 1, 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.': 1, \"The Berlin Wall came down on November 9, 1989, on the evening of that day. The Soviet Union's fall from power and the subsequent peaceful reunification of Germany marked a historic turning point in Europe's history\": 1, 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union. The wall was constructed by the East German government to keep East Germans from escaping to': 1, 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Eastern Bloc.': 1, 'The Berlin Wall came down on November 9, 1989, after the Soviet Union had officially withdrawn its troops from the eastern part of Germany. This event marked the end of the Cold War and the collapse of the Eastern B': 1, 'The Berlin Wall was demolished on November 9, 1989.': 2, 'In 1989, the Berlin Wall was demolished.': 1, 'The question states that you are an oracle who only responds with short and concise answers. The answer to the question is in 1989, when the Berlin Wall was demolished.': 1, 'In 1989, the Berlin Wall was demolished, which marked the end of the Cold War and the division of Europe.': 1, 'The Berlin Wall was demolished in 1989, which was the year the wall was officially dismantled.': 1, 'The fall of the Berlin Wall occurred in 1989, not 1984 as stated in the given text.': 1, 'The fall of the Berlin Wall occurred in 1989, which was 31 years ago.': 1, 'The fall of the Berlin Wall occurred in 1989, which was also the year that the Soviet Union officially dismantled its military presence in the country.': 1, 'The fall of the Berlin Wall occurred in 1989.': 1, 'The fall of the Berlin Wall occurred in 1989, not in 1987.': 1, 'The Berlin Wall was brought down on November 9, 1989, by East German soldiers with a barricade made of concrete and barbed wire.': 1, 'The Berlin Wall was brought down on November 9, 1989, after the Soviet Union allowed German citizens to travel freely for the first time in decades.': 1, 'The Berlin Wall was brought down on November 9, 1989, when East German border guards opened fire on a group of demonstrators attempting to cross the border from West Berlin into East Germany. The event led to the collapse of': 1, 'The Berlin Wall was brought down on November 9, 1989, by the East German government, following a series of protests and demonstrations that began on November 9, 1989, calling for the release of': 1, 'The Berlin Wall was brought down on October 3, 1989, by a group of East German soldiers and civilians, led by East German General Walter Ulbricht, who had been sent to negotiate with the West German': 1, 'The Berlin Wall collapsed in 1989, at the beginning of the fall of the Berlin Wall.': 1, 'The Berlin Wall collapsed in 1989, not in 1981 as stated in the given material.': 1, 'The Berlin Wall collapsed in 1989, the year 1989.': 1, 'The Berlin Wall collapsed on November 9, 1989, in the early hours of the morning.': 1, 'The Berlin Wall collapsed on November 9, 1989, marking the end of the Cold War and the fall of the Soviet Union.': 1, 'The year when the Berlin Wall was torn down is not specified in the given text.': 1, 'The question is not directly related to the given material.': 1, 'that it was on November 9, 1989, when East Germany officially announced its independence from the Soviet Union and the wall was removed.': 1, '1989.': 1, 'The question does not specify a year for which they tear down the Berlin Wall.': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when Soviet forces removed the border barrier that had separated East and West Berlin for over 28 years.': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when East German troops and police forcibly removed barbed wire and blockades from the eastern side of the city, allowing West Berliners to cross over': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when East German officials unveiled a new wall that sealed off East Berlin from the rest of West Berlin. The wall had been built as a symbol of': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when East Germany officially abandoned its border with the West and began to open its borders to West Germans. The Wall had been erected in 196': 1, 'The destruction of the Berlin Wall happened on October 3, 1989, after the fall of the Soviet Union.': 1, 'The Berlin Wall got demolished in 1989, during the Cold War era.': 1, 'The Berlin Wall was demolished on November 9, 1989, in the early hours of the morning after the German Democratic Republic (GDR) government announced its intention to dismantle it.': 1, 'The Berlin Wall got demolished on August 15, 1961, in the early hours of the morning.': 1, 'The Berlin Wall got demolished in 1989.': 1, \"The fall of the Berlin Wall occurred in 1989, marking the end of the Cold War and the end of the Soviet Union's control over East Germany.\": 1, 'The fall of the Berlin Wall occurred in 1989, not 1989 as stated in the given text.': 1, 'The fall of the Berlin Wall occurred in 1989, which was a year before the question was asked.': 1, 'The fall of the Berlin Wall occurred in 1989, not in 1984.': 1, 'The fall of the Berlin Wall, which occurred in 1989, marked the end of the Cold War and the collapse of the Soviet Union. The fall of the wall was a turning point in history, signaling the end of a long': 1}, 'num_paraphrases': 10, 'samples_per_paraphrase': 5}, 'logit_metrics': {'logit_uncertainties_per_paraphrase': [0.8899151086807251, 0.4174751937389374, 1.1662006378173828, 0.5137522220611572, 0.5372160077095032, 0.9283269047737122, 2.1689369678497314, 0.2583453357219696, 1.0907313823699951, 1.2852729558944702], 'mean_logit_uncertainty': 0.9256172716617584, 'std_logit_uncertainty': 0.5277849618054207, 'logits_stats_summary': {'mean_of_means': -1.714435636997223, 'mean_of_stds': 2.5638801574707033}}, 'analysis_params': {'num_paraphrases': 10, 'samples_per_paraphrase': 5, 'included_logits': True}}\n"]}],"source":["print(\"=\"*50)\n","print(\"CALCULANDO INCERTIDUMBRE (VERSIÓN OPTIMIZADA)...\")\n","print(\"=\"*50)\n","\n","# 1. Calcular métricas básicas de incertidumbre\n","uncertainty_metrics = calculate_uncertainty_metrics_lightweight(outputs_per_paraphrase, paraphrases)\n","\n","# 2. Calcular métricas de logits si se solicita\n","logit_metrics = None\n","if include_logits and model is not None and tokenizer is not None:\n","    logit_metrics = calculate_logit_uncertainty_lightweight(\n","        model, tokenizer, paraphrases, device, cleanup=True\n","    )\n","\n","# 3. LIMPIAR MODELO DE MEMORIA SI SE SOLICITA\n","if cleanup_model and model is not None:\n","    print(\"Limpiando modelo de memoria...\")\n","    del model\n","    if tokenizer is not None:\n","        del tokenizer\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    print(\"✓ Modelo removido de memoria\")\n","\n","# 4. Mostrar resultados\n","print(f\"\\n📊 MÉTRICAS DE INCERTIDUMBRE:\")\n","print(f\"├── Incertidumbre Total: {uncertainty_metrics['total_uncertainty']:.3f} bits\")\n","print(f\"├── Incertidumbre Aleatoria: {uncertainty_metrics['aleatoric_uncertainty']:.3f} bits\")\n","print(f\"├── Incertidumbre Epistémica: {uncertainty_metrics['epistemic_uncertainty']:.3f} bits\")\n","print(f\"├── Confianza: {uncertainty_metrics['confidence']:.3f}\")\n","print(f\"├── Respuestas únicas: {uncertainty_metrics['unique_answers']}\")\n","print(f\"├── Consistencia entre paráfrasis: {uncertainty_metrics['consistency_across_paraphrases']:.3f}\")\n","print(f\"└── Respuesta más común: '{uncertainty_metrics['most_common_answer']}'\")\n","\n","if logit_metrics:\n","    print(f\"\\n🔢 MÉTRICAS DE LOGITS:\")\n","    print(f\"├── Incertidumbre promedio: {logit_metrics['mean_logit_uncertainty']:.3f} bits\")\n","    print(f\"├── Desviación estándar: {logit_metrics['std_logit_uncertainty']:.3f} bits\")\n","    print(f\"└── Media de logits: {logit_metrics['logits_stats_summary']['mean_of_means']:.3f}\")\n","\n","# 5. Interpretación\n","print(f\"\\n🧠 INTERPRETACIÓN:\")\n","if uncertainty_metrics['epistemic_uncertainty'] > uncertainty_metrics['aleatoric_uncertainty']:\n","    print(\"├── El modelo tiene más incertidumbre sobre QUÉ responder\")\n","    print(\"└── → Sugiere falta de conocimiento específico\")\n","else:\n","    print(\"├── El modelo tiene más incertidumbre sobre CÓMO responder\")\n","    print(\"└── → Sugiere ambigüedad inherente en la pregunta\")\n","\n","if uncertainty_metrics['consistency_across_paraphrases'] > 0.8:\n","    print(\"├── Alta consistencia entre paráfrasis\")\n","elif uncertainty_metrics['consistency_across_paraphrases'] > 0.5:\n","    print(\"├── Consistencia moderada entre paráfrasis\")\n","else:\n","    print(\"├── Baja consistencia - posible confusión del modelo\")\n","\n","# DEVOLVER SOLO RESULTADOS LIGEROS\n","results = {\n","    'uncertainty_metrics': uncertainty_metrics,\n","    'logit_metrics': logit_metrics,\n","    'analysis_params': {\n","        'num_paraphrases': len(paraphrases),\n","        'samples_per_paraphrase': len(outputs_per_paraphrase[0]) if outputs_per_paraphrase else 0,\n","        'included_logits': include_logits\n","    }\n","}\n","\n","print(results)"]},{"cell_type":"code","execution_count":null,"id":"yvDBDRuyfE8a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84597,"status":"ok","timestamp":1749234579270,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"yvDBDRuyfE8a","outputId":"f266fa86-0eeb-483d-f4ee-103f15dc1246"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generando respuestas...\n","Procesando paráfrasis 1/10: What year did the Berlin Wall fall?\n","Procesando paráfrasis 2/10: When did the Berlin Wall come down?\n","Procesando paráfrasis 3/10: In which year was the Berlin Wall demolished?\n","Procesando paráfrasis 4/10: What year did the fall of the Berlin Wall occur?\n","Procesando paráfrasis 5/10: When was the Berlin Wall brought down?\n","Procesando paráfrasis 6/10: In what year did the Berlin Wall collapse?\n","Procesando paráfrasis 7/10: What year did they tear down the Berlin Wall?\n","Procesando paráfrasis 8/10: When did the destruction of the Berlin Wall happen?\n","Procesando paráfrasis 9/10: In which year did the Berlin Wall get demolished?\n","Procesando paráfrasis 10/10: What year marked the fall of the Berlin Wall?\n"]}],"source":["\n","# Configuración\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","ft_model = ft_model.to(device)\n","if ft_tokenizer.pad_token is None:\n","    ft_tokenizer.pad_token = ft_tokenizer.eos_token\n","\n","# Paráfrasis de la pregunta\n","paraphrases = [\n","    \"What year did the Berlin Wall fall?\",\n","    \"When did the Berlin Wall come down?\",\n","    \"In which year was the Berlin Wall demolished?\",\n","    \"What year did the fall of the Berlin Wall occur?\",\n","    \"When was the Berlin Wall brought down?\",\n","    \"In what year did the Berlin Wall collapse?\",\n","    \"What year did they tear down the Berlin Wall?\",\n","    \"When did the destruction of the Berlin Wall happen?\",\n","    \"In which year did the Berlin Wall get demolished?\",\n","    \"What year marked the fall of the Berlin Wall?\"\n","]\n","\n","# Generar respuestas con sampling\n","outputs_per_paraphrase = []\n","k = 5  # Número de samples por paráfrasis\n","\n","print(\"Generando respuestas...\")\n","for i, paraphrase in enumerate(paraphrases):\n","    print(f\"Procesando paráfrasis {i+1}/{len(paraphrases)}: {paraphrase}\")\n","\n","    outputs = []\n","    ctx = \"You are an oracle who only responds with short and concise answers.\"\n","    msg = f\"Answer the following question: {paraphrase}\\nAnswer:\"\n","    messages = [{\"role\": \"user\", \"content\": ctx + msg}]\n","\n","    prompt = ft_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","    for j in range(k):\n","        with torch.no_grad():\n","            output_ids = ft_model.generate(\n","                **inputs,\n","                max_new_tokens=50,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_k=50,\n","                top_p=0.95,\n","                pad_token_id=ft_tokenizer.eos_token_id\n","            )\n","\n","            # Decodificar solo la parte nueva (sin el prompt)\n","            new_tokens = output_ids[0][inputs['input_ids'].shape[1]:]\n","            decoded = ft_tokenizer.decode(new_tokens, skip_special_tokens=True)\n","            outputs.append(decoded)\n","\n","    outputs_per_paraphrase.append(outputs)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"hlD12V3kfeV-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4441,"status":"ok","timestamp":1749234627036,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"hlD12V3kfeV-","outputId":"1b92ed05-b050-416c-e6e7-b6c14d17085b"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================================\n","CALCULANDO INCERTIDUMBRE (VERSIÓN OPTIMIZADA)...\n","==================================================\n","Calculando incertidumbre por logits (versión ligera)...\n","Procesando logits para paráfrasis 1/10\n","Procesando logits para paráfrasis 2/10\n","Procesando logits para paráfrasis 3/10\n","Procesando logits para paráfrasis 4/10\n","Procesando logits para paráfrasis 5/10\n","Procesando logits para paráfrasis 6/10\n","Procesando logits para paráfrasis 7/10\n","Procesando logits para paráfrasis 8/10\n","Procesando logits para paráfrasis 9/10\n","Procesando logits para paráfrasis 10/10\n","Limpiando modelo de memoria...\n","✓ Modelo removido de memoria\n","\n","📊 MÉTRICAS DE INCERTIDUMBRE:\n","├── Incertidumbre Total: 5.644 bits\n","├── Incertidumbre Aleatoria: 2.322 bits\n","├── Incertidumbre Epistémica: 3.322 bits\n","├── Confianza: 0.020\n","├── Respuestas únicas: 50\n","├── Consistencia entre paráfrasis: 0.179\n","└── Respuesta más común: 'I don't have access to the latest news. However, I'm glad you asked. The Berlin Wall fell on August 14, 1961, and it became a symbol of the Cold War between the Soviet Union and'\n","\n","🔢 MÉTRICAS DE LOGITS:\n","├── Incertidumbre promedio: 3.444 bits\n","├── Desviación estándar: 0.306 bits\n","└── Media de logits: -2.010\n","\n","🧠 INTERPRETACIÓN:\n","├── El modelo tiene más incertidumbre sobre QUÉ responder\n","└── → Sugiere falta de conocimiento específico\n","├── Baja consistencia - posible confusión del modelo\n","{'uncertainty_metrics': {'total_uncertainty': 5.643856189774724, 'aleatoric_uncertainty': 2.3219280948873626, 'epistemic_uncertainty': 3.3219280948873617, 'confidence': 0.02, 'unique_answers': 50, 'most_common_answer': \"I don't have access to the latest news. However, I'm glad you asked. The Berlin Wall fell on August 14, 1961, and it became a symbol of the Cold War between the Soviet Union and\", 'consistency_across_paraphrases': 0.17905680999218387, 'answer_distribution': {\"I don't have access to the latest news. However, I'm glad you asked. The Berlin Wall fell on August 14, 1961, and it became a symbol of the Cold War between the Soviet Union and\": 1, 'The Berlin Wall was divided into four': 1, \"I don't have access to real-time updates. However, according to the article you provided, the Berlin Wall fell in 1989.\\n\\nAs for your question, the author mentioned that it was a historical event that sh\": 1, 'Yes, I can provide you with a response. According to the given material, the Berlin Wall fell in 1989.\\n\\nDetails:\\nThe Berlin Wall fell in 1989, after 28 years of opp': 1, 'I am not able to provide you with specific answers. However, you may find this article helpful: \\n\\n[insert article name]\\n[insert article description]\\n[insert article rating]\\n[insert article content]\\n\\nI hope': 1, \"I don't have access to current events or historical events. However, based on my knowledge, the Berlin Wall was built in 1961 to stop the flow of people and goods between West Germany and East Germany. It was finally broken\": 1, \"I'm not able to provide a specific answer as I don't have access to real-time information. However, I can tell you that the Berlin Wall came down in 1989.\\n\\nIf you need further assistance,\": 1, 'The Berlin Wall came down on November 9': 1, \"I'm sorry but I don't have access to current events or information about specific dates.\\n\\nHowever, based on your previous question, I assume that you're referring to the Berlin Wall. In 1989, East\": 1, \"I don't have the ability to provide long answers. However, I can tell you that the Berlin Wall came down on November 9, 1989. It was a symbol of the fall of the Berlin Wall and the end of\": 1, \"I don't have access to the latest information, but according to one source, the Berlin Wall was demolished in 1989.\\n\\nSource: https://www.time.com/time-magazine/article/0\": 1, 'I\\'m sorry, but I don\\'t have access to the most recent information. However, based on your previous response, I\\'d recommend watching \"A Little Night Music\" on Amazon. It\\'s a great movie, but I\\'': 1, \"I don't have access to real-time information. However, according to the provided text, it mentions that the Berlin Wall was demolished in 1989.\\n\\nSources:\\nhttps://www.youtube.com/\": 1, \"I'm not sure about that answer. Can you provide me with another question?\\n\\nAs for your question, the Berlin Wall demolition was in 1989.\\n\\nIf you need any more answers or specific details, feel\": 1, '1989': 1, 'Certainly! According to Wikipedia, The fall of the Berlin Wall occurred on November 9, 1989. The event marked the end of the Cold War and the reunification of Germany. It was a symbolic event that symbol': 1, 'The fall of the Berlin Wall occurred in 1989, during the Cold War era. The Berlin Wall was a barrier that separated East Germany from West Germany, which was a part of the Soviet Union. The wall was built by the': 1, \"I don't have access to historical events. Please provide me with the answer to your question.\\n\\nHistorical events:\\n\\n1. The fall of the Berlin Wall occurred in November 1989.\\n\\n2.\": 1, 'The fall of the Berlin Wall occurred in 1989, during the Cold War era. The wall was built to divide East Germany and West Germany, and it was the culmination of a decade-long struggle for control of the': 1, \"I don't have any information about specific events. However, I can provide you with a brief summary of the fall of the Berlin Wall. In 1989, the Soviet Union announced that it would dismantle its eastern b\": 1, 'Certainly! The Berlin Wall was brought down on November 9, 1989. This was a decisive moment in the history of the German Democratic Republic. It was the final seal on the collapse of the Soviet Union,': 1, \"I don't have access to current events. However, according to one source, the Berlin Wall was brought down on November 9, 1989. It was the last remaining border between East Germany and West Germany.\\n\\nS\": 1, 'I can provide you with a detailed answer. The Berlin Wall was brought down on November 9, 1989, when the East German government announced that it was dismantling the wall. The wall, which had separated East and': 1, \"I don't have any specific information about the Berlin Wall brought down. However, if you're looking for more details about it, here's a review:\\n\\nReview: The Berlin Wall came down in 1989\": 1, \"I'm sorry, but I don't have the capacity to provide you with a specific answer based on your previous question.\\n\\nIs there anything else you would like me to assist you with?\\n\\n[If yes, tell me the\": 1, 'The Berlin Wall was built in 1961 to separate East and West Berlin. It was deemed necessary to prevent the spread of communism. After the reunification of Germany in 1990, it became a symbol': 1, \"I'm afraid I don't have a definitive answer for you. However, according to various sources, the Berlin Wall collapse was in 1989.\\n\\nSource: https://www.youtube.com/watch?v\": 1, 'I can provide you with more detailed information about the Berlin Wall collapse. In 1989, the German government announced that it would remove the Berlin Wall, which had separated East and West Berlin since 1961. This was a': 1, \"I don't have access to current events. However, in 1989, the Berlin Wall was finally torn down, ending the Cold War.\\n\\nsource: https://en.wikipedia.org/wiki/Berlin_W\": 1, \"I don't have access to the latest information. However, according to the text you provided, you can find the answer in the given passage. In what year did the Berlin Wall collapse?\\n\\nBased on the given passage, the answer\": 1, 'The Berlin Wall was torn down in 1989.\\n\\nHere\\'s a related quote: \"The Berlin Wall was torn down in 1989. The Cold War was over.\" - from the movie \"The American President': 1, \"I can't provide you with specific answers. However, I can tell you that in the 1990s, the Berlin Wall was torn down by the German government.\\n\\nIn summary, the Berlin Wall was torn down by the\": 1, \"I'm sorry but I didn't hear you properly. Can you provide me with more information about the Berlin Wall?\\n\\nAs a child, I had a vivid imagination and loved stories of the Berlin Wall. I could picture a future\": 1, 'I do not have access to specific information about when the Berlin Wall was torn down. However, I can provide you with some similar answers to your question.\\n\\nIn 2007, the Berlin Wall was rebuilt, but it was': 1, 'I don\\'t have access to the latest news. However, I can provide you with a brief summary of a movie called \"The Trip to Italy\" which was released in 2014. It\\'s a comedy starring Steve': 1, \"I'm sorry but I don't have access to the exact date of the destruction of the Berlin Wall. However, according to the given text, the author mentioned that the destruction of the Berlin Wall happened in 1989.\": 1, \"I can't answer your question since I don't have the context of your previous question. However, in general, the destruction of the Berlin Wall was in 1989. The Berlin Wall was built in 1961\": 1, 'I do not have access to current events. However, according to some sources, the destruction of the Berlin Wall happened on August 14, 1989.\\n\\nSources:\\n1. New York Times - \"German': 1, \"I'm sorry, but I don't have the specific answer you're looking for. What would you like me to tell you?\\n\\nBy the way, I can provide you with a short answer. The destruction of the Berlin Wall\": 1, 'I\\'m sorry, but I don\\'t have access to the exact date of the destruction of the Berlin Wall. Please provide me with the relevant information.\\n\\n[User Provides: \"The Berlin Wall was demolished on November 9': 1, \"I'm sorry, but I don't have access to the most recent information. Can you provide me with the answer to your question?\\n\\nAs per my knowledge, the Berlin Wall was demolished on November 9, 19\": 1, \"I don't have access to specific historical events. However, based on the given text, it seems that the Berlin Wall got demolished in 1989. The Berlin Wall, also known as the East-West Berlin Wall, was\": 1, 'specific and relevant to the question': 1, 'Yes, I can provide you with a short and concise answer.The Berlin Wall was demolished in 1989.\\n\\nIf you need more detailed information, please let me know.\\n\\nThank you for your interest!': 1, \"I'm afraid I don't have information about the year you're referring to. Can you provide me with more details about the Berlin Wall and its demolition?\\n\\n>assistant|Sure! According to a reliable source,\": 1, \"Sure, I'm sorry for the oversight. The question you were looking for is: What year marked the fall of the Berlin Wall?\\n\\nAccording to the article you read, it was 1989. The\": 1, \"Certainly! The fall of the Berlin Wall was a significant event in the Cold War. On November 9, 1989, East Germany's leader, Erich Honecker, announced that the Wall would be dism\": 1, \"Sure, I'm sorry for the oversight. In 1989, the Berlin Wall was brought down by the people of East Germany. It took them a few years to do so, but it was a significant moment in\": 1, \"I'm afraid I don't have information on the year of the fall of the Berlin Wall. However, I can tell you that it was a significant event in German history.\\n\\nHere's another question you might be interested in:\": 1, \"I'm sorry but I don't have access to current events. However, I can provide you with a detailed answer based on the given context.\\n\\nThe given context includes the events of 1989. The Berlin Wall was\": 1}, 'num_paraphrases': 10, 'samples_per_paraphrase': 5}, 'logit_metrics': {'logit_uncertainties_per_paraphrase': [3.6031086444854736, 2.917271614074707, 3.672024726867676, 3.6897640228271484, 3.1129651069641113, 3.486865758895874, 3.339991807937622, 3.0483243465423584, 3.7215628623962402, 3.8511717319488525], 'mean_logit_uncertainty': 3.4443050622940063, 'std_logit_uncertainty': 0.30606880206364356, 'logits_stats_summary': {'mean_of_means': -2.0096726417541504, 'mean_of_stds': 2.6050027132034304}}, 'analysis_params': {'num_paraphrases': 10, 'samples_per_paraphrase': 5, 'included_logits': True}}\n"]}],"source":["print(\"=\"*50)\n","print(\"CALCULANDO INCERTIDUMBRE (VERSIÓN OPTIMIZADA)...\")\n","print(\"=\"*50)\n","\n","# 1. Calcular métricas básicas de incertidumbre\n","uncertainty_metrics = calculate_uncertainty_metrics_lightweight(outputs_per_paraphrase, paraphrases)\n","\n","# 2. Calcular métricas de logits si se solicita\n","logit_metrics = None\n","if include_logits and ft_model is not None and tokenizer is not None:\n","    logit_metrics = calculate_logit_uncertainty_lightweight(\n","        ft_model, tokenizer, paraphrases, device, cleanup=True\n","    )\n","\n","# 3. LIMPIAR MO.DELO DE MEMORIA SI SE SOLICITA\n","if cleanup_model and ft_model is not None:\n","    print(\"Limpiando modelo de memoria...\")\n","    del ft_model\n","    if tokenizer is not None:\n","        del tokenizer\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    print(\"✓ Modelo removido de memoria\")\n","\n","# 4. Mostrar resultados\n","print(f\"\\n📊 MÉTRICAS DE INCERTIDUMBRE:\")\n","print(f\"├── Incertidumbre Total: {uncertainty_metrics['total_uncertainty']:.3f} bits\")\n","print(f\"├── Incertidumbre Aleatoria: {uncertainty_metrics['aleatoric_uncertainty']:.3f} bits\")\n","print(f\"├── Incertidumbre Epistémica: {uncertainty_metrics['epistemic_uncertainty']:.3f} bits\")\n","print(f\"├── Confianza: {uncertainty_metrics['confidence']:.3f}\")\n","print(f\"├── Respuestas únicas: {uncertainty_metrics['unique_answers']}\")\n","print(f\"├── Consistencia entre paráfrasis: {uncertainty_metrics['consistency_across_paraphrases']:.3f}\")\n","print(f\"└── Respuesta más común: '{uncertainty_metrics['most_common_answer']}'\")\n","\n","if logit_metrics:\n","    print(f\"\\n🔢 MÉTRICAS DE LOGITS:\")\n","    print(f\"├── Incertidumbre promedio: {logit_metrics['mean_logit_uncertainty']:.3f} bits\")\n","    print(f\"├── Desviación estándar: {logit_metrics['std_logit_uncertainty']:.3f} bits\")\n","    print(f\"└── Media de logits: {logit_metrics['logits_stats_summary']['mean_of_means']:.3f}\")\n","\n","# 5. Interpretación\n","print(f\"\\n🧠 INTERPRETACIÓN:\")\n","if uncertainty_metrics['epistemic_uncertainty'] > uncertainty_metrics['aleatoric_uncertainty']:\n","    print(\"├── El modelo tiene más incertidumbre sobre QUÉ responder\")\n","    print(\"└── → Sugiere falta de conocimiento específico\")\n","else:\n","    print(\"├── El modelo tiene más incertidumbre sobre CÓMO responder\")\n","    print(\"└── → Sugiere ambigüedad inherente en la pregunta\")\n","\n","if uncertainty_metrics['consistency_across_paraphrases'] > 0.8:\n","    print(\"├── Alta consistencia entre paráfrasis\")\n","elif uncertainty_metrics['consistency_across_paraphrases'] > 0.5:\n","    print(\"├── Consistencia moderada entre paráfrasis\")\n","else:\n","    print(\"├── Baja consistencia - posible confusión del modelo\")\n","\n","# DEVOLVER SOLO RESULTADOS LIGEROS\n","results = {\n","    'uncertainty_metrics': uncertainty_metrics,\n","    'logit_metrics': logit_metrics,\n","    'analysis_params': {\n","        'num_paraphrases': len(paraphrases),\n","        'samples_per_paraphrase': len(outputs_per_paraphrase[0]) if outputs_per_paraphrase else 0,\n","        'included_logits': include_logits\n","    }\n","}\n","\n","print(results)"]},{"cell_type":"code","execution_count":null,"id":"k5Tmxon6lZOH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1630,"status":"ok","timestamp":1749235943997,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"k5Tmxon6lZOH","outputId":"1751abc1-a5f5-4d05-c48c-201d68c11082"},"outputs":[{"name":"stdout","output_type":"stream","text":["🎯 Limpieza específica para modelo LoRA...\n","   ├── Eliminando: tokenizer\n","   ├── Eliminando: pipe\n","   ├── Eliminando: inputs\n","   ├── Eliminando: output_ids\n","   ├── Eliminando: prompt\n","   ├── Eliminando: messages\n","   ├── Eliminando: ctx\n","   ├── Eliminando: msg\n","   ├── Eliminando: decoded\n","   ├── Eliminando: outputs\n","\n","📊 Variables mantenidas:\n","   ├── item_map: 9687 elementos\n","   ├── Conversation: 16935069 elementos\n","   ├── model_name: 34 elementos\n","   ├── outputs_f_s: 20 elementos\n","   ├── outputs_ft_s: 20 elementos\n","   ├── rand_conversations: 20 elementos\n","   ├── num_test_items: <class 'int'>\n","   ├── train_conv: 2512 elementos\n","   ├── test_conv: 619 elementos\n","   ├── num_test_items: <class 'int'>\n","   ├── few_shot_data: 5 elementos\n"]}],"source":["limpiar_y_guardar()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["205c6de8","ZAL_o9_wG1e5","tbkknIrVmsd_","SZkhMuOgm5eB","f1neSitTkVCp","14e7c87a","0426f46b","24214eec","bpYn7Iyp3vJ4","Mr5beh1_2nRo","kEEEo2aUjYsJ"],"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f1a33ae15e854819af3d02649e8bcd4c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e7b81f625774fc79631208bb620d3e9","IPY_MODEL_30421a275ffb40199bd111644209d924","IPY_MODEL_d67d264b961c446aa4b353bf64e4b35a"],"layout":"IPY_MODEL_fe528a78955441fdb9952545509529bb"}},"7e7b81f625774fc79631208bb620d3e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9263cefe3c174f07b439ab1ca9647b71","placeholder":"​","style":"IPY_MODEL_5fe3f633822649e39ac333cbb42b2097","value":"model.safetensors: 100%"}},"30421a275ffb40199bd111644209d924":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b565442c09fb4d20b8cd1de3bfed54c7","max":762453371,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08b5ebfd72914efe8a7ff0a4d4c5eceb","value":762453371}},"d67d264b961c446aa4b353bf64e4b35a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6dba2c2766d94d36b05b663ab81e1a19","placeholder":"​","style":"IPY_MODEL_d451deead68347a8a96be7c6262a3d08","value":" 762M/762M [00:03&lt;00:00, 379MB/s]"}},"fe528a78955441fdb9952545509529bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9263cefe3c174f07b439ab1ca9647b71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fe3f633822649e39ac333cbb42b2097":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b565442c09fb4d20b8cd1de3bfed54c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08b5ebfd72914efe8a7ff0a4d4c5eceb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6dba2c2766d94d36b05b663ab81e1a19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d451deead68347a8a96be7c6262a3d08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b00c7a1645284b34ac320cbff216c079":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f080b480acb46aba9f8ada2f23bb328","IPY_MODEL_ff3b5895aea246deb539ca28af753ce9","IPY_MODEL_457eb056dae8402eb1cd3f9e2a43982b"],"layout":"IPY_MODEL_b744d449380e41bfb1b365bb6d9594ec"}},"6f080b480acb46aba9f8ada2f23bb328":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_862252f78d9143f0877a0e734cf96655","placeholder":"​","style":"IPY_MODEL_c2455ee7bf5b41ae8059eece6a3cc0a7","value":"generation_config.json: 100%"}},"ff3b5895aea246deb539ca28af753ce9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_92fe12b97d704678970fdb94cd1258fa","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bac87d1dc496429e9003c4f2043d1987","value":124}},"457eb056dae8402eb1cd3f9e2a43982b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3b320b8e5a4a89bbeeb7a23d2bd97e","placeholder":"​","style":"IPY_MODEL_5d573944883f443e935d2c2e1a1ad9a7","value":" 124/124 [00:00&lt;00:00, 15.0kB/s]"}},"b744d449380e41bfb1b365bb6d9594ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"862252f78d9143f0877a0e734cf96655":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2455ee7bf5b41ae8059eece6a3cc0a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"92fe12b97d704678970fdb94cd1258fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bac87d1dc496429e9003c4f2043d1987":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc3b320b8e5a4a89bbeeb7a23d2bd97e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d573944883f443e935d2c2e1a1ad9a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}