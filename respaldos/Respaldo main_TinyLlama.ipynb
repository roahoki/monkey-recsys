{"cells":[{"cell_type":"markdown","id":"205c6de8","metadata":{"id":"205c6de8"},"source":["### G-Drive Setup"]},{"cell_type":"markdown","id":"G91y3WGR8JVS","metadata":{"id":"G91y3WGR8JVS"},"source":["Este notebook asume que se est√° ejecutando en Google Colab, y que el dataset `LLM-Redial` disponible en https://drive.google.com/drive/folders/1TIP4PFm9z0C4R4--KnHoWuiB1uK-dv5m se encuentra descargado en el drive del usuario."]},{"cell_type":"code","execution_count":null,"id":"Zpy6FRKqFfjT","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3124,"status":"ok","timestamp":1750191578214,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"Zpy6FRKqFfjT","outputId":"4188e5f5-801a-4f2b-e70e-80242237271c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","source":["### Instalaciones necesarias"],"metadata":{"id":"ZAL_o9_wG1e5"},"id":"ZAL_o9_wG1e5"},{"cell_type":"code","source":["%pip install unsloth[colab-new] xformers trl peft accelerate bitsandbytes rapidfuzz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"TcYxpu00GjeX","executionInfo":{"status":"ok","timestamp":1750191583249,"user_tz":240,"elapsed":4993,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"43e6ff5b-25ea-4e86-9708-26b760cc8b92"},"id":"TcYxpu00GjeX","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xformers in /usr/local/lib/python3.11/dist-packages (0.0.30)\n","Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.18.2)\n","Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n","Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (3.13.0)\n","Requirement already satisfied: unsloth[colab-new] in /usr/local/lib/python3.11/dist-packages (2025.6.2)\n","Requirement already satisfied: unsloth_zoo>=2025.6.1 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2025.6.1)\n","Requirement already satisfied: torch<=2.7.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.7.0)\n","Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (24.2)\n","Requirement already satisfied: tyro in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.9.24)\n","Requirement already satisfied: transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (4.52.4)\n","Requirement already satisfied: datasets>=3.4.1 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.6.0)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (2.0.2)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (3.20.3)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.33.0)\n","Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.1.9)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.33.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth[colab-new]) (0.22.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (4.14.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (9.5.1.17)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (0.6.3)\n","Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (2.26.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.11.1.6)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth[colab-new]) (75.2.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth[colab-new]) (0.70.15)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth[colab-new]) (1.1.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth[colab-new]) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth[colab-new]) (0.21.1)\n","Requirement already satisfied: cut_cross_entropy in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.1->unsloth[colab-new]) (25.1.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.1->unsloth[colab-new]) (11.2.1)\n","Requirement already satisfied: msgspec in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.6.1->unsloth[colab-new]) (0.19.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth[colab-new]) (8.7.0)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (13.9.4)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (1.7.2)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth[colab-new]) (4.4.3)\n","\u001b[33mWARNING: unsloth 2025.6.2 does not provide the extra 'triton'\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (3.11.15)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth[colab-new]) (2025.4.26)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth[colab-new]) (2.19.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (1.3.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth[colab-new]) (3.23.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<=2.7.0,>=2.4.0->unsloth[colab-new]) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth[colab-new]) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth[colab-new]) (1.20.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth[colab-new]) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth[colab-new]) (1.17.0)\n"]}]},{"cell_type":"code","execution_count":null,"id":"uop1TotjGGPP","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":997,"status":"ok","timestamp":1750191584293,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"uop1TotjGGPP","outputId":"9dbf1f96-82fa-42b8-8cc4-3c6c1139faaa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/Tools.py'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import shutil\n","import os\n","\n","source_path = '/content/gdrive/MyDrive/Proyecto LLMonkeys/Tools.py'\n","destination_path = '/content/Tools.py'\n","\n","shutil.copy(source_path, destination_path)"]},{"cell_type":"code","execution_count":null,"id":"Xs5gDWtwGbge","metadata":{"id":"Xs5gDWtwGbge"},"outputs":[],"source":["import zipfile\n","\n","zip_path = '/content/gdrive/MyDrive/Proyecto LLMonkeys/LLM_Redial.zip'\n","extract_path = '/content/LLM_Redial'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)"]},{"cell_type":"code","execution_count":null,"id":"495d7471","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["a2451bbe3d004d1087185a8cdcc69f43","a88b0c4f0c4d4847af9db5f9ccaa70c6","0225d641b1744c8b8f08755353352bec","28e94efa713c42ce9b944a9442b7f60a","de659da2708046a695fbc58382d477f6","889894fac9b147338a922e1d13a23535","cb3ce520a290461cbeeb84b545ac6cbf","e952be3dd2e64666947d2e7240c1eb33","ad09650412394084875c44682549e92b","ddd41ad1189d47f2a1c7995e2153ed61","d26f10ef6b334f11b6556cf2963268b4","84a7c97e7c4e43a2896f22814569fb60","70063c2c689742aca985bf67710aa44f","d74d3fd8d6d94cc6a9afa34b5e227aa9","5f2b4ce302f6494cbd51634ec7357ecd","1ef566dd039d481e8fa8fd16d3531ccf","4cb1b0cf428244d0a5960f159d8d45ef","705eed9afee94f9292fa90e9978b3070","ed3184a948714ed990b8c301f8eba8fb","978afc6635614b59ad054588b9282eec","a611b91f678143578c64fedc8531da33","74c5fe1151b84fc08e236e245a08827a","96a21a7196464619825d155be0b999b6","a4391426a8494f57bb8a2b72c0cd964c","1f639d5e26db4b45a7a3368e64d1ac20","8d728d16f0f04ef49a26ad2ba6704ed9","395554c73a2247c99aa361d87018c0ef","64eea959a3f242f9bbf78b97ea1ddbba","9e58355700bb42e1b20571ed0783b37f","09585fe909fe4e6381cde16bcb04ae0c","9d16803b2c9c4bd69a69a4d6da28fefa","da86389edfc946afa7804c0e4e127d91","be03fd89fc8344b2bba174b925db3ade","0f59e8c0df9e4865bf2292bf7443f44c","48edd33c9bb6467db9e5f32c8e04c631","c5db6f6ae92349a3ad95ef48204c34c9","f11084089943480cb013b6e360588912","a18a0b4da77c4b97a344744e7180bcfe","edfbf556561349d88b0e46f62cb3cd02","695efbf4c7b9464588eee142c010ad8e","667fdbfbcb1a4b359571cf6edf19ed46","719e5e8465be47468d8a951bcefd7ad7","f15d151fd6d849c5939d1e76d0176e7b","6a88619e1f434a3c8dd37738e8910685"]},"executionInfo":{"elapsed":6429,"status":"ok","timestamp":1750191592944,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"495d7471","outputId":"04faa40d-63d1-48f1-f8a7-e8feeb6c2e71"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2451bbe3d004d1087185a8cdcc69f43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a7c97e7c4e43a2896f22814569fb60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a21a7196464619825d155be0b999b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f59e8c0df9e4865bf2292bf7443f44c"}},"metadata":{}}],"source":["from transformers import AutoTokenizer\n","\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"id":"c9f178ea","metadata":{"id":"c9f178ea"},"outputs":[],"source":["# Cargar datos del Dataset\n","\n","import Tools as t\n","\n","path = \"./LLM_Redial/Movie\"\n","final_data_path = '{}/final_data.jsonl'.format(path)\n","Conversation_path = '{}/Conversation.txt'.format(path)\n","user_map_path = '{}/user_ids.json'.format(path)\n","item_map_path = '{}/item_map.json'.format(path)\n","\n","final_data = t.read_jsonl(final_data_path)\n","user_map = t.read_json(user_map_path)\n","item_map = t.read_json(item_map_path)\n","Conversation = t.read_dialogue(Conversation_path)"]},{"cell_type":"markdown","id":"tbkknIrVmsd_","metadata":{"id":"tbkknIrVmsd_"},"source":["### Para limpiar el entorno y guardar las variables relevantes"]},{"cell_type":"code","execution_count":null,"id":"76eC882Smq-8","metadata":{"id":"76eC882Smq-8"},"outputs":[],"source":["import gc\n","import torch\n","import dill\n","\n","def cleanup_for_dill_serialization():\n","    \"\"\"\n","    Limpia todas las variables pesadas que pueden causar problemas con dill\n","    Mantiene solo los outputs y variables esenciales\n","    \"\"\"\n","    # Variables que debes eliminar ANTES de dill.dump_session()\n","    variables_to_delete = [\n","        'model', 'base_model', 'config', 'pipe',\n","\n","        # Tensores y objetos de PyTorch\n","        'inputs', 'output_ids', 'outputs',\n","\n","        # Variables temporales del loop\n","        'decoded', 'prompt', 'messages', 'ctx', 'msg',\n","\n","        # Indices y variables de control\n","        'i', 'n', 'k',\n","    ]\n","\n","    # Lista de variables que S√ç quieres mantener\n","    variables_to_keep = [\n","        'item_map',\n","        'Conversation',\n","        'model_name',\n","        'output_r'\n","        'outputs_mp'\n","        'outputs_z_s',\n","        'outputs_f_s',\n","        'outputs_ft_s',\n","        'outputs_z_n',\n","        'outputs_f_n',\n","        'outputs_ft_n',\n","        'rand_conversations',\n","        'num_test_items',\n","        'train_conv',\n","        'test_conv',\n","        'few_shot_users',\n","        'num_test_items',\n","        'few_shot_data'\n","    ]\n","\n","    # Obtener todas las variables globales\n","    global_vars = list(globals().keys())\n","\n","    # Eliminar variables espec√≠ficamente problem√°ticas\n","    for var_name in variables_to_delete:\n","        if var_name in globals():\n","            print(f\"   ‚îú‚îÄ‚îÄ Eliminando: {var_name}\")\n","            try:\n","                del globals()[var_name]\n","            except:\n","                print(f\"No se pudo eliminar {var_name}\")\n","\n","    vars_to_remove = []\n","    for var_name in global_vars:\n","        if var_name.startswith('_'):  # Variables privadas\n","            continue\n","\n","        if var_name in variables_to_keep:  # No eliminar variables importantes\n","            continue\n","\n","        try:\n","            var_obj = globals().get(var_name)\n","            var_type = str(type(var_obj))\n","\n","            # Detectar objetos problem√°ticos\n","            problematic_types = [\n","                'transformers',\n","                'peft',\n","                'torch.nn',\n","                'pipeline',\n","                'PreTrainedModel',\n","                'PreTrainedTokenizer',\n","                'PeftModel',\n","                'Tensor'\n","            ]\n","\n","            if any(prob_type in var_type for prob_type in problematic_types):\n","                vars_to_remove.append(var_name)\n","\n","        except Exception as e:\n","            vars_to_remove.append(var_name)\n","\n","    # Eliminar variables problem√°ticas detectadas\n","    for var_name in vars_to_remove:\n","        print(f\"   ‚îú‚îÄ‚îÄ Eliminando: {var_name}\")\n","        try:\n","            del globals()[var_name]\n","        except:\n","            print(f\"No se pudo eliminar: {var_name}\")\n","\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","    gc.collect()\n","\n","    # Verificar tama√±o de variables mantenidas\n","    print(\"\\nüìä Variables mantenidas:\")\n","    for var_name in variables_to_keep:\n","        if var_name in globals():\n","            var_obj = globals()[var_name]\n","            try:\n","                if hasattr(var_obj, '__len__'):\n","                    print(f\"   ‚îú‚îÄ‚îÄ {var_name}: {len(var_obj)} elementos\")\n","                else:\n","                    print(f\"   ‚îú‚îÄ‚îÄ {var_name}: {type(var_obj)}\")\n","            except:\n","                print(f\"   ‚îú‚îÄ‚îÄ {var_name}: (no se puede medir)\")\n","\n","    return True\n","\n","# FUNCI√ìN PRINCIPAL PARA TU CASO\n","def cleanup_after_lora_generation():\n","    \"\"\"\n","    Limpieza espec√≠fica despu√©s de generar con modelo LoRA\n","    \"\"\"\n","    print(\"üéØ Limpieza espec√≠fica para modelo LoRA...\")\n","\n","    # Variables espec√≠ficas de tu c√≥digo LoRA\n","    lora_specific_vars = [\n","        'peft_model_path',\n","        'config',           # PeftConfig\n","        'base_model',       # Modelo base\n","        'model',           # PeftModel final\n","        'tokenizer',       # Tokenizer\n","        'pipe',           # Pipeline\n","        'inputs',          # Tensors de input\n","        'output_ids',      # Tensors de output IDs\n","        'prompt',          # Prompt generado\n","        'messages',        # Mensajes del chat template\n","        'ctx',            # Context string\n","        'msg',            # Message string\n","        'decoded',        # String decodificado\n","        'outputs',        # Lista temporal (no outputs_ft_s)\n","    ]\n","\n","    for var_name in lora_specific_vars:\n","        if var_name in globals():\n","            print(f\"   ‚îú‚îÄ‚îÄ Eliminando: {var_name}\")\n","            try:\n","                del globals()[var_name]\n","            except Exception as e:\n","                print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ Error: {e}\")\n","\n","    # Limpieza general\n","    cleanup_for_dill_serialization()\n","\n","def limpiar_y_guardar():\n","  cleanup_after_lora_generation()\n","  path = '/content/gdrive/MyDrive/Proyecto LLMonkeys/sessions/TinyLlama/TinyLlama_notebook_env.db'\n","  with open(path, 'wb') as f:\n","      dill.dump_session(f)"]},{"cell_type":"markdown","id":"SZkhMuOgm5eB","metadata":{"id":"SZkhMuOgm5eB"},"source":["### Para retornar la respuesta del modelo como una lista de nombres"]},{"cell_type":"code","execution_count":null,"id":"Juqn42WWm4pO","metadata":{"id":"Juqn42WWm4pO"},"outputs":[],"source":["import re\n","def format_ans(ans, n):\n","    answer = \"None\"\n","    try:\n","        answer_list = []\n","        current_pos = 0\n","\n","        for j in range(1, n + 1):\n","            # Busca el patr√≥n del n√∫mero y el inicio del texto\n","            patterns = [\n","                f\"{j}. \",\n","                f\"Movie {j}: \",\n","                f\"Movie name {j}: \",\n","                f\"Movie Name {j}: \",\n","                f\"[{j}] \",\n","            ]\n","            for pattern in patterns:\n","                pattern_start = pattern\n","                start_index = ans.find(pattern_start, current_pos)\n","                if start_index != -1:\n","                    break\n","\n","            if start_index == -1:\n","                break  # Si no se encuentra el n√∫mero, salimos del bucle\n","\n","            start_text = start_index + len(pattern_start)\n","\n","            # Busca el final del texto: el inicio del siguiente n√∫mero O un \" - \"\n","            next_number_start = ans.find(f\"{j + 1}. \", start_text)\n","            dash_start = ans.find(\" - \", start_text)\n","\n","            end_text = -1\n","\n","            # Determina el final del texto basado en la primera ocurrencia\n","            if next_number_start != -1 and dash_start != -1:\n","                end_text = min(next_number_start, dash_start)\n","            elif next_number_start != -1:\n","                end_text = next_number_start\n","            elif dash_start != -1:\n","                end_text = dash_start\n","            else:\n","                # Si no hay siguiente n√∫mero ni \" - \", toma hasta el final de la l√≠nea o la cadena\n","                end_line = ans.find(\"\\n\", start_text)\n","                if end_line != -1:\n","                    end_text = end_line\n","                else:\n","                    end_text = len(ans)\n","\n","            if end_text == -1: # Si no se encontr√≥ un final v√°lido, toma hasta el final de la cadena\n","                end_text = len(ans)\n","\n","            movie_name = ans[start_text:end_text].strip()\n","            answer_list.append(movie_name)\n","            current_pos = end_text # Actualiza la posici√≥n para la pr√≥xima b√∫squeda\n","\n","        if not answer_list: # Si no se encontraron pel√≠culas\n","             return \"Formato de respuesta incorrecto\", i\n","\n","    except Exception as e:\n","        answer = f\"Error: {e}\"\n","\n","    finally:\n","        pattern = r\"\\(\\d{4}\\)\"\n","        answer_list = [m.replace('\\\"', '') for m in answer_list]\n","        answer_list = [re.sub(pattern, '', m).strip() for m in answer_list]\n","        return answer_list"]},{"cell_type":"markdown","id":"7Dw3xNSv2Im7","metadata":{"id":"7Dw3xNSv2Im7"},"source":["### Para guardar y cargar las respuestas del modelo"]},{"cell_type":"code","execution_count":null,"id":"4KkUn2zl2NGh","metadata":{"id":"4KkUn2zl2NGh"},"outputs":[],"source":["import json\n","\n","def guardar_datos_json(lista_de_listas, nombre_archivo):\n","  \"\"\"\n","  Guarda una lista de listas en un archivo de texto en formato JSON.\n","\n","  Args:\n","    lista_de_listas: La lista de listas a guardar.\n","    nombre_archivo: El nombre del archivo donde se guardar√°.\n","  \"\"\"\n","  try:\n","    with open(nombre_archivo, 'w', encoding='utf-8') as f:\n","      json.dump(lista_de_listas, f, ensure_ascii=False, indent=4)\n","    print(f\"Outputs guardados exitosamente en '{nombre_archivo}'\")\n","  except Exception as e:\n","    print(f\"Error al guardar outputs: {e}\")\n","\n","def cargar_datos_json(nombre_archivo):\n","  \"\"\"\n","  Carga una lista de listas desde un archivo de texto en formato JSON.\n","\n","  Args:\n","    nombre_archivo: El nombre del archivo desde donde se cargar√°.\n","\n","  Returns:\n","    La lista de listas cargada, o None si hay un error.\n","  \"\"\"\n","  try:\n","    with open(nombre_archivo, 'r', encoding='utf-8') as f:\n","      lista_de_listas = json.load(f)\n","    print(f\"Outputs cargados exitosamente desde '{nombre_archivo}'\")\n","    return lista_de_listas\n","  except FileNotFoundError:\n","    print(f\"Error: El archivo '{nombre_archivo}' no fue encontrado.\")\n","    return None\n","  except json.JSONDecodeError:\n","    print(f\"Error: El archivo '{nombre_archivo}' no es un JSON v√°lido.\")\n","    return None\n","  except Exception as e:\n","    print(f\"Error al cargar outputs: {e}\")\n","    return None"]},{"cell_type":"markdown","source":["### Cargar las conversaciones a utilizar para el entrenamiento, validaci√≥n y testeo"],"metadata":{"id":"f1neSitTkVCp"},"id":"f1neSitTkVCp"},{"cell_type":"code","source":["def load_conversations(conv_path):\n","\n","    all_conversations = {}\n","    current_conv_id = None\n","    current_conv_lines = []\n","\n","    with open(conv_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            line_stripped = line.strip()\n","\n","            # Si es un n√∫mero (ID de conversaci√≥n)\n","            if line_stripped.isdigit():\n","                # Guardar conversaci√≥n anterior si existe\n","                if current_conv_id is not None:\n","                    all_conversations[current_conv_id] = \"\".join(current_conv_lines[2:])\n","\n","                # Iniciar nueva conversaci√≥n\n","                current_conv_id = int(line_stripped)\n","                current_conv_lines = [line]\n","            else:\n","                # Agregar l√≠nea a la conversaci√≥n actual\n","                if current_conv_id is not None:\n","                    current_conv_lines.append(line)\n","\n","    if current_conv_id is not None:\n","        all_conversations[current_conv_id] = current_conv_lines\n","\n","\n","    all_conversations[len(all_conversations)-1] = \"\".join(all_conversations[len(all_conversations)-1][2:])\n","\n","    return all_conversations\n","\n","all_conversations = load_conversations(Conversation_path)\n","n_conversations = len(all_conversations)\n","print(n_conversations)"],"metadata":{"id":"REHcyqT0kdKa"},"id":"REHcyqT0kdKa","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"14e7c87a","metadata":{"id":"14e7c87a"},"source":["### Separaci√≥n de di√°logos en train, test y val, cuidando que todas las conversaciones de un usuario en particular se encuentren en s√≥lo train o s√≥lo en test\n"]},{"cell_type":"code","execution_count":null,"id":"93ada744","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":470,"status":"ok","timestamp":1750191593661,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"93ada744","outputId":"34fc69ef-b68d-4c43-fd91-1b995faa2f96"},"outputs":[{"output_type":"stream","name":"stdout","text":["316 289 2526 3131 10089\n"]}],"source":["import random\n","import json\n","\n","path = './LLM_Redial/Movie/final_data.jsonl'\n","\n","# Cada entrada de final_data.jsonl se ve as√≠\n","#{\n","#  \"A30Q8X8B1S3GGT\": {\n","#    \"history_interaction\": [...],\n","#    \"user_might_like\": [...],\n","#    \"Conversation\": [...]\n","#  }\n","#}\n","\n","with open(path, 'r', encoding='utf-8') as file:\n","    data = [json.loads(line) for line in file]\n","\n","train_len = n_conversations * 0.8\n","test_val_len = n_conversations * 0.1\n","\n","train_conv = []\n","test_conv = []\n","val_conv = []\n","used_users = []\n","\n","user_data_map = {user_id: user_info for item in data for user_id, user_info in item.items()}\n","\n","aux = []\n","\n","while len(aux) < test_val_len:\n","    try:\n","        convs = []\n","        random.seed(42)\n","\n","        # Usando used_users aseguramos que un mismo usuario no aparezca en Train y Test al mismo tiempo\n","        user_id = random.choice(list(set(user_map.keys())^set(used_users)))\n","        used_users.append(user_id)\n","\n","        # Guardamos la info del usuario y sus dialogos\n","        user_data = user_data_map.get(user_id)\n","        user_conversations = user_data.get(\"Conversation\", [])\n","\n","        for i in range(len(user_conversations)):\n","            selected_conversation = user_conversations[i]\n","            conversation_details = list(selected_conversation.values())[0]\n","            conversation_id = conversation_details[\"conversation_id\"]\n","            conversation = all_conversations[conversation_id]\n","\n","            convs.append(conversation)\n","            aux.append(conversation)\n","        test_conv.append([user_id,convs])\n","    except ValueError:\n","        print(\"ValueError: \", user_id, f\"{conversation_id}\\n\")\n","\n","aux = []\n","\n","while len(aux) < test_val_len:\n","    try:\n","        convs = []\n","        random.seed(42)\n","        user_id = random.choice(list(set(user_map.keys())^set(used_users)))\n","        used_users.append(user_id)\n","        user_data = user_data_map.get(user_id)\n","        user_conversations = user_data.get(\"Conversation\", [])\n","        for i in range(len(user_conversations)):\n","            selected_conversation = user_conversations[i]\n","            conversation_details = list(selected_conversation.values())[0]\n","            conversation_id = conversation_details[\"conversation_id\"]\n","            conversation = all_conversations[conversation_id]\n","\n","            convs.append(conversation)\n","            aux.append(conversation)\n","        val_conv.append([user_id,convs])\n","    except ValueError:\n","        print(\"ValueError: \", user_id, f\"{conversation_id}\\n\")\n","\n","for user_id in list(set(user_map.keys()) ^ set(used_users)):\n","    try:\n","        convs = []\n","        user_data = user_data_map.get(user_id)\n","        user_conversations = user_data.get(\"Conversation\", [])\n","        for i in range(len(user_conversations)):\n","            selected_conversation = user_conversations[i]\n","            conversation_details = list(selected_conversation.values())[0]\n","            conversation_id = conversation_details[\"conversation_id\"]\n","            conversation = all_conversations[conversation_id]\n","\n","            convs.append(conversation)\n","        train_conv.append([user_id,convs])\n","    except ValueError:\n","        print(\"ValueError: \", user_id, f\"{conversation_id}\\n\")\n","\n","print(len(test_conv), len(val_conv), len(train_conv), len(test_conv) + len(train_conv) + len(val_conv), n_conversations)"]},{"cell_type":"markdown","id":"oVguNVh2j5Kp","metadata":{"id":"oVguNVh2j5Kp"},"source":["### Elegimos si generamos respuestas nuevas o si cargamos datos ya generados"]},{"cell_type":"code","execution_count":null,"id":"hbJ-ELCQkCk1","metadata":{"id":"hbJ-ELCQkCk1"},"outputs":[],"source":["eleccion = \"1\"\n","while eleccion not in [\"1\", \"2\"]:\n","  eleccion = input(\"Generar respuestas nuevas (1) o cargar respuestas anteriores (2)? \")\n","  if eleccion not in [\"1\", \"2\"]:\n","    print(\"Opci√≥n inv√°lida. Por favor, elige 1 o 2.\\n\")"]},{"cell_type":"markdown","id":"0426f46b","metadata":{"id":"0426f46b"},"source":["### Selecci√≥n aleatoria de di√°logos para testear"]},{"cell_type":"code","execution_count":null,"id":"QRWLZzGdRg4g","metadata":{"id":"QRWLZzGdRg4g"},"outputs":[],"source":["num_test_items = 100"]},{"cell_type":"code","execution_count":null,"id":"aFWZECPmDrYK","metadata":{"id":"aFWZECPmDrYK"},"outputs":[],"source":["import random\n","def extraer_dialogos(conversations, num_conversations):\n","  rand_conversations = []\n","  all = []\n","  used_convs = []\n","  for conv in conversations:\n","    all.extend([f\"{conv[0]}:::{c}\" for c in conv[1]])\n","\n","  while len(rand_conversations) < num_conversations:\n","\n","    rand_user = random.choice(list(set(all)^set(used_convs)))\n","    used_convs.append(rand_user)\n","    rand_user = rand_user.split(\":::\")\n","    user_id = rand_user[0]\n","    user_conversation  = rand_user[1]\n","    user_data = user_data_map[user_id]\n","    convs = user_data.get(\"Conversation\", [])\n","    for i in range(len(convs)):\n","      conv_details = list(convs[i].values())[0]\n","      conversation = all_conversations[conv_details[\"conversation_id\"]]\n","      # print(conversation)\n","      # print(user_conversation)\n","      if user_conversation == conversation:\n","        rand_user_conv_id = i\n","    dialog = \"\\n\\n\".join(user_conversation.split(\"\\n\\n\")[:3])\n","    dialog_id = list(convs[rand_user_conv_id].values())[0][\"conversation_id\"]\n","    dialog_ground_truth = list(convs[rand_user_conv_id].values())[0][\"rec_item\"]\n","    rand_user_interactions = user_data.get(\"history_interaction\", [])\n","    rand_user_interactions = [item_map[m] for m in rand_user_interactions]\n","    rand_conversations.append([\n","        rand_user_conv_id,      # index 0\n","        dialog,                 # index 1\n","        user_data,              # index 2\n","        dialog_id,              # index 3\n","        dialog_ground_truth,    # index 4\n","        rand_user_interactions  # index 5\n","    ])\n","  return rand_conversations"]},{"cell_type":"code","execution_count":null,"id":"29c1fa03","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29c1fa03","executionInfo":{"status":"ok","timestamp":1750191599602,"user_tz":240,"elapsed":878,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"1d1e7402-35d1-4637-b7f9-9d9afaec56ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Outputs guardados exitosamente en '/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/rand_conversations.json'\n"]}],"source":["if eleccion == \"1\":\n","  rand_conversations = extraer_dialogos(test_conv, num_test_items)\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/rand_conversations.json\"\n","  guardar_datos_json(rand_conversations, path)\n","elif eleccion == \"2\":\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/rand_conversations.json\"\n","  rand_conversations = cargar_datos_json(path)"]},{"cell_type":"markdown","id":"lwDqdAOmrEh1","metadata":{"id":"lwDqdAOmrEh1"},"source":["### Seleccionamos aleatoriamente los datos de Few shot desde train"]},{"cell_type":"code","execution_count":null,"id":"fRZjdrKHj0kW","metadata":{"id":"fRZjdrKHj0kW"},"outputs":[],"source":["# ESTO QUEDA POR ARREGLAR, PASAR DATOS FEW-SHOT CON EL MISMO FORMATO DEL INPUT-RESPUESTA\n","\n","few_shot_data = []\n","random.seed(42)\n","\n","sorted_train_conv = []\n","for user, texts in train_conv:\n","  ordered_text = sorted(texts, key=len)\n","  sorted_train_conv.append([user, ordered_text])\n","\n","sorted_train_conv.sort(key=lambda x: len(x[1][0]))\n","\n","few_shot_users = random.sample(sorted_train_conv[:500], 5)\n","\n","for u in few_shot_users:\n","  user_data = next((item[u[0]] for item in data if u[0] in item), None)\n","  user_interactions = user_data.get(\"history_interaction\", [])\n","  user_interactions = [item_map[m] for m in user_interactions]\n","\n","  conversation = min(u[1], key=len)\n","  conversation[conversation.index(\"User:\"):-2]\n","  convs = user_data.get(\"Conversation\", [])\n","  for c in convs:\n","    user_likes = list(c.values())[0][\"user_likes\"]\n","    user_dislikes = list(c.values())[0][\"user_dislikes\"]\n","    recs= list(c.values())[0][\"rec_item\"]\n","\n","    user_likes = [item_map[m] for m in user_likes]\n","    user_dislikes = [item_map[m] for m in user_dislikes]\n","    recs = [item_map[m] for m in recs]\n","  few_shot_data.append({\n","      # \"user_interactions\": user_interactions,\n","      \"conversation\": conversation,\n","      \"user_likes\": user_likes,\n","      \"user_dislikes\": user_dislikes,\n","      \"recs\": recs\n","  })"]},{"cell_type":"code","source":["def contar_tokens(texto):\n","  print(len(tokenizer.encode(texto)))"],"metadata":{"id":"qezkYAs8SaiR"},"id":"qezkYAs8SaiR","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"guy_SY1wevkC","metadata":{"id":"guy_SY1wevkC"},"source":["# Generaci√≥n de 20 listas de recomendaci√≥n de 10 pel√≠culas"]},{"cell_type":"markdown","id":"24214eec","metadata":{"id":"24214eec"},"source":["### Zero-Shot con interacciones hist√≥ricas:"]},{"cell_type":"code","execution_count":1,"id":"kG3zS4s2Ci5n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"elapsed":18395,"status":"error","timestamp":1750287779839,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"kG3zS4s2Ci5n","outputId":"bbda9738-050d-44f2-f484-07fba0c7fa18"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'eleccion' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-3585924526.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0meleccion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# MAJOR OPTIMIZATION: Initialize pipeline ONCE outside the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'eleccion' is not defined"]}],"source":["import torch\n","from transformers import pipeline\n","import random\n","import time\n","\n","if eleccion == \"1\":\n","\n","  # MAJOR OPTIMIZATION: Initialize pipeline ONCE outside the loop\n","  pipe = pipeline(\n","      \"text-generation\",\n","      model=model_name,\n","      device_map=\"auto\",\n","      torch_dtype=torch.float16,\n","  )\n","\n","  # Pre-compile the base context to avoid string concatenation in loop\n","  base_ctx = (\n","      \"You are a Movie Recommendation System.\"\n","      \"Generate a numbered list of 10 Movies. \\n\"\n","      \"RULES: \\n\"\n","      \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","      \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","      \"c) You MUST recommend 10 movies, nothing more, nothing less. \\n\"\n","      \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","      \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","  )\n","\n","  outputs_z_s = []\n","  k = 20\n","\n","  # Generation parameters (moved outside loop)\n","  gen_kwargs = {\n","      \"max_new_tokens\": 200,\n","      \"do_sample\": True,\n","      \"temperature\": 0.7,\n","      \"top_k\": 50,\n","      \"top_p\": 0.95,\n","      \"pad_token_id\": pipe.tokenizer.eos_token_id  # Prevents warnings\n","  }\n","  total_start_time = time.time()\n","  for n in range(num_test_items):\n","      if torch.cuda.is_available():\n","          torch.cuda.empty_cache()\n","      if n%5 == 0:\n","        start_time = time.time()\n","      print(f\"Generating for test item {n+1}...\")\n","      outputs = []\n","\n","      # Build message once per test item\n","      msg = (\n","          \"\\nBased on the following conversation: \\n\"\n","          f\"{rand_conversations[n][1]} \\n\\n\"\n","          \"And the movies the user has previously interacted with: \\n\"\n","          f\"{random.sample(rand_conversations[n][5], min(10, len(rand_conversations[n][5])))}\\n\\n\"\n","          \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","      )\n","\n","      messages = [{\"role\": \"user\", \"content\": base_ctx + msg}]\n","      prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","      # Tokenize once per test item, not per generation\n","      inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","      # Batch generation option (much faster if memory allows)\n","      with torch.no_grad():\n","          iter_start_time = time.time()\n","          output_ids = pipe.model.generate(\n","              **inputs,\n","              num_return_sequences=k,\n","              **gen_kwargs\n","          )\n","          for output_id in output_ids:\n","              decoded = pipe.tokenizer.decode(output_id, skip_special_tokens=True)\n","              outputs.append(decoded)\n","          # print(f\"Iteration {i} of test item {n+1} generated in {time.time() - iter_start_time} seconds.\")\n","\n","      outputs_z_s.append(outputs)\n","      if (n+1)%5 == 0:\n","        print(f\"Test items {n-3}-{n+1} generated in {(time.time() - start_time):.3f} seconds.\\n\")\n","  total = time.time() - total_start_time\n","  print(f\"Total generation time: {int(total//60)} minutes and {int(total - total//60*60)} seconds.\\n\")\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_zero_shot.json\"\n","  guardar_datos_json(outputs_z_s, path)\n","\n","elif eleccion == \"2\":\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_zero_shot.json\"\n","  outputs_z_s = cargar_datos_json(path)\n"]},{"cell_type":"markdown","id":"bpYn7Iyp3vJ4","metadata":{"id":"bpYn7Iyp3vJ4"},"source":["### Few-Shot con interacci√≥n hist√≥rica"]},{"cell_type":"code","execution_count":null,"id":"woC8p7lVrgZW","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":746990,"status":"ok","timestamp":1750189042546,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"woC8p7lVrgZW","outputId":"a4872d58-7b9b-453f-87b8-ae2216b5ccbe"},"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["Generating for test item 1...\n","Generating for test item 2...\n","Generating for test item 3...\n","Generating for test item 4...\n","Generating for test item 5...\n","Test items 1-5 generated in 73.995 seconds.\n","\n","Generating for test item 6...\n","Generating for test item 7...\n","Generating for test item 8...\n","Generating for test item 9...\n","Generating for test item 10...\n","Test items 6-10 generated in 74.225 seconds.\n","\n","Generating for test item 11...\n","Generating for test item 12...\n","Generating for test item 13...\n","Generating for test item 14...\n","Generating for test item 15...\n","Test items 11-15 generated in 73.807 seconds.\n","\n","Generating for test item 16...\n","Generating for test item 17...\n","Generating for test item 18...\n","Generating for test item 19...\n","Generating for test item 20...\n","Test items 16-20 generated in 76.282 seconds.\n","\n","Generating for test item 21...\n","Generating for test item 22...\n","Generating for test item 23...\n","Generating for test item 24...\n","Generating for test item 25...\n","Test items 21-25 generated in 74.038 seconds.\n","\n","Generating for test item 26...\n","Generating for test item 27...\n","Generating for test item 28...\n","Generating for test item 29...\n","Generating for test item 30...\n","Test items 26-30 generated in 76.459 seconds.\n","\n","Generating for test item 31...\n","Generating for test item 32...\n","Generating for test item 33...\n","Generating for test item 34...\n","Generating for test item 35...\n","Test items 31-35 generated in 74.040 seconds.\n","\n","Generating for test item 36...\n","Generating for test item 37...\n","Generating for test item 38...\n","Generating for test item 39...\n","Generating for test item 40...\n","Test items 36-40 generated in 73.145 seconds.\n","\n","Generating for test item 41...\n","Generating for test item 42...\n","Generating for test item 43...\n","Generating for test item 44...\n","Generating for test item 45...\n","Test items 41-45 generated in 74.669 seconds.\n","\n","Generating for test item 46...\n","Generating for test item 47...\n","Generating for test item 48...\n","Generating for test item 49...\n","Generating for test item 50...\n","Test items 46-50 generated in 73.741 seconds.\n","\n","Total generation time: 12 minutes and 24 seconds.\n","\n","Outputs guardados exitosamente en '/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_few_shot.json'\n"]}],"source":["import torch\n","from transformers import pipeline\n","import random\n","import time\n","\n","if eleccion == \"1\":\n","  # MAJOR OPTIMIZATION: Initialize pipeline ONCE outside the loop\n","  pipe = pipeline(\n","      \"text-generation\",\n","      model=model_name,\n","      device_map=\"auto\",\n","      torch_dtype=torch.float16,\n","  )\n","\n","  # Pre-compile the base context to avoid string concatenation in loop\n","  base_ctx = (\n","      \"You are a Movie Recommendation System.\"\n","      \"Generate a numbered list of 10 Movies. \\n\"\n","      \"RULES: \\n\"\n","      \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","      \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","      \"c) You MUST recommend 10 movies. \\n\"\n","      \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","      \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","  )\n","\n","  outputs_f_s = []\n","  k = 20\n","\n","  # Generation parameters (moved outside loop)\n","  gen_kwargs = {\n","      \"max_new_tokens\": 180,\n","      \"do_sample\": True,\n","      \"temperature\": 0.7,\n","      \"top_k\": 50,\n","      \"top_p\": 0.95,\n","      \"pad_token_id\": pipe.tokenizer.eos_token_id  # Prevents warnings\n","  }\n","  total_start_time = time.time()\n","  for n in range(num_test_items):\n","      if torch.cuda.is_available():\n","          torch.cuda.empty_cache()\n","      if n%5 == 0:\n","        start_time = time.time()\n","      print(f\"Generating for test item {n+1}...\")\n","      outputs = []\n","\n","      # Build message once per test item\n","      msg = (\n","        \"\\nBased on these 4 examples: \\n\"\n","        f\"{few_shot_data[0]}\\n\"\n","        f\"{few_shot_data[1]}\\n\"\n","        f\"{few_shot_data[2]}\\n\"\n","        f\"{few_shot_data[3]}\\n\\n\"\n","        \"And based on the following conversation: \\n\"\n","        f\"{rand_conversations[n][1]} \\n\\n\"\n","        \"And the movies the user has previously interacted with: \\n\"\n","        f\"{random.sample(rand_conversations[n][5], min(10, len(rand_conversations[n][5])))}\\n\\n\"\n","        \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","    )\n","\n","      messages = [{\"role\": \"user\", \"content\": base_ctx + msg}]\n","      prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","      # Tokenize once per test item, not per generation\n","      inputs = pipe.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","      # Batch generation option (much faster if memory allows)\n","      with torch.no_grad():\n","          iter_start_time = time.time()\n","          output_ids = pipe.model.generate(\n","              **inputs,\n","              num_return_sequences=k,\n","              **gen_kwargs\n","          )\n","          for output_id in output_ids:\n","              decoded = pipe.tokenizer.decode(output_id, skip_special_tokens=True)\n","              outputs.append(decoded)\n","          # print(f\"Iteration {i} of test item {n+1} generated in {time.time() - iter_start_time} seconds.\")\n","\n","      outputs_f_s.append(outputs)\n","      if (n+1)%5 == 0:\n","        print(f\"Test items {n-3}-{n+1} generated in {(time.time() - start_time):.3f} seconds.\\n\")\n","  total = time.time() - total_start_time\n","  print(f\"Total generation time: {int(total//60)} minutes and {int(total - total//60*60)} seconds.\\n\")\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_few_shot.json\"\n","  guardar_datos_json(outputs_f_s, path)\n","\n","elif eleccion == \"2\":\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_few_shot.json\"\n","  outputs_f_s = cargar_datos_json(path)"]},{"cell_type":"markdown","id":"Mr5beh1_2nRo","metadata":{"id":"Mr5beh1_2nRo"},"source":["### Fine-Tuning"]},{"cell_type":"code","execution_count":null,"id":"wPVY25F87MvP","metadata":{"id":"wPVY25F87MvP"},"outputs":[],"source":["from huggingface_hub import login\n","login(token=\"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\")"]},{"cell_type":"code","execution_count":null,"id":"dpJiVHkQnWf8","metadata":{"id":"dpJiVHkQnWf8"},"outputs":[],"source":["movie_likes = {}\n","for u in train_conv:\n","  user = u[0]\n","  user_data = user_data_map[user]\n","  convs = user_data.get(\"Conversation\", [])\n","  for c in convs:\n","    user_likes = list(c.values())[0][\"user_likes\"]\n","    for m in user_likes:\n","      try:\n","        movie_likes[m] += 1\n","      except:\n","        movie_likes[m] = 1\n","\n","sorted_movie_likes = dict(sorted(movie_likes.items(), key=lambda item: item[1], reverse=True))\n","top_20_movies = dict(list(sorted_movie_likes.items())[:20])\n","top_20_movie_names = [item_map[m] for m in list(top_20_movies.keys())]"]},{"cell_type":"code","execution_count":null,"id":"3ZMth60PEJWr","metadata":{"id":"3ZMth60PEJWr"},"outputs":[],"source":["import random\n","def preparar_datos_fine_tuning(rand_conversations):\n","\n","    fine_tune_data = []\n","\n","    base_ctx = (\n","        \"You are a Movie Recommendation System.\"\n","        \"Generate a numbered list of 10 Movies. \\n\"\n","        \"RULES: \\n\"\n","        \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","        \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","        \"c) You MUST recommend 10 movies, nothing more, nothing less. \\n\"\n","        \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","        \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","    )\n","\n","    for conversation_data in rand_conversations:\n","      msg = (\n","          \"\\nBased on the following conversation: \\n\"\n","          f\"{conversation_data[1]} \\n\\n\"\n","          \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","      )\n","\n","      input_text = base_ctx + msg\n","\n","      top_10_recommendations = []\n","\n","      top_10_recommendations.append(item_map[conversation_data[4][0]]) # ground truth numero 1\n","\n","      user_convs = list(conversation_data[2].get(\"Conversation\", []))\n","      user_conv_id = conversation_data[0]\n","      user_likes = list(user_convs[user_conv_id].values())[0][\"user_likes\"]\n","      user_likes = [item_map[m] for m in user_likes]\n","\n","      # agregamos 3 pel√≠culas que le gustan al usuario que no se mencionan en el extracto\n","      for m in user_likes:\n","        if m not in conversation_data[1] and len(top_10_recommendations) < 4:\n","          top_10_recommendations.append(m)\n","\n","      # agregamos 4 pel√≠culas de sus interacciones\n","      interactions = random.sample(conversation_data[5], min(10, len(conversation_data[5])))\n","      top_10_recommendations.extend(interactions[:4])\n","\n","      # rellenamos con pel√≠culas al azar de las 20 m√°s populares\n","      while len(top_10_recommendations) < 10:\n","        random_movie = random.choice(top_20_movie_names)\n","        if random_movie not in top_10_recommendations:\n","          top_10_recommendations.append(random_movie)\n","\n","      output_text = \"\\n\".join([f\"{i+1}. {movie}\" for i, movie in enumerate(top_10_recommendations)])\n","      fine_tune_data.append({\"text\": f\"\"\"{input_text}{output_text}<|endoftext|>\"\"\"})\n","\n","    return fine_tune_data\n"]},{"cell_type":"code","source":["entrenar = \"2\"\n","while entrenar not in [\"1\", \"2\"]:\n","  entrenar = input(\"Entrenar desde cero (1) o cargar modelo pre-entrenado (2)? \")\n","\n","  if entrenar not in [\"1\", \"2\"]:\n","    print(\"Opci√≥n inv√°lida. Por favor, elige 1 o 2.\\n\")"],"metadata":{"id":"zkygHv4gGA_n"},"id":"zkygHv4gGA_n","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LrvlZg_5DUo8"},"outputs":[],"source":["if entrenar == \"1\":\n","  fine_tune_convs = extraer_dialogos(train_conv, 4000)\n","  fine_tune_convs_val = extraer_dialogos(val_conv, min(1000, len(val_conv)))"],"id":"LrvlZg_5DUo8"},{"cell_type":"code","execution_count":null,"id":"Moa5BHLaDjLK","metadata":{"id":"Moa5BHLaDjLK"},"outputs":[],"source":["if entrenar == \"1\":\n","  fine_tune_data = preparar_datos_fine_tuning(fine_tune_convs)\n","  fine_tune_data_val = preparar_datos_fine_tuning(fine_tune_convs_val)"]},{"cell_type":"code","execution_count":null,"id":"ewRcVV3z2RGa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10303,"status":"ok","timestamp":1750192350377,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"ewRcVV3z2RGa","outputId":"3a799fbe-3084-4c57-aa8d-6cc53774c343"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-169044351>:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n","\n","Please restructure your imports with 'import unsloth' at the top of your file.\n","  from unsloth import FastLanguageModel\n"]},{"output_type":"stream","name":"stdout","text":["ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ü¶• Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["from unsloth import FastLanguageModel\n","import torch\n","from trl import SFTTrainer\n","from transformers import TrainingArguments\n","from datasets import Dataset\n","\n","if entrenar == \"1\":\n","  model, tokenizer = FastLanguageModel.from_pretrained(\n","      model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n","      max_seq_length = 2048,\n","      dtype = None,\n","      load_in_4bit = True,\n","      trust_remote_code = True,\n","  )\n","\n","  model = FastLanguageModel.get_peft_model(\n","      model,\n","      r = 16,\n","      target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n","      lora_alpha = 16,\n","      lora_dropout = 0.1,\n","      bias = \"none\",\n","      use_gradient_checkpointing = True,\n","      random_state = 1234,\n","  )"]},{"cell_type":"code","execution_count":null,"id":"Q9XeROteOQIG","metadata":{"id":"Q9XeROteOQIG"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n","from datasets import Dataset\n","\n","if entrenar == \"1\":\n","  train_dataset = Dataset.from_list(fine_tune_data)\n","  val_dataset = Dataset.from_list(fine_tune_data_val)\n","\n","  data_collator = DataCollatorForLanguageModeling(\n","      tokenizer=tokenizer,\n","      mlm=False,  # False para modelos como TinyLlama (causal LM)\n","      pad_to_multiple_of=8,  # Optimizaci√≥n para tensor cores\n","      return_tensors=\"pt\"\n","  )\n","\n","  training_args = TrainingArguments(\n","      output_dir=\"/content/gdrive/MyDrive/Proyecto LLMonkeys/TinyLlama-fine-tune-3\",\n","      per_device_train_batch_size=4,        # batch por GPU\n","      gradient_accumulation_steps=8,        # acumular gradientes para simular un batch mayor\n","      warmup_steps = 50,\n","      max_steps = 500,                    # ~2-3 epochs para 4000 ejemplos\n","      learning_rate = 2e-4,               # Relativamente alto para LoRA\n","      bf16 = True,                        # Crucial para eficiencia\n","      logging_steps = 25,\n","      optim = \"adamw_8bit\",               # Optimizador eficiente\n","      weight_decay = 0.01,\n","      lr_scheduler_type = \"cosine\",\n","      seed = 1234,\n","      save_steps = 100,\n","      eval_steps = 100,\n","      eval_strategy = \"steps\",\n","      load_best_model_at_end = True,\n","      metric_for_best_model = \"eval_loss\",\n","      greater_is_better = False,\n","  )\n","\n","  trainer = SFTTrainer(\n","      model = model,\n","      tokenizer = tokenizer,\n","      train_dataset = train_dataset,\n","      eval_dataset = val_dataset,\n","      data_collator=data_collator,\n","      dataset_text_field = \"text\",\n","      max_seq_length = 2048,\n","      dataset_num_proc = 2,\n","      args = training_args,\n","  )"]},{"cell_type":"code","execution_count":null,"id":"b6XGbQMIJG35","metadata":{"id":"b6XGbQMIJG35"},"outputs":[],"source":["if entrenar == \"1\":\n","  trainer.train()\n","\n","  model.save_pretrained(\"/content/gdrive/MyDrive/Proyecto LLMonkeys/TinyLlama-fine-tune-3\")\n","  tokenizer.save_pretrained(\"/content/gdrive/MyDrive/Proyecto LLMonkeys/TinyLlama-fine-tune-3\")"]},{"cell_type":"code","execution_count":null,"id":"S58uYKu5ZhiL","metadata":{"id":"S58uYKu5ZhiL","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3385990f9e154013830b9a334febdeaf","7d087b1f96ad42adad79f8f3cb63cbd6","b06efcf3d890481ca28712ff23c54301","efc0d19b03a24ec9ab733d7a74112fc9","6f462510171b454e8b7f6d09f26b2228","d774ee12aec446a3939cfeda5e493980","7990accebc8c4bcd9175d6f0f13d7d8c","8aab6ff39f284871822458a809a4e4c3","76b554cce7d4420db27d207faf2250f5","541da9cea5984f0bb6522d6aa9335723","0bd3abfab28e44c2b6971ff6b675b58a","f9953ae83ee048a0bd4978063f346010","83abd0ecf7004cae9e999e869eaebb21","fb2afd79add74394a8a3cb37cacaac8c","11e885658701464baf6797e5042e5cb0","201040722f6f4101b6e70b4a0584732d","4248d38078dc4b3d8e64adb155cdd9c7","162aea432a6942e3be43ab8e5fa853d7","7d2e16fd37c14c4ca061dd1335b3a169","0c3728a8011d491f945a5cd372c9266e","671515132b684aa586da4316fe658ec1","a52dd147547041bbae76415bf00aaa80"]},"executionInfo":{"status":"ok","timestamp":1750192367778,"user_tz":240,"elapsed":17335,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"}},"outputId":"a1b7ffec-f8d2-495c-dbfd-364a4940f8c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.6.2: Fast Llama patching. Transformers: 4.52.4.\n","   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/762M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3385990f9e154013830b9a334febdeaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9953ae83ee048a0bd4978063f346010"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["unsloth/tinyllama-chat-bnb-4bit does not have a padding token! Will use pad_token = <unk>.\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.6.2 patched 22 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"]},{"output_type":"execute_result","data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): LlamaForCausalLM(\n","      (model): LlamaModel(\n","        (embed_tokens): Embedding(32000, 2048, padding_idx=0)\n","        (layers): ModuleList(\n","          (0-21): 22 x LlamaDecoderLayer(\n","            (self_attn): LlamaAttention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2048, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=2048, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2048, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=256, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2048, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=256, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (o_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2048, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=2048, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (rotary_emb): LlamaRotaryEmbedding()\n","            )\n","            (mlp): LlamaMLP(\n","              (gate_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2048, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=5632, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (up_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2048, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=5632, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (down_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.1, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=5632, out_features=16, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=16, out_features=2048, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (act_fn): SiLU()\n","            )\n","            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","          )\n","        )\n","        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (rotary_emb): LlamaRotaryEmbedding()\n","      )\n","      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":28}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel, PeftConfig\n","from unsloth import FastLanguageModel\n","import torch\n","\n","ft_model_path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/TinyLlama-fine-tune-3\"\n","\n","ft_model, ft_tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = ft_model_path,\n","    max_seq_length = 2048,\n","    dtype = None,\n","    load_in_4bit = True,\n",")\n","FastLanguageModel.for_inference(ft_model)"]},{"cell_type":"code","execution_count":null,"id":"xiCufUvsQ5R-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1646371,"status":"ok","timestamp":1750194014154,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"xiCufUvsQ5R-","outputId":"3f52d0c1-be2b-4d60-f7cf-cedff5adb1ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generating for test item 1...\n","Generating for test item 2...\n","Generating for test item 3...\n","Generating for test item 4...\n","Generating for test item 5...\n","Test items 1-5 generated in 84.408 seconds.\n","\n","Generating for test item 6...\n","Generating for test item 7...\n","Generating for test item 8...\n","Generating for test item 9...\n","Generating for test item 10...\n","Test items 6-10 generated in 80.938 seconds.\n","\n","Generating for test item 11...\n","Generating for test item 12...\n","Generating for test item 13...\n","Generating for test item 14...\n","Generating for test item 15...\n","Test items 11-15 generated in 80.824 seconds.\n","\n","Generating for test item 16...\n","Generating for test item 17...\n","Generating for test item 18...\n","Generating for test item 19...\n","Generating for test item 20...\n","Test items 16-20 generated in 85.460 seconds.\n","\n","Generating for test item 21...\n","Generating for test item 22...\n","Generating for test item 23...\n","Generating for test item 24...\n","Generating for test item 25...\n","Test items 21-25 generated in 80.751 seconds.\n","\n","Generating for test item 26...\n","Generating for test item 27...\n","Generating for test item 28...\n","Generating for test item 29...\n","Generating for test item 30...\n","Test items 26-30 generated in 79.021 seconds.\n","\n","Generating for test item 31...\n","Generating for test item 32...\n","Generating for test item 33...\n","Generating for test item 34...\n","Generating for test item 35...\n","Test items 31-35 generated in 82.626 seconds.\n","\n","Generating for test item 36...\n","Generating for test item 37...\n","Generating for test item 38...\n","Generating for test item 39...\n","Generating for test item 40...\n","Test items 36-40 generated in 84.422 seconds.\n","\n","Generating for test item 41...\n","Generating for test item 42...\n","Generating for test item 43...\n","Generating for test item 44...\n","Generating for test item 45...\n","Test items 41-45 generated in 86.305 seconds.\n","\n","Generating for test item 46...\n","Generating for test item 47...\n","Generating for test item 48...\n","Generating for test item 49...\n","Generating for test item 50...\n","Test items 46-50 generated in 81.779 seconds.\n","\n","Generating for test item 51...\n","Generating for test item 52...\n","Generating for test item 53...\n","Generating for test item 54...\n","Generating for test item 55...\n","Test items 51-55 generated in 80.412 seconds.\n","\n","Generating for test item 56...\n","Generating for test item 57...\n","Generating for test item 58...\n","Generating for test item 59...\n","Generating for test item 60...\n","Test items 56-60 generated in 83.203 seconds.\n","\n","Generating for test item 61...\n","Generating for test item 62...\n","Generating for test item 63...\n","Generating for test item 64...\n","Generating for test item 65...\n","Test items 61-65 generated in 79.696 seconds.\n","\n","Generating for test item 66...\n","Generating for test item 67...\n","Generating for test item 68...\n","Generating for test item 69...\n","Generating for test item 70...\n","Test items 66-70 generated in 81.544 seconds.\n","\n","Generating for test item 71...\n","Generating for test item 72...\n","Generating for test item 73...\n","Generating for test item 74...\n","Generating for test item 75...\n","Test items 71-75 generated in 79.748 seconds.\n","\n","Generating for test item 76...\n","Generating for test item 77...\n","Generating for test item 78...\n","Generating for test item 79...\n","Generating for test item 80...\n","Test items 76-80 generated in 79.691 seconds.\n","\n","Generating for test item 81...\n","Generating for test item 82...\n","Generating for test item 83...\n","Generating for test item 84...\n","Generating for test item 85...\n","Test items 81-85 generated in 84.878 seconds.\n","\n","Generating for test item 86...\n","Generating for test item 87...\n","Generating for test item 88...\n","Generating for test item 89...\n","Generating for test item 90...\n","Test items 86-90 generated in 82.267 seconds.\n","\n","Generating for test item 91...\n","Generating for test item 92...\n","Generating for test item 93...\n","Generating for test item 94...\n","Generating for test item 95...\n","Test items 91-95 generated in 84.829 seconds.\n","\n","Generating for test item 96...\n","Generating for test item 97...\n","Generating for test item 98...\n","Generating for test item 99...\n","Generating for test item 100...\n","Test items 96-100 generated in 82.833 seconds.\n","\n","Total generation time: 27 minutes and 25 seconds.\n","\n","Outputs guardados exitosamente en '/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_fine_tuned.json'\n"]}],"source":["from transformers import pipeline\n","\n","if eleccion == \"1\":\n","\n","  # Pre-compile the base context to avoid string concatenation in loop\n","  base_ctx = (\n","      \"You are a Movie Recommendation System.\"\n","      \"Generate a numbered list of 10 Movies. \\n\"\n","      \"RULES: \\n\"\n","      \"a) DO NOT write dialogs, explanations nor additional text or information. \\n\"\n","      \"b) DO NOT recommend movies already mentioned in the conversation. \\n\"\n","      \"c) You MUST recommend 10 movies, nothing more, nothing less. \\n\"\n","      \"d) The movies MUST be numbered from 1 to 10, with one movie name per line. \\n\"\n","      \"\\nFailure to follow the rules will result in incorrect output and be discarded by the system.\"\n","  )\n","\n","  outputs_ft_s = []\n","  k = 20\n","\n","  # Generation parameters (moved outside loop)\n","  gen_kwargs = {\n","      \"max_new_tokens\": 200,\n","      \"do_sample\": True,\n","      \"temperature\": 0.7,\n","      \"top_k\": 50,\n","      \"top_p\": 0.95,\n","      \"pad_token_id\": ft_tokenizer.eos_token_id,\n","      \"eos_token_id\": ft_tokenizer.eos_token_id\n","    }\n","\n","  total_start_time = time.time()\n","  for n in range(num_test_items):\n","      if torch.cuda.is_available():\n","          torch.cuda.empty_cache()\n","      if n%5 == 0:\n","        start_time = time.time()\n","      print(f\"Generating for test item {n+1}...\")\n","      outputs = []\n","\n","      # Build message once per test item\n","      msg = (\n","          \"\\nBased on the following conversation: \\n\"\n","          f\"{rand_conversations[n][1]} \\n\\n\"\n","          # \"And the movies the user has previously interacted with: \\n\"\n","          # f\"{random.sample(rand_conversations[n][5], min(10, len(rand_conversations[n][5])))}\\n\\n\"\n","          \"Generate a list of 10 recommended movies (JUST NAMES, ONE PER LINE):\"\n","      )\n","\n","      messages = [{\"role\": \"user\", \"content\": base_ctx + msg}]\n","      prompt = ft_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","\n","      # Tokenize once per test item, not per generation\n","      inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","      # Batch generation option (much faster if memory allows)\n","      with torch.no_grad():\n","          iter_start_time = time.time()\n","          output_ids = ft_model.generate(\n","              **inputs,\n","              num_return_sequences=k,\n","              **gen_kwargs\n","          )\n","          for output_id in output_ids:\n","              decoded = ft_tokenizer.decode(output_id, skip_special_tokens=True)\n","              outputs.append(decoded)\n","          # print(f\"Iteration {i} of test item {n+1} generated in {time.time() - iter_start_time} seconds.\")\n","\n","      outputs_ft_s.append(outputs)\n","      if (n+1)%5 == 0:\n","        print(f\"Test items {n-3}-{n+1} generated in {(time.time() - start_time):.3f} seconds.\\n\")\n","  total = time.time() - total_start_time\n","  print(f\"Total generation time: {int(total//60)} minutes and {int(total - total//60*60)} seconds.\\n\")\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_fine_tuned.json\"\n","  guardar_datos_json(outputs_ft_s, path)\n","\n","elif eleccion == \"2\":\n","  path = \"/content/gdrive/MyDrive/Proyecto LLMonkeys/outputs/TinyLlama/outputs_fine_tuned.json\"\n","  outputs_ft_s = cargar_datos_json(path)\n"]},{"cell_type":"markdown","id":"kEEEo2aUjYsJ","metadata":{"id":"kEEEo2aUjYsJ"},"source":["## Evaluaci√≥n de los modelos:"]},{"cell_type":"code","execution_count":null,"id":"5ckBE_kOquuF","metadata":{"id":"5ckBE_kOquuF"},"outputs":[],"source":["import numpy as np\n","import re\n","from sklearn.metrics import ndcg_score\n","import re\n","import html\n","from rapidfuzz import fuzz\n","\n","def normalizar_titulo(titulo):\n","    # Decode entidades HTML como &amp;\n","    titulo = html.unescape(titulo)\n","    # Min√∫sculas\n","    titulo = titulo.lower()\n","    # Eliminar puntuaci√≥n excepto letras, n√∫meros y &\n","    titulo = re.sub(r\"[^a-z0-9& ]+\", \"\", titulo)\n","    # Eliminar m√∫ltiples espacios\n","    titulo = re.sub(r\"\\s+\", \" \", titulo).strip()\n","    return titulo\n","\n","def comparar_titulos(t1, t2):\n","    t1 = normalizar_titulo(t1)\n","    t2 = normalizar_titulo(t2)\n","    return fuzz.token_set_ratio(t1, t2)\n","\n","# Funciones generadas por DeepSeek\n","def recall_at_k(generated_recommendations, ground_truth, k=10):\n","    hits = 0\n","    # Tomar las primeras K recomendaciones generadas\n","    top_k = generated_recommendations[:k]\n","    for e in top_k:\n","      if comparar_titulos(e, ground_truth[0]) > 80:\n","        hits = 1\n","\n","    # Evitar divisi√≥n por cero\n","    return hits\n","\n","def ndcg_at_k(generated_recommendations, ground_truth, k=10):\n","    # Crear una lista binaria de relevancia (1 si est√° en ground truth, 0 si no)\n","    relevance = [1 if item in ground_truth else 0 for item in generated_recommendations[:k]]\n","\n","    # Crear el \"ideal ranking\" (todas las relevantes primero)\n","    ideal_relevance = sorted(relevance, reverse=True)\n","\n","    # Calcular NDCG\n","    return ndcg_score([relevance], [ideal_relevance])\n"]},{"cell_type":"markdown","id":"W9dFcT15rGqg","metadata":{"id":"W9dFcT15rGqg"},"source":["### Recall@5 y NDCG@5:"]},{"cell_type":"code","execution_count":null,"id":"z8V6EPI4nvqU","metadata":{"id":"z8V6EPI4nvqU"},"outputs":[],"source":["z_s_rec_lists = []\n","i=1\n","for outputs in outputs_z_s:\n","  rec_lists = []\n","  for out in outputs:\n","      ans = format_ans(out[out.index(\"<|assistant|>\"):],10)\n","      rec_lists.append(format_ans(out[out.index(\"<|assistant|>\"):],10))\n","\n","  # print(rec_lists)\n","  z_s_rec_lists.append(rec_lists)"]},{"cell_type":"code","execution_count":null,"id":"KFtGD754n0ao","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1144,"status":"ok","timestamp":1750194015463,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"KFtGD754n0ao","outputId":"fa88e750-a1f1-4aa2-df1d-9da594d43e68"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.12 0.03079388872450849\n","0.17 0.06764274720732359\n"]}],"source":["recall_zs_1_5 = 0\n","ndcg_zs_1_5 = 0\n","best_recall_zs_5 = 0.0\n","best_ndcg_zs_5 = 0.0\n","\n","for i in range(num_test_items):\n","  # Zero-shot sin sampling\n","  # print(i)\n","  if len(z_s_rec_lists[i][0]) > 1:\n","    recall_zs_1_5 += recall_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    ndcg_zs_1_5 += ndcg_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","\n","  # Zero-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","\n","  for l in z_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_zs_5 += best_r\n","  best_ndcg_zs_5 += best_n\n","print(recall_zs_1_5/num_test_items, ndcg_zs_1_5/num_test_items)\n","print(best_recall_zs_5/num_test_items, best_ndcg_zs_5/num_test_items)"]},{"cell_type":"code","execution_count":null,"id":"2AGf4D8nxxsF","metadata":{"id":"2AGf4D8nxxsF"},"outputs":[],"source":["f_s_rec_lists = []\n","i=1\n","for outputs in outputs_f_s:\n","  rec_lists = []\n","  for out in outputs:\n","      ans = format_ans(out[out.index(\"<|assistant|>\"):],10)\n","      rec_lists.append(format_ans(out[out.index(\"<|assistant|>\"):],10))\n","\n","  # print(rec_lists)\n","  f_s_rec_lists.append(rec_lists)"]},{"cell_type":"code","execution_count":null,"id":"hzbGGXLXxbXH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":467,"status":"ok","timestamp":1750189899204,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"hzbGGXLXxbXH","outputId":"2e1ee901-6797-42b5-a25b-f17366592a9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.0 0.0\n","0.12 0.05079388872450849\n"]}],"source":["recall_fs_1_5 = 0\n","ndcg_fs_1_5 = 0\n","best_recall_fs_5 = 0.0\n","best_ndcg_fs_5 = 0.0\n","\n","for i in range(num_test_items):\n","  # Few-shot sin sampling\n","  # print(f\"\\n{f_s_rec_lists[i][0]}\")\n","  # print([item_map[m] for m in rand_conversations[i][4]])\n","  if len(f_s_rec_lists[i][0]) > 1:\n","    recall_fs_1_5 += recall_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    ndcg_fs_1_5 += ndcg_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","\n","  # Few-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in f_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fs_5 += best_r\n","  best_ndcg_fs_5 += best_n\n","print(recall_fs_1_5/num_test_items, ndcg_fs_1_5/num_test_items)\n","print(best_recall_fs_5/num_test_items, best_ndcg_fs_5/num_test_items)"]},{"cell_type":"code","source":["ft_s_rec_lists = []\n","i=1\n","for outputs in outputs_ft_s:\n","  rec_lists = []\n","  for out in outputs:\n","      ans = format_ans(out[out.index(\"<|assistant|>\"):],10)\n","      rec_lists.append(format_ans(out[out.index(\"<|assistant|>\"):],10))\n","\n","  # print(rec_lists)\n","  ft_s_rec_lists.append(rec_lists)"],"metadata":{"id":"fcbfXEatWRNQ"},"id":"fcbfXEatWRNQ","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"zQ1TgaF3RSd4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1185,"status":"ok","timestamp":1750194016779,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"zQ1TgaF3RSd4","outputId":"746abe05-26bc-4cdc-e196-c200411ce558"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.11 0.08743509370014568\n","0.2 0.15230624149734415\n"]}],"source":["recall_fts_1_5 = 0\n","ndcg_fts_1_5 = 0\n","best_recall_fts_5 = 0.0\n","best_ndcg_fts_5 = 0.0\n","\n","for i in range(num_test_items):\n","  # Few-shot sin sampling\n","  # print(f\"\\n{ft_s_rec_lists[i][0]}\")\n","  # print([item_map[m] for m in rand_conversations[i][4]])\n","  if len(ft_s_rec_lists[i][0]) > 1:\n","    recall_fts_1_5 += recall_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    ndcg_fts_1_5 += ndcg_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=5)\n","\n","  # Few-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in ft_s_rec_lists[i]:\n","    # print(l)\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=5)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fts_5 += best_r\n","  best_ndcg_fts_5 += best_n\n","print(recall_fts_1_5/num_test_items, ndcg_fts_1_5/num_test_items)\n","print(best_recall_fts_5/num_test_items, best_ndcg_fts_5/num_test_items)"]},{"cell_type":"markdown","id":"qmki7Uk51XRq","metadata":{"id":"qmki7Uk51XRq"},"source":["### Recall@10 y NDCG@10:"]},{"cell_type":"markdown","id":"_vUG0JzS27dP","metadata":{"id":"_vUG0JzS27dP"},"source":["Zero-Shot:"]},{"cell_type":"code","execution_count":null,"id":"V2-5q7OS2658","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1173,"status":"ok","timestamp":1750194017954,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"V2-5q7OS2658","outputId":"32c10661-1ca5-4811-b540-9f89247a7f0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.14 0.031455116214975126\n","0.22 0.07299769145246339\n"]}],"source":["recall_zs_1_10 = 0\n","ndcg_zs_1_10 = 0\n","best_recall_zs_10 = 0.0\n","best_ndcg_zs_10 = 0.0\n","\n","for i in range(num_test_items):\n","  # Zero-shot sin sampling\n","  if len(z_s_rec_lists[i][0]) > 1:\n","    recall_zs_1_10 += recall_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    ndcg_zs_1_10 += ndcg_at_k(z_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","\n","  # Zero-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in z_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_zs_10 += best_r\n","  best_ndcg_zs_10 += best_n\n","print(recall_zs_1_10/num_test_items, ndcg_zs_1_10/num_test_items)\n","print(best_recall_zs_10/num_test_items, best_ndcg_zs_10/num_test_items)"]},{"cell_type":"markdown","id":"TfxN0Sbb3MQU","metadata":{"id":"TfxN0Sbb3MQU"},"source":["Few-Shot"]},{"cell_type":"code","execution_count":null,"id":"eTt6k8Rm3NAE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":529,"status":"ok","timestamp":1750189901001,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"eTt6k8Rm3NAE","outputId":"424c38ba-76bf-4e2a-faf8-332ca69166b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.06 0.015749152613725975\n","0.24 0.07094383904259266\n"]}],"source":["recall_fs_1_10 = 0\n","ndcg_fs_1_10 = 0\n","best_recall_fs_10 = 0.0\n","best_ndcg_fs_10 = 0.0\n","\n","for i in range(num_test_items):\n","  # Few-shot sin sampling\n","  if len(f_s_rec_lists[i][0]) > 1:\n","    recall_fs_1_10 += recall_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    ndcg_fs_1_10 += ndcg_at_k(f_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","\n","  # Few-shot con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in f_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fs_10 += best_r\n","  best_ndcg_fs_10 += best_n\n","print(recall_fs_1_10/num_test_items, ndcg_fs_1_10/num_test_items)\n","print(best_recall_fs_10/num_test_items, best_ndcg_fs_10/num_test_items)"]},{"cell_type":"code","execution_count":null,"id":"8bb9hE3VYSz6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1320,"status":"ok","timestamp":1750194019278,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"8bb9hE3VYSz6","outputId":"4e2578e4-5657-4d27-bc2a-f1864bce3176"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.12 0.08444275460530175\n","0.24 0.15678200754544513\n"]}],"source":["recall_fts_1_10 = 0\n","ndcg_fts_1_10 = 0\n","best_recall_fts_10 = 0.0\n","best_ndcg_fts_10 = 0.0\n","\n","for i in range(num_test_items):\n","  # Fine-tuned sin sampling\n","  if len(ft_s_rec_lists[i][0]) > 1:\n","    recall_fts_1_10 += recall_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    ndcg_fts_1_10 += ndcg_at_k(ft_s_rec_lists[i][0], [item_map[m] for m in rand_conversations[i][4]], k=10)\n","\n","  # Fine-tuned con sampling\n","  best_r = 0.0\n","  best_n = 0.0\n","  recall = 0\n","  ndcg = 0\n","\n","  for l in ft_s_rec_lists[i]:\n","    if len(l) > 1:\n","      recall = recall_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","      ndcg = ndcg_at_k(l, [item_map[m] for m in rand_conversations[i][4]], k=10)\n","    if recall > best_r:\n","      best_r = recall\n","    if ndcg > best_n:\n","      best_n = ndcg\n","\n","  best_recall_fts_10 += best_r\n","  best_ndcg_fts_10 += best_n\n","print(recall_fts_1_10/num_test_items, ndcg_fts_1_10/num_test_items)\n","print(best_recall_fts_10/num_test_items, best_ndcg_fts_10/num_test_items)"]},{"cell_type":"code","execution_count":null,"id":"UzWXLtfi3XxD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1750194669474,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"UzWXLtfi3XxD","outputId":"eaa9ded4-f8e8-42dd-a918-86bb1e2b124e"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Zero-Shot con interacciones hist√≥ricas:\n","Sin sampling: Recall@5: 0.120, NDCG@5: 0.031, Recall@10: 0.140, NDCG@10: 0.031\n","Con sampling: Recall@5: 0.170, NDCG@5: 0.068, Recall@10: 0.220, NDCG@10: 0.073\n","\n","Fine-tuned sin interacciones hist√≥ricas:\n","Sin sampling: Recall@5: 0.110, NDCG@5: 0.087, Recall@10: 0.120, NDCG@10: 0.084\n","Con sampling: Recall@5: 0.200, NDCG@5: 0.152, Recall@10: 0.240, NDCG@10: 0.157\n"]}],"source":["print(\"\\nZero-Shot con interacciones hist√≥ricas:\")\n","print(f\"Sin sampling: Recall@5: {recall_zs_1_5/num_test_items:.3f}, NDCG@5: {ndcg_zs_1_5/num_test_items:.3f}, Recall@10: {recall_zs_1_10/num_test_items:.3f}, NDCG@10: {ndcg_zs_1_10/num_test_items:.3f}\")\n","print(f\"Con sampling: Recall@5: {best_recall_zs_5/num_test_items:.3f}, NDCG@5: {best_ndcg_zs_5/num_test_items:.3f}, Recall@10: {best_recall_zs_10/num_test_items:.3f}, NDCG@10: {best_ndcg_zs_10/num_test_items:.3f}\")\n","\n","# print(\"\\nFew-Shot con interacciones hist√≥ricas:\")\n","# print(f\"Sin sampling: Recall@5: {recall_fs_1_5/num_test_items:.3f}, NDCG@5: {ndcg_fs_1_5/num_test_items:.3f}, Recall@10: {recall_fs_1_10/num_test_items:.3f}, NDCG@10: {ndcg_fs_1_10/num_test_items:.3f}\")\n","# print(f\"Con sampling: Recall@5: {best_recall_fs_5/num_test_items:.3f}, NDCG@5: {best_ndcg_fs_5/num_test_items:.3f}, Recall@10: {best_recall_fs_10/num_test_items:.3f}, NDCG@10: {best_ndcg_fs_10/num_test_items:.3f}\")\n","\n","print(\"\\nFine-tuned sin interacciones hist√≥ricas:\")\n","print(f\"Sin sampling: Recall@5: {recall_fts_1_5/num_test_items:.3f}, NDCG@5: {ndcg_fts_1_5/num_test_items:.3f}, Recall@10: {recall_fts_1_10/num_test_items:.3f}, NDCG@10: {ndcg_fts_1_10/num_test_items:.3f}\")\n","print(f\"Con sampling: Recall@5: {best_recall_fts_5/num_test_items:.3f}, NDCG@5: {best_ndcg_fts_5/num_test_items:.3f}, Recall@10: {best_recall_fts_10/num_test_items:.3f}, NDCG@10: {best_ndcg_fts_10/num_test_items:.3f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"JWy3oCvI4PsI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2104,"status":"ok","timestamp":1749224091155,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"JWy3oCvI4PsI","outputId":"b07de7da-b4f9-4e37-f646-db8b74cb7e4b"},"outputs":[{"data":{"text/plain":["LlamaForCausalLM(\n","  (model): LlamaModel(\n","    (embed_tokens): Embedding(32000, 2048)\n","    (layers): ModuleList(\n","      (0-21): 22 x LlamaDecoderLayer(\n","        (self_attn): LlamaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","        )\n","        (mlp): LlamaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n","          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n","      )\n","    )\n","    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",")"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","import torch.nn.functional as F\n","import random\n","import numpy as np\n","\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Ahora con AutoModelForCausal a diferencia de la primera instancia\n","model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n","model.eval()"]},{"cell_type":"code","execution_count":null,"id":"sIndZYYaOO60","metadata":{"id":"sIndZYYaOO60"},"outputs":[],"source":["import torch\n","import numpy as np\n","from collections import Counter\n","import re\n","import gc\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from scipy.stats import entropy\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer\n","\n","def extract_answer(text, question_text):\n","    \"\"\"Extrae la respuesta del texto generado\"\"\"\n","    # Buscar despu√©s de \"Answer:\" o similar\n","    patterns = [r\"Answer:\\s*(.+?)(?:\\n|$)\", r\"answer:\\s*(.+?)(?:\\n|$)\", r\"Answer is\\s*(.+?)(?:\\n|$)\"]\n","\n","    for pattern in patterns:\n","        match = re.search(pattern, text, re.IGNORECASE)\n","        if match:\n","            return match.group(1).strip()\n","\n","    # Si no encuentra patr√≥n, tomar lo que viene despu√©s del prompt\n","    try:\n","        # Dividir por el texto de la pregunta y tomar la parte despu√©s\n","        parts = text.split(\"Answer:\")\n","        if len(parts) > 1:\n","            return parts[-1].strip().split('\\n')[0].strip()\n","    except:\n","        pass\n","\n","    return text.strip()\n","\n","def calculate_uncertainty_metrics_lightweight(outputs_per_paraphrase, paraphrases):\n","    \"\"\"\n","    Calcula m√©tricas de incertidumbre usando Input Clarification Ensembling\n","\n","    Args:\n","        outputs_per_paraphrase: Lista de listas, cada sublista contiene outputs para una par√°frasis\n","        paraphrases: Lista de par√°frasis usadas\n","\n","    Returns:\n","        dict con m√©tricas de incertidumbre\n","    \"\"\"\n","\n","    # 1. Extraer respuestas limpias\n","    all_answers = []\n","    answers_by_paraphrase = []\n","\n","    for i, outputs in enumerate(outputs_per_paraphrase):\n","        paraphrase_answers = []\n","        for output in outputs:\n","            answer = extract_answer(output, paraphrases[i])\n","            paraphrase_answers.append(answer)\n","            all_answers.append(answer)\n","        answers_by_paraphrase.append(paraphrase_answers)\n","\n","    # 2. Calcular frecuencias de respuestas\n","    answer_counts = Counter(all_answers)\n","    total_responses = len(all_answers)\n","\n","    # 3. INCERTIDUMBRE TOTAL (Shannon Entropy de todas las respuestas)\n","    probs = np.array(list(answer_counts.values())) / total_responses\n","    total_uncertainty = entropy(probs, base=2)  # bits\n","\n","    # 4. INCERTIDUMBRE ALEATORIA (promedio de entrop√≠as por par√°frasis)\n","    aleatoric_uncertainties = []\n","    for answers in answers_by_paraphrase:\n","        local_counts = Counter(answers)\n","        local_probs = np.array(list(local_counts.values())) / len(answers)\n","        if len(local_probs) > 1:\n","            aleatoric_uncertainties.append(entropy(local_probs, base=2))\n","        else:\n","            aleatoric_uncertainties.append(0.0)\n","\n","    aleatoric_uncertainty = np.mean(aleatoric_uncertainties)\n","\n","    # 5. INCERTIDUMBRE EPIST√âMICA (diferencia)\n","    epistemic_uncertainty = total_uncertainty - aleatoric_uncertainty\n","\n","    # 6. M√©tricas adicionales\n","    unique_answers = len(set(all_answers))\n","    most_common_answer, most_common_count = answer_counts.most_common(1)[0]\n","    confidence = most_common_count / total_responses\n","\n","    # 7. Consistencia entre par√°frasis\n","    consistency_scores = []\n","    for i in range(len(paraphrases)):\n","        for j in range(i+1, len(paraphrases)):\n","            # Comparar respuestas m√°s frecuentes de cada par√°frasis\n","            answers_i = Counter(answers_by_paraphrase[i])\n","            answers_j = Counter(answers_by_paraphrase[j])\n","\n","            most_common_i = answers_i.most_common(1)[0][0] if answers_i else \"\"\n","            most_common_j = answers_j.most_common(1)[0][0] if answers_j else \"\"\n","\n","            # Similaridad simple (exacta o parcial)\n","            if most_common_i.lower().strip() == most_common_j.lower().strip():\n","                consistency_scores.append(1.0)\n","            else:\n","                # Similaridad parcial usando tokens comunes\n","                tokens_i = set(most_common_i.lower().split())\n","                tokens_j = set(most_common_j.lower().split())\n","                if tokens_i and tokens_j:\n","                    jaccard = len(tokens_i.intersection(tokens_j)) / len(tokens_i.union(tokens_j))\n","                    consistency_scores.append(jaccard)\n","                else:\n","                    consistency_scores.append(0.0)\n","\n","    consistency = np.mean(consistency_scores) if consistency_scores else 0.0\n","\n","    return {\n","        'total_uncertainty': float(total_uncertainty),\n","        'aleatoric_uncertainty': float(aleatoric_uncertainty),\n","        'epistemic_uncertainty': float(epistemic_uncertainty),\n","        'confidence': float(confidence),\n","        'unique_answers': int(unique_answers),\n","        'most_common_answer': str(most_common_answer),\n","        'consistency_across_paraphrases': float(consistency),\n","        'answer_distribution': {str(k): int(v) for k, v in answer_counts.items()},\n","        'num_paraphrases': len(paraphrases),\n","        'samples_per_paraphrase': len(outputs_per_paraphrase[0]) if outputs_per_paraphrase else 0\n","    }\n","\n","def calculate_logit_uncertainty_lightweight(model, tokenizer, paraphrases, device=\"cuda\", cleanup=True):\n","    \"\"\"\n","    Versi√≥n optimizada que limpia memoria agresivamente\n","    \"\"\"\n","    print(\"Calculando incertidumbre por logits (versi√≥n ligera)...\")\n","\n","    uncertainties_per_paraphrase = []\n","    logits_stats = []\n","\n","    for i, paraphrase in enumerate(paraphrases):\n","        print(f\"Procesando logits para par√°frasis {i+1}/{len(paraphrases)}\")\n","\n","        ctx = \"You are an oracle who only responds with short and concise answers.\"\n","        msg = f\"Answer the following question: {paraphrase}\\nAnswer:\"\n","        messages = [{\"role\": \"user\", \"content\": ctx + msg}]\n","\n","        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","            next_token_logits = outputs.logits[0, -1, :].cpu()  # Mover a CPU inmediatamente\n","\n","            # Calcular probabilidades y entrop√≠a\n","            probs = torch.softmax(next_token_logits, dim=-1)\n","            uncertainty = entropy(probs.numpy(), base=2)\n","            uncertainties_per_paraphrase.append(float(uncertainty))\n","\n","            # Guardar solo estad√≠sticas b√°sicas, no los logits completos\n","            logits_stats.append({\n","                'mean': float(next_token_logits.mean()),\n","                'std': float(next_token_logits.std()),\n","                'max': float(next_token_logits.max()),\n","                'min': float(next_token_logits.min())\n","            })\n","\n","            # Limpiar memoria inmediatamente\n","            del outputs, next_token_logits, probs, inputs\n","            if cleanup:\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","    # Calcular m√©tricas finales sin guardar arrays grandes\n","    mean_uncertainty = np.mean(uncertainties_per_paraphrase)\n","    std_uncertainty = np.std(uncertainties_per_paraphrase)\n","\n","    return {\n","        'logit_uncertainties_per_paraphrase': uncertainties_per_paraphrase,\n","        'mean_logit_uncertainty': float(mean_uncertainty),\n","        'std_logit_uncertainty': float(std_uncertainty),\n","        'logits_stats_summary': {\n","            'mean_of_means': float(np.mean([s['mean'] for s in logits_stats])),\n","            'mean_of_stds': float(np.mean([s['std'] for s in logits_stats])),\n","        }\n","    }\n"]},{"cell_type":"code","execution_count":null,"id":"vXg4zGJy4D6e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45589,"status":"ok","timestamp":1749232577003,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"vXg4zGJy4D6e","outputId":"3268e98d-e674-4820-8f71-d54775586785"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generando respuestas...\n","Procesando par√°frasis 1/10: What year did the Berlin Wall fall?\n","Procesando par√°frasis 2/10: When did the Berlin Wall come down?\n","Procesando par√°frasis 3/10: In which year was the Berlin Wall demolished?\n","Procesando par√°frasis 4/10: What year did the fall of the Berlin Wall occur?\n","Procesando par√°frasis 5/10: When was the Berlin Wall brought down?\n","Procesando par√°frasis 6/10: In what year did the Berlin Wall collapse?\n","Procesando par√°frasis 7/10: What year did they tear down the Berlin Wall?\n","Procesando par√°frasis 8/10: When did the destruction of the Berlin Wall happen?\n","Procesando par√°frasis 9/10: In which year did the Berlin Wall get demolished?\n","Procesando par√°frasis 10/10: What year marked the fall of the Berlin Wall?\n"]}],"source":["\n","# Configuraci√≥n\n","model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # o el modelo que uses\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Cargar modelo y tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","# Par√°frasis de la pregunta\n","paraphrases = [\n","    \"What year did the Berlin Wall fall?\",\n","    \"When did the Berlin Wall come down?\",\n","    \"In which year was the Berlin Wall demolished?\",\n","    \"What year did the fall of the Berlin Wall occur?\",\n","    \"When was the Berlin Wall brought down?\",\n","    \"In what year did the Berlin Wall collapse?\",\n","    \"What year did they tear down the Berlin Wall?\",\n","    \"When did the destruction of the Berlin Wall happen?\",\n","    \"In which year did the Berlin Wall get demolished?\",\n","    \"What year marked the fall of the Berlin Wall?\"\n","]\n","\n","# Generar respuestas con sampling\n","outputs_per_paraphrase = []\n","k = 5  # N√∫mero de samples por par√°frasis\n","\n","print(\"Generando respuestas...\")\n","for i, paraphrase in enumerate(paraphrases):\n","    print(f\"Procesando par√°frasis {i+1}/{len(paraphrases)}: {paraphrase}\")\n","\n","    outputs = []\n","    ctx = \"You are an oracle who only responds with short and concise answers.\"\n","    msg = f\"Answer the following question: {paraphrase}\\nAnswer:\"\n","    messages = [{\"role\": \"user\", \"content\": ctx + msg}]\n","\n","    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","    for j in range(k):\n","        with torch.no_grad():\n","            output_ids = model.generate(\n","                **inputs,\n","                max_new_tokens=50,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_k=50,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id\n","            )\n","\n","            # Decodificar solo la parte nueva (sin el prompt)\n","            new_tokens = output_ids[0][inputs['input_ids'].shape[1]:]\n","            decoded = tokenizer.decode(new_tokens, skip_special_tokens=True)\n","            outputs.append(decoded)\n","\n","    outputs_per_paraphrase.append(outputs)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"WuCMGEixYJQF","metadata":{"id":"WuCMGEixYJQF"},"outputs":[],"source":["include_logits = True\n","cleanup_model = True"]},{"cell_type":"code","execution_count":null,"id":"J7IbxEarPAV0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4683,"status":"ok","timestamp":1749232589598,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"J7IbxEarPAV0","outputId":"28aff982-0159-42e2-ce68-a225bc5bfd33"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================================\n","CALCULANDO INCERTIDUMBRE (VERSI√ìN OPTIMIZADA)...\n","==================================================\n","Respuestas extra√≠das:\n","Par√°frasis 1: [\"The Berlin Wall fell in 1989, on November 9, 1989, when East Germany's government announced the end of the wall separating East and West Berlin.\", 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the division of East and West Germany.', 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.', 'The Berlin Wall fell on November 9, 1989, which was 24 years ago.', 'The Berlin Wall fell on November 9, 1989, at 03:15 a.m. CET (Central European Time) in the early morning hours of November 9, 1989.']\n","Par√°frasis 2: ['The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.', \"The Berlin Wall came down on November 9, 1989, on the evening of that day. The Soviet Union's fall from power and the subsequent peaceful reunification of Germany marked a historic turning point in Europe's history\", 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union. The wall was constructed by the East German government to keep East Germans from escaping to', 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Eastern Bloc.', 'The Berlin Wall came down on November 9, 1989, after the Soviet Union had officially withdrawn its troops from the eastern part of Germany. This event marked the end of the Cold War and the collapse of the Eastern B']\n","Par√°frasis 3: ['The Berlin Wall was demolished on November 9, 1989.', 'In 1989, the Berlin Wall was demolished.', 'The question states that you are an oracle who only responds with short and concise answers. The answer to the question is in 1989, when the Berlin Wall was demolished.', 'In 1989, the Berlin Wall was demolished, which marked the end of the Cold War and the division of Europe.', 'The Berlin Wall was demolished in 1989, which was the year the wall was officially dismantled.']\n","Par√°frasis 4: ['The fall of the Berlin Wall occurred in 1989, not 1984 as stated in the given text.', 'The fall of the Berlin Wall occurred in 1989, which was 31 years ago.', 'The fall of the Berlin Wall occurred in 1989, which was also the year that the Soviet Union officially dismantled its military presence in the country.', 'The fall of the Berlin Wall occurred in 1989.', 'The fall of the Berlin Wall occurred in 1989, not in 1987.']\n","Par√°frasis 5: ['The Berlin Wall was brought down on November 9, 1989, by East German soldiers with a barricade made of concrete and barbed wire.', 'The Berlin Wall was brought down on November 9, 1989, after the Soviet Union allowed German citizens to travel freely for the first time in decades.', 'The Berlin Wall was brought down on November 9, 1989, when East German border guards opened fire on a group of demonstrators attempting to cross the border from West Berlin into East Germany. The event led to the collapse of', 'The Berlin Wall was brought down on November 9, 1989, by the East German government, following a series of protests and demonstrations that began on November 9, 1989, calling for the release of', 'The Berlin Wall was brought down on October 3, 1989, by a group of East German soldiers and civilians, led by East German General Walter Ulbricht, who had been sent to negotiate with the West German']\n","Par√°frasis 6: ['The Berlin Wall collapsed in 1989, at the beginning of the fall of the Berlin Wall.', 'The Berlin Wall collapsed in 1989, not in 1981 as stated in the given material.', 'The Berlin Wall collapsed in 1989, the year 1989.', 'The Berlin Wall collapsed on November 9, 1989, in the early hours of the morning.', 'The Berlin Wall collapsed on November 9, 1989, marking the end of the Cold War and the fall of the Soviet Union.']\n","Par√°frasis 7: ['The year when the Berlin Wall was torn down is not specified in the given text.', 'The question is not directly related to the given material.', 'that it was on November 9, 1989, when East Germany officially announced its independence from the Soviet Union and the wall was removed.', '1989.', 'The question does not specify a year for which they tear down the Berlin Wall.']\n","Par√°frasis 8: ['The destruction of the Berlin Wall happened on November 9, 1989, when Soviet forces removed the border barrier that had separated East and West Berlin for over 28 years.', 'The destruction of the Berlin Wall happened on November 9, 1989, when East German troops and police forcibly removed barbed wire and blockades from the eastern side of the city, allowing West Berliners to cross over', 'The destruction of the Berlin Wall happened on November 9, 1989, when East German officials unveiled a new wall that sealed off East Berlin from the rest of West Berlin. The wall had been built as a symbol of', 'The destruction of the Berlin Wall happened on November 9, 1989, when East Germany officially abandoned its border with the West and began to open its borders to West Germans. The Wall had been erected in 196', 'The destruction of the Berlin Wall happened on October 3, 1989, after the fall of the Soviet Union.']\n","Par√°frasis 9: ['The Berlin Wall got demolished in 1989, during the Cold War era.', 'The Berlin Wall was demolished on November 9, 1989, in the early hours of the morning after the German Democratic Republic (GDR) government announced its intention to dismantle it.', 'The Berlin Wall got demolished on August 15, 1961, in the early hours of the morning.', 'The Berlin Wall got demolished in 1989.', 'The Berlin Wall was demolished on November 9, 1989.']\n","Par√°frasis 10: [\"The fall of the Berlin Wall occurred in 1989, marking the end of the Cold War and the end of the Soviet Union's control over East Germany.\", 'The fall of the Berlin Wall occurred in 1989, not 1989 as stated in the given text.', 'The fall of the Berlin Wall occurred in 1989, which was a year before the question was asked.', 'The fall of the Berlin Wall occurred in 1989, not in 1984.', 'The fall of the Berlin Wall, which occurred in 1989, marked the end of the Cold War and the collapse of the Soviet Union. The fall of the wall was a turning point in history, signaling the end of a long']\n","Calculando incertidumbre por logits (versi√≥n ligera)...\n","Procesando logits para par√°frasis 1/10\n","Procesando logits para par√°frasis 2/10\n","Procesando logits para par√°frasis 3/10\n","Procesando logits para par√°frasis 4/10\n","Procesando logits para par√°frasis 5/10\n","Procesando logits para par√°frasis 6/10\n","Procesando logits para par√°frasis 7/10\n","Procesando logits para par√°frasis 8/10\n","Procesando logits para par√°frasis 9/10\n","Procesando logits para par√°frasis 10/10\n","Limpiando modelo de memoria...\n","‚úì Modelo removido de memoria\n","\n","üìä M√âTRICAS DE INCERTIDUMBRE:\n","‚îú‚îÄ‚îÄ Incertidumbre Total: 5.604 bits\n","‚îú‚îÄ‚îÄ Incertidumbre Aleatoria: 2.322 bits\n","‚îú‚îÄ‚îÄ Incertidumbre Epist√©mica: 3.282 bits\n","‚îú‚îÄ‚îÄ Confianza: 0.040\n","‚îú‚îÄ‚îÄ Respuestas √∫nicas: 49\n","‚îú‚îÄ‚îÄ Consistencia entre par√°frasis: 0.233\n","‚îî‚îÄ‚îÄ Respuesta m√°s com√∫n: 'The Berlin Wall was demolished on November 9, 1989.'\n","\n","üî¢ M√âTRICAS DE LOGITS:\n","‚îú‚îÄ‚îÄ Incertidumbre promedio: 0.926 bits\n","‚îú‚îÄ‚îÄ Desviaci√≥n est√°ndar: 0.528 bits\n","‚îî‚îÄ‚îÄ Media de logits: -1.714\n","\n","üß† INTERPRETACI√ìN:\n","‚îú‚îÄ‚îÄ El modelo tiene m√°s incertidumbre sobre QU√â responder\n","‚îî‚îÄ‚îÄ ‚Üí Sugiere falta de conocimiento espec√≠fico\n","‚îú‚îÄ‚îÄ Baja consistencia - posible confusi√≥n del modelo\n","{'uncertainty_metrics': {'total_uncertainty': 5.603856189774723, 'aleatoric_uncertainty': 2.3219280948873626, 'epistemic_uncertainty': 3.281928094887361, 'confidence': 0.04, 'unique_answers': 49, 'most_common_answer': 'The Berlin Wall was demolished on November 9, 1989.', 'consistency_across_paraphrases': 0.2328805773295286, 'answer_distribution': {\"The Berlin Wall fell in 1989, on November 9, 1989, when East Germany's government announced the end of the wall separating East and West Berlin.\": 1, 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the division of East and West Germany.': 1, 'The Berlin Wall fell on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.': 1, 'The Berlin Wall fell on November 9, 1989, which was 24 years ago.': 1, 'The Berlin Wall fell on November 9, 1989, at 03:15 a.m. CET (Central European Time) in the early morning hours of November 9, 1989.': 1, 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union.': 1, \"The Berlin Wall came down on November 9, 1989, on the evening of that day. The Soviet Union's fall from power and the subsequent peaceful reunification of Germany marked a historic turning point in Europe's history\": 1, 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Soviet Union. The wall was constructed by the East German government to keep East Germans from escaping to': 1, 'The Berlin Wall came down on November 9, 1989, marking the end of the Cold War and the collapse of the Eastern Bloc.': 1, 'The Berlin Wall came down on November 9, 1989, after the Soviet Union had officially withdrawn its troops from the eastern part of Germany. This event marked the end of the Cold War and the collapse of the Eastern B': 1, 'The Berlin Wall was demolished on November 9, 1989.': 2, 'In 1989, the Berlin Wall was demolished.': 1, 'The question states that you are an oracle who only responds with short and concise answers. The answer to the question is in 1989, when the Berlin Wall was demolished.': 1, 'In 1989, the Berlin Wall was demolished, which marked the end of the Cold War and the division of Europe.': 1, 'The Berlin Wall was demolished in 1989, which was the year the wall was officially dismantled.': 1, 'The fall of the Berlin Wall occurred in 1989, not 1984 as stated in the given text.': 1, 'The fall of the Berlin Wall occurred in 1989, which was 31 years ago.': 1, 'The fall of the Berlin Wall occurred in 1989, which was also the year that the Soviet Union officially dismantled its military presence in the country.': 1, 'The fall of the Berlin Wall occurred in 1989.': 1, 'The fall of the Berlin Wall occurred in 1989, not in 1987.': 1, 'The Berlin Wall was brought down on November 9, 1989, by East German soldiers with a barricade made of concrete and barbed wire.': 1, 'The Berlin Wall was brought down on November 9, 1989, after the Soviet Union allowed German citizens to travel freely for the first time in decades.': 1, 'The Berlin Wall was brought down on November 9, 1989, when East German border guards opened fire on a group of demonstrators attempting to cross the border from West Berlin into East Germany. The event led to the collapse of': 1, 'The Berlin Wall was brought down on November 9, 1989, by the East German government, following a series of protests and demonstrations that began on November 9, 1989, calling for the release of': 1, 'The Berlin Wall was brought down on October 3, 1989, by a group of East German soldiers and civilians, led by East German General Walter Ulbricht, who had been sent to negotiate with the West German': 1, 'The Berlin Wall collapsed in 1989, at the beginning of the fall of the Berlin Wall.': 1, 'The Berlin Wall collapsed in 1989, not in 1981 as stated in the given material.': 1, 'The Berlin Wall collapsed in 1989, the year 1989.': 1, 'The Berlin Wall collapsed on November 9, 1989, in the early hours of the morning.': 1, 'The Berlin Wall collapsed on November 9, 1989, marking the end of the Cold War and the fall of the Soviet Union.': 1, 'The year when the Berlin Wall was torn down is not specified in the given text.': 1, 'The question is not directly related to the given material.': 1, 'that it was on November 9, 1989, when East Germany officially announced its independence from the Soviet Union and the wall was removed.': 1, '1989.': 1, 'The question does not specify a year for which they tear down the Berlin Wall.': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when Soviet forces removed the border barrier that had separated East and West Berlin for over 28 years.': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when East German troops and police forcibly removed barbed wire and blockades from the eastern side of the city, allowing West Berliners to cross over': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when East German officials unveiled a new wall that sealed off East Berlin from the rest of West Berlin. The wall had been built as a symbol of': 1, 'The destruction of the Berlin Wall happened on November 9, 1989, when East Germany officially abandoned its border with the West and began to open its borders to West Germans. The Wall had been erected in 196': 1, 'The destruction of the Berlin Wall happened on October 3, 1989, after the fall of the Soviet Union.': 1, 'The Berlin Wall got demolished in 1989, during the Cold War era.': 1, 'The Berlin Wall was demolished on November 9, 1989, in the early hours of the morning after the German Democratic Republic (GDR) government announced its intention to dismantle it.': 1, 'The Berlin Wall got demolished on August 15, 1961, in the early hours of the morning.': 1, 'The Berlin Wall got demolished in 1989.': 1, \"The fall of the Berlin Wall occurred in 1989, marking the end of the Cold War and the end of the Soviet Union's control over East Germany.\": 1, 'The fall of the Berlin Wall occurred in 1989, not 1989 as stated in the given text.': 1, 'The fall of the Berlin Wall occurred in 1989, which was a year before the question was asked.': 1, 'The fall of the Berlin Wall occurred in 1989, not in 1984.': 1, 'The fall of the Berlin Wall, which occurred in 1989, marked the end of the Cold War and the collapse of the Soviet Union. The fall of the wall was a turning point in history, signaling the end of a long': 1}, 'num_paraphrases': 10, 'samples_per_paraphrase': 5}, 'logit_metrics': {'logit_uncertainties_per_paraphrase': [0.8899151086807251, 0.4174751937389374, 1.1662006378173828, 0.5137522220611572, 0.5372160077095032, 0.9283269047737122, 2.1689369678497314, 0.2583453357219696, 1.0907313823699951, 1.2852729558944702], 'mean_logit_uncertainty': 0.9256172716617584, 'std_logit_uncertainty': 0.5277849618054207, 'logits_stats_summary': {'mean_of_means': -1.714435636997223, 'mean_of_stds': 2.5638801574707033}}, 'analysis_params': {'num_paraphrases': 10, 'samples_per_paraphrase': 5, 'included_logits': True}}\n"]}],"source":["print(\"=\"*50)\n","print(\"CALCULANDO INCERTIDUMBRE (VERSI√ìN OPTIMIZADA)...\")\n","print(\"=\"*50)\n","\n","# 1. Calcular m√©tricas b√°sicas de incertidumbre\n","uncertainty_metrics = calculate_uncertainty_metrics_lightweight(outputs_per_paraphrase, paraphrases)\n","\n","# 2. Calcular m√©tricas de logits si se solicita\n","logit_metrics = None\n","if include_logits and model is not None and tokenizer is not None:\n","    logit_metrics = calculate_logit_uncertainty_lightweight(\n","        model, tokenizer, paraphrases, device, cleanup=True\n","    )\n","\n","# 3. LIMPIAR MODELO DE MEMORIA SI SE SOLICITA\n","if cleanup_model and model is not None:\n","    print(\"Limpiando modelo de memoria...\")\n","    del model\n","    if tokenizer is not None:\n","        del tokenizer\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    print(\"‚úì Modelo removido de memoria\")\n","\n","# 4. Mostrar resultados\n","print(f\"\\nüìä M√âTRICAS DE INCERTIDUMBRE:\")\n","print(f\"‚îú‚îÄ‚îÄ Incertidumbre Total: {uncertainty_metrics['total_uncertainty']:.3f} bits\")\n","print(f\"‚îú‚îÄ‚îÄ Incertidumbre Aleatoria: {uncertainty_metrics['aleatoric_uncertainty']:.3f} bits\")\n","print(f\"‚îú‚îÄ‚îÄ Incertidumbre Epist√©mica: {uncertainty_metrics['epistemic_uncertainty']:.3f} bits\")\n","print(f\"‚îú‚îÄ‚îÄ Confianza: {uncertainty_metrics['confidence']:.3f}\")\n","print(f\"‚îú‚îÄ‚îÄ Respuestas √∫nicas: {uncertainty_metrics['unique_answers']}\")\n","print(f\"‚îú‚îÄ‚îÄ Consistencia entre par√°frasis: {uncertainty_metrics['consistency_across_paraphrases']:.3f}\")\n","print(f\"‚îî‚îÄ‚îÄ Respuesta m√°s com√∫n: '{uncertainty_metrics['most_common_answer']}'\")\n","\n","if logit_metrics:\n","    print(f\"\\nüî¢ M√âTRICAS DE LOGITS:\")\n","    print(f\"‚îú‚îÄ‚îÄ Incertidumbre promedio: {logit_metrics['mean_logit_uncertainty']:.3f} bits\")\n","    print(f\"‚îú‚îÄ‚îÄ Desviaci√≥n est√°ndar: {logit_metrics['std_logit_uncertainty']:.3f} bits\")\n","    print(f\"‚îî‚îÄ‚îÄ Media de logits: {logit_metrics['logits_stats_summary']['mean_of_means']:.3f}\")\n","\n","# 5. Interpretaci√≥n\n","print(f\"\\nüß† INTERPRETACI√ìN:\")\n","if uncertainty_metrics['epistemic_uncertainty'] > uncertainty_metrics['aleatoric_uncertainty']:\n","    print(\"‚îú‚îÄ‚îÄ El modelo tiene m√°s incertidumbre sobre QU√â responder\")\n","    print(\"‚îî‚îÄ‚îÄ ‚Üí Sugiere falta de conocimiento espec√≠fico\")\n","else:\n","    print(\"‚îú‚îÄ‚îÄ El modelo tiene m√°s incertidumbre sobre C√ìMO responder\")\n","    print(\"‚îî‚îÄ‚îÄ ‚Üí Sugiere ambig√ºedad inherente en la pregunta\")\n","\n","if uncertainty_metrics['consistency_across_paraphrases'] > 0.8:\n","    print(\"‚îú‚îÄ‚îÄ Alta consistencia entre par√°frasis\")\n","elif uncertainty_metrics['consistency_across_paraphrases'] > 0.5:\n","    print(\"‚îú‚îÄ‚îÄ Consistencia moderada entre par√°frasis\")\n","else:\n","    print(\"‚îú‚îÄ‚îÄ Baja consistencia - posible confusi√≥n del modelo\")\n","\n","# DEVOLVER SOLO RESULTADOS LIGEROS\n","results = {\n","    'uncertainty_metrics': uncertainty_metrics,\n","    'logit_metrics': logit_metrics,\n","    'analysis_params': {\n","        'num_paraphrases': len(paraphrases),\n","        'samples_per_paraphrase': len(outputs_per_paraphrase[0]) if outputs_per_paraphrase else 0,\n","        'included_logits': include_logits\n","    }\n","}\n","\n","print(results)"]},{"cell_type":"code","execution_count":null,"id":"yvDBDRuyfE8a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84597,"status":"ok","timestamp":1749234579270,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"yvDBDRuyfE8a","outputId":"f266fa86-0eeb-483d-f4ee-103f15dc1246"},"outputs":[{"name":"stdout","output_type":"stream","text":["Generando respuestas...\n","Procesando par√°frasis 1/10: What year did the Berlin Wall fall?\n","Procesando par√°frasis 2/10: When did the Berlin Wall come down?\n","Procesando par√°frasis 3/10: In which year was the Berlin Wall demolished?\n","Procesando par√°frasis 4/10: What year did the fall of the Berlin Wall occur?\n","Procesando par√°frasis 5/10: When was the Berlin Wall brought down?\n","Procesando par√°frasis 6/10: In what year did the Berlin Wall collapse?\n","Procesando par√°frasis 7/10: What year did they tear down the Berlin Wall?\n","Procesando par√°frasis 8/10: When did the destruction of the Berlin Wall happen?\n","Procesando par√°frasis 9/10: In which year did the Berlin Wall get demolished?\n","Procesando par√°frasis 10/10: What year marked the fall of the Berlin Wall?\n"]}],"source":["\n","# Configuraci√≥n\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","ft_model = ft_model.to(device)\n","if ft_tokenizer.pad_token is None:\n","    ft_tokenizer.pad_token = ft_tokenizer.eos_token\n","\n","# Par√°frasis de la pregunta\n","paraphrases = [\n","    \"What year did the Berlin Wall fall?\",\n","    \"When did the Berlin Wall come down?\",\n","    \"In which year was the Berlin Wall demolished?\",\n","    \"What year did the fall of the Berlin Wall occur?\",\n","    \"When was the Berlin Wall brought down?\",\n","    \"In what year did the Berlin Wall collapse?\",\n","    \"What year did they tear down the Berlin Wall?\",\n","    \"When did the destruction of the Berlin Wall happen?\",\n","    \"In which year did the Berlin Wall get demolished?\",\n","    \"What year marked the fall of the Berlin Wall?\"\n","]\n","\n","# Generar respuestas con sampling\n","outputs_per_paraphrase = []\n","k = 5  # N√∫mero de samples por par√°frasis\n","\n","print(\"Generando respuestas...\")\n","for i, paraphrase in enumerate(paraphrases):\n","    print(f\"Procesando par√°frasis {i+1}/{len(paraphrases)}: {paraphrase}\")\n","\n","    outputs = []\n","    ctx = \"You are an oracle who only responds with short and concise answers.\"\n","    msg = f\"Answer the following question: {paraphrase}\\nAnswer:\"\n","    messages = [{\"role\": \"user\", \"content\": ctx + msg}]\n","\n","    prompt = ft_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","    inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(device)\n","\n","    for j in range(k):\n","        with torch.no_grad():\n","            output_ids = ft_model.generate(\n","                **inputs,\n","                max_new_tokens=50,\n","                do_sample=True,\n","                temperature=0.7,\n","                top_k=50,\n","                top_p=0.95,\n","                pad_token_id=ft_tokenizer.eos_token_id\n","            )\n","\n","            # Decodificar solo la parte nueva (sin el prompt)\n","            new_tokens = output_ids[0][inputs['input_ids'].shape[1]:]\n","            decoded = ft_tokenizer.decode(new_tokens, skip_special_tokens=True)\n","            outputs.append(decoded)\n","\n","    outputs_per_paraphrase.append(outputs)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"hlD12V3kfeV-","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4441,"status":"ok","timestamp":1749234627036,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"hlD12V3kfeV-","outputId":"1b92ed05-b050-416c-e6e7-b6c14d17085b"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================================\n","CALCULANDO INCERTIDUMBRE (VERSI√ìN OPTIMIZADA)...\n","==================================================\n","Calculando incertidumbre por logits (versi√≥n ligera)...\n","Procesando logits para par√°frasis 1/10\n","Procesando logits para par√°frasis 2/10\n","Procesando logits para par√°frasis 3/10\n","Procesando logits para par√°frasis 4/10\n","Procesando logits para par√°frasis 5/10\n","Procesando logits para par√°frasis 6/10\n","Procesando logits para par√°frasis 7/10\n","Procesando logits para par√°frasis 8/10\n","Procesando logits para par√°frasis 9/10\n","Procesando logits para par√°frasis 10/10\n","Limpiando modelo de memoria...\n","‚úì Modelo removido de memoria\n","\n","üìä M√âTRICAS DE INCERTIDUMBRE:\n","‚îú‚îÄ‚îÄ Incertidumbre Total: 5.644 bits\n","‚îú‚îÄ‚îÄ Incertidumbre Aleatoria: 2.322 bits\n","‚îú‚îÄ‚îÄ Incertidumbre Epist√©mica: 3.322 bits\n","‚îú‚îÄ‚îÄ Confianza: 0.020\n","‚îú‚îÄ‚îÄ Respuestas √∫nicas: 50\n","‚îú‚îÄ‚îÄ Consistencia entre par√°frasis: 0.179\n","‚îî‚îÄ‚îÄ Respuesta m√°s com√∫n: 'I don't have access to the latest news. However, I'm glad you asked. The Berlin Wall fell on August 14, 1961, and it became a symbol of the Cold War between the Soviet Union and'\n","\n","üî¢ M√âTRICAS DE LOGITS:\n","‚îú‚îÄ‚îÄ Incertidumbre promedio: 3.444 bits\n","‚îú‚îÄ‚îÄ Desviaci√≥n est√°ndar: 0.306 bits\n","‚îî‚îÄ‚îÄ Media de logits: -2.010\n","\n","üß† INTERPRETACI√ìN:\n","‚îú‚îÄ‚îÄ El modelo tiene m√°s incertidumbre sobre QU√â responder\n","‚îî‚îÄ‚îÄ ‚Üí Sugiere falta de conocimiento espec√≠fico\n","‚îú‚îÄ‚îÄ Baja consistencia - posible confusi√≥n del modelo\n","{'uncertainty_metrics': {'total_uncertainty': 5.643856189774724, 'aleatoric_uncertainty': 2.3219280948873626, 'epistemic_uncertainty': 3.3219280948873617, 'confidence': 0.02, 'unique_answers': 50, 'most_common_answer': \"I don't have access to the latest news. However, I'm glad you asked. The Berlin Wall fell on August 14, 1961, and it became a symbol of the Cold War between the Soviet Union and\", 'consistency_across_paraphrases': 0.17905680999218387, 'answer_distribution': {\"I don't have access to the latest news. However, I'm glad you asked. The Berlin Wall fell on August 14, 1961, and it became a symbol of the Cold War between the Soviet Union and\": 1, 'The Berlin Wall was divided into four': 1, \"I don't have access to real-time updates. However, according to the article you provided, the Berlin Wall fell in 1989.\\n\\nAs for your question, the author mentioned that it was a historical event that sh\": 1, 'Yes, I can provide you with a response. According to the given material, the Berlin Wall fell in 1989.\\n\\nDetails:\\nThe Berlin Wall fell in 1989, after 28 years of opp': 1, 'I am not able to provide you with specific answers. However, you may find this article helpful: \\n\\n[insert article name]\\n[insert article description]\\n[insert article rating]\\n[insert article content]\\n\\nI hope': 1, \"I don't have access to current events or historical events. However, based on my knowledge, the Berlin Wall was built in 1961 to stop the flow of people and goods between West Germany and East Germany. It was finally broken\": 1, \"I'm not able to provide a specific answer as I don't have access to real-time information. However, I can tell you that the Berlin Wall came down in 1989.\\n\\nIf you need further assistance,\": 1, 'The Berlin Wall came down on November 9': 1, \"I'm sorry but I don't have access to current events or information about specific dates.\\n\\nHowever, based on your previous question, I assume that you're referring to the Berlin Wall. In 1989, East\": 1, \"I don't have the ability to provide long answers. However, I can tell you that the Berlin Wall came down on November 9, 1989. It was a symbol of the fall of the Berlin Wall and the end of\": 1, \"I don't have access to the latest information, but according to one source, the Berlin Wall was demolished in 1989.\\n\\nSource: https://www.time.com/time-magazine/article/0\": 1, 'I\\'m sorry, but I don\\'t have access to the most recent information. However, based on your previous response, I\\'d recommend watching \"A Little Night Music\" on Amazon. It\\'s a great movie, but I\\'': 1, \"I don't have access to real-time information. However, according to the provided text, it mentions that the Berlin Wall was demolished in 1989.\\n\\nSources:\\nhttps://www.youtube.com/\": 1, \"I'm not sure about that answer. Can you provide me with another question?\\n\\nAs for your question, the Berlin Wall demolition was in 1989.\\n\\nIf you need any more answers or specific details, feel\": 1, '1989': 1, 'Certainly! According to Wikipedia, The fall of the Berlin Wall occurred on November 9, 1989. The event marked the end of the Cold War and the reunification of Germany. It was a symbolic event that symbol': 1, 'The fall of the Berlin Wall occurred in 1989, during the Cold War era. The Berlin Wall was a barrier that separated East Germany from West Germany, which was a part of the Soviet Union. The wall was built by the': 1, \"I don't have access to historical events. Please provide me with the answer to your question.\\n\\nHistorical events:\\n\\n1. The fall of the Berlin Wall occurred in November 1989.\\n\\n2.\": 1, 'The fall of the Berlin Wall occurred in 1989, during the Cold War era. The wall was built to divide East Germany and West Germany, and it was the culmination of a decade-long struggle for control of the': 1, \"I don't have any information about specific events. However, I can provide you with a brief summary of the fall of the Berlin Wall. In 1989, the Soviet Union announced that it would dismantle its eastern b\": 1, 'Certainly! The Berlin Wall was brought down on November 9, 1989. This was a decisive moment in the history of the German Democratic Republic. It was the final seal on the collapse of the Soviet Union,': 1, \"I don't have access to current events. However, according to one source, the Berlin Wall was brought down on November 9, 1989. It was the last remaining border between East Germany and West Germany.\\n\\nS\": 1, 'I can provide you with a detailed answer. The Berlin Wall was brought down on November 9, 1989, when the East German government announced that it was dismantling the wall. The wall, which had separated East and': 1, \"I don't have any specific information about the Berlin Wall brought down. However, if you're looking for more details about it, here's a review:\\n\\nReview: The Berlin Wall came down in 1989\": 1, \"I'm sorry, but I don't have the capacity to provide you with a specific answer based on your previous question.\\n\\nIs there anything else you would like me to assist you with?\\n\\n[If yes, tell me the\": 1, 'The Berlin Wall was built in 1961 to separate East and West Berlin. It was deemed necessary to prevent the spread of communism. After the reunification of Germany in 1990, it became a symbol': 1, \"I'm afraid I don't have a definitive answer for you. However, according to various sources, the Berlin Wall collapse was in 1989.\\n\\nSource: https://www.youtube.com/watch?v\": 1, 'I can provide you with more detailed information about the Berlin Wall collapse. In 1989, the German government announced that it would remove the Berlin Wall, which had separated East and West Berlin since 1961. This was a': 1, \"I don't have access to current events. However, in 1989, the Berlin Wall was finally torn down, ending the Cold War.\\n\\nsource: https://en.wikipedia.org/wiki/Berlin_W\": 1, \"I don't have access to the latest information. However, according to the text you provided, you can find the answer in the given passage. In what year did the Berlin Wall collapse?\\n\\nBased on the given passage, the answer\": 1, 'The Berlin Wall was torn down in 1989.\\n\\nHere\\'s a related quote: \"The Berlin Wall was torn down in 1989. The Cold War was over.\" - from the movie \"The American President': 1, \"I can't provide you with specific answers. However, I can tell you that in the 1990s, the Berlin Wall was torn down by the German government.\\n\\nIn summary, the Berlin Wall was torn down by the\": 1, \"I'm sorry but I didn't hear you properly. Can you provide me with more information about the Berlin Wall?\\n\\nAs a child, I had a vivid imagination and loved stories of the Berlin Wall. I could picture a future\": 1, 'I do not have access to specific information about when the Berlin Wall was torn down. However, I can provide you with some similar answers to your question.\\n\\nIn 2007, the Berlin Wall was rebuilt, but it was': 1, 'I don\\'t have access to the latest news. However, I can provide you with a brief summary of a movie called \"The Trip to Italy\" which was released in 2014. It\\'s a comedy starring Steve': 1, \"I'm sorry but I don't have access to the exact date of the destruction of the Berlin Wall. However, according to the given text, the author mentioned that the destruction of the Berlin Wall happened in 1989.\": 1, \"I can't answer your question since I don't have the context of your previous question. However, in general, the destruction of the Berlin Wall was in 1989. The Berlin Wall was built in 1961\": 1, 'I do not have access to current events. However, according to some sources, the destruction of the Berlin Wall happened on August 14, 1989.\\n\\nSources:\\n1. New York Times - \"German': 1, \"I'm sorry, but I don't have the specific answer you're looking for. What would you like me to tell you?\\n\\nBy the way, I can provide you with a short answer. The destruction of the Berlin Wall\": 1, 'I\\'m sorry, but I don\\'t have access to the exact date of the destruction of the Berlin Wall. Please provide me with the relevant information.\\n\\n[User Provides: \"The Berlin Wall was demolished on November 9': 1, \"I'm sorry, but I don't have access to the most recent information. Can you provide me with the answer to your question?\\n\\nAs per my knowledge, the Berlin Wall was demolished on November 9, 19\": 1, \"I don't have access to specific historical events. However, based on the given text, it seems that the Berlin Wall got demolished in 1989. The Berlin Wall, also known as the East-West Berlin Wall, was\": 1, 'specific and relevant to the question': 1, 'Yes, I can provide you with a short and concise answer.The Berlin Wall was demolished in 1989.\\n\\nIf you need more detailed information, please let me know.\\n\\nThank you for your interest!': 1, \"I'm afraid I don't have information about the year you're referring to. Can you provide me with more details about the Berlin Wall and its demolition?\\n\\n>assistant|Sure! According to a reliable source,\": 1, \"Sure, I'm sorry for the oversight. The question you were looking for is: What year marked the fall of the Berlin Wall?\\n\\nAccording to the article you read, it was 1989. The\": 1, \"Certainly! The fall of the Berlin Wall was a significant event in the Cold War. On November 9, 1989, East Germany's leader, Erich Honecker, announced that the Wall would be dism\": 1, \"Sure, I'm sorry for the oversight. In 1989, the Berlin Wall was brought down by the people of East Germany. It took them a few years to do so, but it was a significant moment in\": 1, \"I'm afraid I don't have information on the year of the fall of the Berlin Wall. However, I can tell you that it was a significant event in German history.\\n\\nHere's another question you might be interested in:\": 1, \"I'm sorry but I don't have access to current events. However, I can provide you with a detailed answer based on the given context.\\n\\nThe given context includes the events of 1989. The Berlin Wall was\": 1}, 'num_paraphrases': 10, 'samples_per_paraphrase': 5}, 'logit_metrics': {'logit_uncertainties_per_paraphrase': [3.6031086444854736, 2.917271614074707, 3.672024726867676, 3.6897640228271484, 3.1129651069641113, 3.486865758895874, 3.339991807937622, 3.0483243465423584, 3.7215628623962402, 3.8511717319488525], 'mean_logit_uncertainty': 3.4443050622940063, 'std_logit_uncertainty': 0.30606880206364356, 'logits_stats_summary': {'mean_of_means': -2.0096726417541504, 'mean_of_stds': 2.6050027132034304}}, 'analysis_params': {'num_paraphrases': 10, 'samples_per_paraphrase': 5, 'included_logits': True}}\n"]}],"source":["print(\"=\"*50)\n","print(\"CALCULANDO INCERTIDUMBRE (VERSI√ìN OPTIMIZADA)...\")\n","print(\"=\"*50)\n","\n","# 1. Calcular m√©tricas b√°sicas de incertidumbre\n","uncertainty_metrics = calculate_uncertainty_metrics_lightweight(outputs_per_paraphrase, paraphrases)\n","\n","# 2. Calcular m√©tricas de logits si se solicita\n","logit_metrics = None\n","if include_logits and ft_model is not None and tokenizer is not None:\n","    logit_metrics = calculate_logit_uncertainty_lightweight(\n","        ft_model, tokenizer, paraphrases, device, cleanup=True\n","    )\n","\n","# 3. LIMPIAR MO.DELO DE MEMORIA SI SE SOLICITA\n","if cleanup_model and ft_model is not None:\n","    print(\"Limpiando modelo de memoria...\")\n","    del ft_model\n","    if tokenizer is not None:\n","        del tokenizer\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    print(\"‚úì Modelo removido de memoria\")\n","\n","# 4. Mostrar resultados\n","print(f\"\\nüìä M√âTRICAS DE INCERTIDUMBRE:\")\n","print(f\"‚îú‚îÄ‚îÄ Incertidumbre Total: {uncertainty_metrics['total_uncertainty']:.3f} bits\")\n","print(f\"‚îú‚îÄ‚îÄ Incertidumbre Aleatoria: {uncertainty_metrics['aleatoric_uncertainty']:.3f} bits\")\n","print(f\"‚îú‚îÄ‚îÄ Incertidumbre Epist√©mica: {uncertainty_metrics['epistemic_uncertainty']:.3f} bits\")\n","print(f\"‚îú‚îÄ‚îÄ Confianza: {uncertainty_metrics['confidence']:.3f}\")\n","print(f\"‚îú‚îÄ‚îÄ Respuestas √∫nicas: {uncertainty_metrics['unique_answers']}\")\n","print(f\"‚îú‚îÄ‚îÄ Consistencia entre par√°frasis: {uncertainty_metrics['consistency_across_paraphrases']:.3f}\")\n","print(f\"‚îî‚îÄ‚îÄ Respuesta m√°s com√∫n: '{uncertainty_metrics['most_common_answer']}'\")\n","\n","if logit_metrics:\n","    print(f\"\\nüî¢ M√âTRICAS DE LOGITS:\")\n","    print(f\"‚îú‚îÄ‚îÄ Incertidumbre promedio: {logit_metrics['mean_logit_uncertainty']:.3f} bits\")\n","    print(f\"‚îú‚îÄ‚îÄ Desviaci√≥n est√°ndar: {logit_metrics['std_logit_uncertainty']:.3f} bits\")\n","    print(f\"‚îî‚îÄ‚îÄ Media de logits: {logit_metrics['logits_stats_summary']['mean_of_means']:.3f}\")\n","\n","# 5. Interpretaci√≥n\n","print(f\"\\nüß† INTERPRETACI√ìN:\")\n","if uncertainty_metrics['epistemic_uncertainty'] > uncertainty_metrics['aleatoric_uncertainty']:\n","    print(\"‚îú‚îÄ‚îÄ El modelo tiene m√°s incertidumbre sobre QU√â responder\")\n","    print(\"‚îî‚îÄ‚îÄ ‚Üí Sugiere falta de conocimiento espec√≠fico\")\n","else:\n","    print(\"‚îú‚îÄ‚îÄ El modelo tiene m√°s incertidumbre sobre C√ìMO responder\")\n","    print(\"‚îî‚îÄ‚îÄ ‚Üí Sugiere ambig√ºedad inherente en la pregunta\")\n","\n","if uncertainty_metrics['consistency_across_paraphrases'] > 0.8:\n","    print(\"‚îú‚îÄ‚îÄ Alta consistencia entre par√°frasis\")\n","elif uncertainty_metrics['consistency_across_paraphrases'] > 0.5:\n","    print(\"‚îú‚îÄ‚îÄ Consistencia moderada entre par√°frasis\")\n","else:\n","    print(\"‚îú‚îÄ‚îÄ Baja consistencia - posible confusi√≥n del modelo\")\n","\n","# DEVOLVER SOLO RESULTADOS LIGEROS\n","results = {\n","    'uncertainty_metrics': uncertainty_metrics,\n","    'logit_metrics': logit_metrics,\n","    'analysis_params': {\n","        'num_paraphrases': len(paraphrases),\n","        'samples_per_paraphrase': len(outputs_per_paraphrase[0]) if outputs_per_paraphrase else 0,\n","        'included_logits': include_logits\n","    }\n","}\n","\n","print(results)"]},{"cell_type":"code","execution_count":null,"id":"k5Tmxon6lZOH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1630,"status":"ok","timestamp":1749235943997,"user":{"displayName":"Monkey RecSys","userId":"15812993793125887963"},"user_tz":240},"id":"k5Tmxon6lZOH","outputId":"1751abc1-a5f5-4d05-c48c-201d68c11082"},"outputs":[{"name":"stdout","output_type":"stream","text":["üéØ Limpieza espec√≠fica para modelo LoRA...\n","   ‚îú‚îÄ‚îÄ Eliminando: tokenizer\n","   ‚îú‚îÄ‚îÄ Eliminando: pipe\n","   ‚îú‚îÄ‚îÄ Eliminando: inputs\n","   ‚îú‚îÄ‚îÄ Eliminando: output_ids\n","   ‚îú‚îÄ‚îÄ Eliminando: prompt\n","   ‚îú‚îÄ‚îÄ Eliminando: messages\n","   ‚îú‚îÄ‚îÄ Eliminando: ctx\n","   ‚îú‚îÄ‚îÄ Eliminando: msg\n","   ‚îú‚îÄ‚îÄ Eliminando: decoded\n","   ‚îú‚îÄ‚îÄ Eliminando: outputs\n","\n","üìä Variables mantenidas:\n","   ‚îú‚îÄ‚îÄ item_map: 9687 elementos\n","   ‚îú‚îÄ‚îÄ Conversation: 16935069 elementos\n","   ‚îú‚îÄ‚îÄ model_name: 34 elementos\n","   ‚îú‚îÄ‚îÄ outputs_f_s: 20 elementos\n","   ‚îú‚îÄ‚îÄ outputs_ft_s: 20 elementos\n","   ‚îú‚îÄ‚îÄ rand_conversations: 20 elementos\n","   ‚îú‚îÄ‚îÄ num_test_items: <class 'int'>\n","   ‚îú‚îÄ‚îÄ train_conv: 2512 elementos\n","   ‚îú‚îÄ‚îÄ test_conv: 619 elementos\n","   ‚îú‚îÄ‚îÄ num_test_items: <class 'int'>\n","   ‚îú‚îÄ‚îÄ few_shot_data: 5 elementos\n"]}],"source":["limpiar_y_guardar()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["tbkknIrVmsd_","SZkhMuOgm5eB","7Dw3xNSv2Im7","14e7c87a","lwDqdAOmrEh1"],"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a2451bbe3d004d1087185a8cdcc69f43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a88b0c4f0c4d4847af9db5f9ccaa70c6","IPY_MODEL_0225d641b1744c8b8f08755353352bec","IPY_MODEL_28e94efa713c42ce9b944a9442b7f60a"],"layout":"IPY_MODEL_de659da2708046a695fbc58382d477f6"}},"a88b0c4f0c4d4847af9db5f9ccaa70c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_889894fac9b147338a922e1d13a23535","placeholder":"‚Äã","style":"IPY_MODEL_cb3ce520a290461cbeeb84b545ac6cbf","value":"tokenizer_config.json:‚Äá100%"}},"0225d641b1744c8b8f08755353352bec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e952be3dd2e64666947d2e7240c1eb33","max":1289,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad09650412394084875c44682549e92b","value":1289}},"28e94efa713c42ce9b944a9442b7f60a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddd41ad1189d47f2a1c7995e2153ed61","placeholder":"‚Äã","style":"IPY_MODEL_d26f10ef6b334f11b6556cf2963268b4","value":"‚Äá1.29k/1.29k‚Äá[00:00&lt;00:00,‚Äá147kB/s]"}},"de659da2708046a695fbc58382d477f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"889894fac9b147338a922e1d13a23535":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb3ce520a290461cbeeb84b545ac6cbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e952be3dd2e64666947d2e7240c1eb33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad09650412394084875c44682549e92b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddd41ad1189d47f2a1c7995e2153ed61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d26f10ef6b334f11b6556cf2963268b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"84a7c97e7c4e43a2896f22814569fb60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70063c2c689742aca985bf67710aa44f","IPY_MODEL_d74d3fd8d6d94cc6a9afa34b5e227aa9","IPY_MODEL_5f2b4ce302f6494cbd51634ec7357ecd"],"layout":"IPY_MODEL_1ef566dd039d481e8fa8fd16d3531ccf"}},"70063c2c689742aca985bf67710aa44f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb1b0cf428244d0a5960f159d8d45ef","placeholder":"‚Äã","style":"IPY_MODEL_705eed9afee94f9292fa90e9978b3070","value":"tokenizer.model:‚Äá100%"}},"d74d3fd8d6d94cc6a9afa34b5e227aa9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed3184a948714ed990b8c301f8eba8fb","max":499723,"min":0,"orientation":"horizontal","style":"IPY_MODEL_978afc6635614b59ad054588b9282eec","value":499723}},"5f2b4ce302f6494cbd51634ec7357ecd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a611b91f678143578c64fedc8531da33","placeholder":"‚Äã","style":"IPY_MODEL_74c5fe1151b84fc08e236e245a08827a","value":"‚Äá500k/500k‚Äá[00:00&lt;00:00,‚Äá11.7MB/s]"}},"1ef566dd039d481e8fa8fd16d3531ccf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cb1b0cf428244d0a5960f159d8d45ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"705eed9afee94f9292fa90e9978b3070":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed3184a948714ed990b8c301f8eba8fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"978afc6635614b59ad054588b9282eec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a611b91f678143578c64fedc8531da33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c5fe1151b84fc08e236e245a08827a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96a21a7196464619825d155be0b999b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a4391426a8494f57bb8a2b72c0cd964c","IPY_MODEL_1f639d5e26db4b45a7a3368e64d1ac20","IPY_MODEL_8d728d16f0f04ef49a26ad2ba6704ed9"],"layout":"IPY_MODEL_395554c73a2247c99aa361d87018c0ef"}},"a4391426a8494f57bb8a2b72c0cd964c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64eea959a3f242f9bbf78b97ea1ddbba","placeholder":"‚Äã","style":"IPY_MODEL_9e58355700bb42e1b20571ed0783b37f","value":"tokenizer.json:‚Äá100%"}},"1f639d5e26db4b45a7a3368e64d1ac20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09585fe909fe4e6381cde16bcb04ae0c","max":1842767,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d16803b2c9c4bd69a69a4d6da28fefa","value":1842767}},"8d728d16f0f04ef49a26ad2ba6704ed9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da86389edfc946afa7804c0e4e127d91","placeholder":"‚Äã","style":"IPY_MODEL_be03fd89fc8344b2bba174b925db3ade","value":"‚Äá1.84M/1.84M‚Äá[00:00&lt;00:00,‚Äá13.3MB/s]"}},"395554c73a2247c99aa361d87018c0ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64eea959a3f242f9bbf78b97ea1ddbba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e58355700bb42e1b20571ed0783b37f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09585fe909fe4e6381cde16bcb04ae0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d16803b2c9c4bd69a69a4d6da28fefa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"da86389edfc946afa7804c0e4e127d91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be03fd89fc8344b2bba174b925db3ade":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f59e8c0df9e4865bf2292bf7443f44c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_48edd33c9bb6467db9e5f32c8e04c631","IPY_MODEL_c5db6f6ae92349a3ad95ef48204c34c9","IPY_MODEL_f11084089943480cb013b6e360588912"],"layout":"IPY_MODEL_a18a0b4da77c4b97a344744e7180bcfe"}},"48edd33c9bb6467db9e5f32c8e04c631":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edfbf556561349d88b0e46f62cb3cd02","placeholder":"‚Äã","style":"IPY_MODEL_695efbf4c7b9464588eee142c010ad8e","value":"special_tokens_map.json:‚Äá100%"}},"c5db6f6ae92349a3ad95ef48204c34c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_667fdbfbcb1a4b359571cf6edf19ed46","max":551,"min":0,"orientation":"horizontal","style":"IPY_MODEL_719e5e8465be47468d8a951bcefd7ad7","value":551}},"f11084089943480cb013b6e360588912":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f15d151fd6d849c5939d1e76d0176e7b","placeholder":"‚Äã","style":"IPY_MODEL_6a88619e1f434a3c8dd37738e8910685","value":"‚Äá551/551‚Äá[00:00&lt;00:00,‚Äá62.8kB/s]"}},"a18a0b4da77c4b97a344744e7180bcfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"edfbf556561349d88b0e46f62cb3cd02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"695efbf4c7b9464588eee142c010ad8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"667fdbfbcb1a4b359571cf6edf19ed46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"719e5e8465be47468d8a951bcefd7ad7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f15d151fd6d849c5939d1e76d0176e7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a88619e1f434a3c8dd37738e8910685":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3385990f9e154013830b9a334febdeaf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7d087b1f96ad42adad79f8f3cb63cbd6","IPY_MODEL_b06efcf3d890481ca28712ff23c54301","IPY_MODEL_efc0d19b03a24ec9ab733d7a74112fc9"],"layout":"IPY_MODEL_6f462510171b454e8b7f6d09f26b2228"}},"7d087b1f96ad42adad79f8f3cb63cbd6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d774ee12aec446a3939cfeda5e493980","placeholder":"‚Äã","style":"IPY_MODEL_7990accebc8c4bcd9175d6f0f13d7d8c","value":"model.safetensors:‚Äá100%"}},"b06efcf3d890481ca28712ff23c54301":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aab6ff39f284871822458a809a4e4c3","max":762453371,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76b554cce7d4420db27d207faf2250f5","value":762453371}},"efc0d19b03a24ec9ab733d7a74112fc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_541da9cea5984f0bb6522d6aa9335723","placeholder":"‚Äã","style":"IPY_MODEL_0bd3abfab28e44c2b6971ff6b675b58a","value":"‚Äá762M/762M‚Äá[00:02&lt;00:00,‚Äá743MB/s]"}},"6f462510171b454e8b7f6d09f26b2228":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d774ee12aec446a3939cfeda5e493980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7990accebc8c4bcd9175d6f0f13d7d8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8aab6ff39f284871822458a809a4e4c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76b554cce7d4420db27d207faf2250f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"541da9cea5984f0bb6522d6aa9335723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd3abfab28e44c2b6971ff6b675b58a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9953ae83ee048a0bd4978063f346010":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83abd0ecf7004cae9e999e869eaebb21","IPY_MODEL_fb2afd79add74394a8a3cb37cacaac8c","IPY_MODEL_11e885658701464baf6797e5042e5cb0"],"layout":"IPY_MODEL_201040722f6f4101b6e70b4a0584732d"}},"83abd0ecf7004cae9e999e869eaebb21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4248d38078dc4b3d8e64adb155cdd9c7","placeholder":"‚Äã","style":"IPY_MODEL_162aea432a6942e3be43ab8e5fa853d7","value":"generation_config.json:‚Äá100%"}},"fb2afd79add74394a8a3cb37cacaac8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d2e16fd37c14c4ca061dd1335b3a169","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c3728a8011d491f945a5cd372c9266e","value":124}},"11e885658701464baf6797e5042e5cb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_671515132b684aa586da4316fe658ec1","placeholder":"‚Äã","style":"IPY_MODEL_a52dd147547041bbae76415bf00aaa80","value":"‚Äá124/124‚Äá[00:00&lt;00:00,‚Äá11.7kB/s]"}},"201040722f6f4101b6e70b4a0584732d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4248d38078dc4b3d8e64adb155cdd9c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"162aea432a6942e3be43ab8e5fa853d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d2e16fd37c14c4ca061dd1335b3a169":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c3728a8011d491f945a5cd372c9266e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"671515132b684aa586da4316fe658ec1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a52dd147547041bbae76415bf00aaa80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}